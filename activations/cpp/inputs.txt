std::optional<std::vector<bool>> symbolic_dims) const {\nShape copy = *this;\ncopy.is_symbolic_ = symbolic_dims;\nreturn copy;\n}\n\nbool symbolicShapeEnabled() {\nstatic bool enabled = std::getenv("LTC_ENABLE_SYMBOLIC_SHAPES") != nullptr;\n
\nShape::Shape(\nat::ScalarType scalar_type,\nc10::ArrayRef<int64_t> sizes,\nstd::optional<std::vector<bool>> is_symbolic)\n: scalar_type_(scalar_type),\nsizes_(sizes.begin(), sizes.end()),\nis_symbolic_(std::move(is_symbolic)) {}\n
\nstatic c10::SymbolicShape get_symbolic_shape(at::Tensor& tensor) {\nauto ltc_tensor = TryGetLtcTensor(tensor);\nif (!ltc_tensor) {\n// Set Concrete sizes for Concrete tensors\nreturn c10::SymbolicShape(tensor.sizes());\n
\nbool Shape::operator==(const Shape& other) const {\nreturn scalar_type_ == other.scalar_type_ && sizes_ == other.sizes_;\n}\n\nstd::ostream& operator<<(std::ostream& out, const Shape& shape) {\nreturn out << shape.to_string();\n
if (*start < 0) {\n*start += length;\nif (*start < 0) {\n*start = (step < 0) ? -1 : 0;\n}\n} else if (*start >= length) {\n*start = (step < 0) ? length - 1 : length;\n}\n\nif (*stop < 0) {\n*stop += length;\n
*stop = (step < 0) ? length - 1 : length;\n}\n\nif (step < 0) {\nif (*stop < *start) {\nreturn (*start - *stop - 1) / (-step) + 1;\n}\n} else {\nif (*start < *stop) {\nreturn (*stop - *start - 1) / step + 1;\n
} else {\nif (*start < *stop) {\nreturn (*stop - *start - 1) / step + 1;\n}\n}\nreturn 0;\n}\n\n} // namespace torch::jit\n
}\n}\nreturn 0;\n}\n\n} // namespace torch::jit\n
// Otherwise, we force the unwrapped dtype.\ntensor = internal_new_from_data(\nTensorOptions(),\ndtype_unwrapped,\ndevice,\nobj,\n/* copy_variables = */ false,\n/* copy_numpy = */ false,\n/* type_inference = */ !dtype.has_value());\n
#include <vector>\n\nusing at::Device;\nusing at::IntArrayRef;\nusing at::kInt;\nusing at::kLong;\nusing at::ScalarType;\nusing at::Storage;\nusing at::Tensor;\nusing at::TensorOptions;\nusing std::optional;\n
if (scalarType == ScalarType::ComplexDouble) {\n// this won't change (unless we hit undefined, but that will fail\n// later).\nreturn *scalarType;\n}\n}\n// NOLINTNEXTLINE(bugprone-unchecked-optional-access)\n
\n// If the device is Meta, take the shortcut. We don't want to allocate\n// an empty CPU tensor which would break our contract for meta tensors.\nif (device == at::kMeta) {\nreturn at::empty(sizes, opts.device(device));\n
}\n};\n\nTEST(DeadlockDetection, basic) {\nASSERT_FALSE(check_python_gil());\nDummyPythonGILHooks hooks;\nSetPythonGILHooks(&hooks);\nASSERT_TRUE(check_python_gil());\nSetPythonGILHooks(nullptr);\n}\n
\nusing namespace ::testing;\nusing namespace c10::impl;\n\nstruct DummyPythonGILHooks : public PythonGILHooks {\nbool check_python_gil() const override {\nreturn true;\n}\n};\n\nTEST(DeadlockDetection, basic) {\n
}\n\n#ifndef _WIN32\nTEST(DeadlockDetection, disable) {\nsetenv("TORCH_DISABLE_DEADLOCK_DETECTION", "1", 1);\nDummyPythonGILHooks hooks;\nSetPythonGILHooks(&hooks);\nSetPythonGILHooks(&hooks);\n}\n#endif\n
using namespace c10::impl;\n\nstruct DummyPythonGILHooks : public PythonGILHooks {\nbool check_python_gil() const override {\nreturn true;\n}\n};\n\nTEST(DeadlockDetection, basic) {\nASSERT_FALSE(check_python_gil());\n
ContextMapping ctx_mapping(graph);\nAddRequiresGradOnOutputNodes(graph->block(), ctx_mapping);\n}\n} // anonymous namespace\n\nstd::vector<Node*> CreateAutodiffSubgraphs(\nconst std::shared_ptr<Graph>& graph,\n
std::unordered_map<const Node*, const Node*> node_to_ctx_;\n\nvoid processNode(Node* n) {\nnode_to_ctx_[n] = ctx_stack_.back();\n\nif (n->kind() == prim::Enter) {\nctx_stack_.push_back(n);\n} else if (n->kind() == prim::Exit) {\n
Node* begin() {\nreturn this->first;\n}\nNode* end() {\nreturn this->second;\n}\n};\n\nclass SubgraphSlicer {\npublic:\nSubgraphSlicer(\nBlock* block,\nstd::shared_ptr<Graph> graph,\nsize_t minSubgraphSize,\n
}\n}\n}\n\nvoid buildupSubgraphs() {\n// We need to run the slicer multiple times in order to get all merge\n// opportunities. This is because moveBeforeTopologicalValid may reorder\n// nodes to be AFTER the current iteration point. In order to properly\n
auto scale_vec = Vectorized<float>(scale);\nauto zero_point_vec = Vectorized<float>((float)zero_point);\nauto scale_neg_zp_premul_vec = scale_vec * zero_point_vec.neg();\nint64_t output_zero_point = zero_point;\n
_fake_quant_per_channel_cachemask_cpu_helper<scalar_t>(iter, iter_mask, quant_min, quant_max);\n});\n}\n\n\nvoid fake_quantize_learnable_channel_grad_kernel_cpu(\nTensorIterator& iter,\nint64_t quant_min,\n
output_zero_point);\n#else\nQuantizeAvx512<typename T::underlying>(\n(float*)acc_buffer_fp,\no_p + c,\ncend * vec_width,\nmultiplier,\noutput_zero_point);\n#endif\n}\nc_start = csize / vec_width * vec_width;\n
#if (defined(CPU_CAPABILITY_AVX2) || defined(CPU_CAPABILITY_AVX512)) && !defined(_MSC_VER)\nconstexpr auto vec_width = Vectorized<T>::size() / 4;\n#ifdef CPU_CAPABILITY_AVX2\nif (vec_width == 8) {\n#else\n
// Try to convert the key to an IValue.\ntry {\nkey_ivalue = toIValue(std::move(key), self->type()->getKeyType());\n} catch (const py::cast_error& e) {\nthrow py::type_error();\n}\n\n// If removed = false, that means the key didn't exist in the\n
"keys",\n[](const std::shared_ptr<ScriptDict>& self) { return self->iter(); },\npy::keep_alive<0, 1>()); // ScriptDict needs to be alive at least as\n// long as the iterator\n}\n\n} // namespace torch::jit\n
"__delitem__",\n[](const std::shared_ptr<ScriptDict>& self, py::object key) {\nIValue key_ivalue;\n\n// Try to convert the key to an IValue.\ntry {\nkey_ivalue = toIValue(std::move(key), self->type()->getKeyType());\n
throw py::stop_iteration();\n}\n\n// Since this is the iterator for .keys() and __iter__(), return only the key.\nIValue result = iter_->key();\n\n// Advance the iterator for next time.\niter_++;\n\nreturn result;\n
qnnpack_softmax_output_scale,\nqnnpack_softmax_output_zero_point,\nc10::nullopt);\n\nconst size_t channels = qx.size(dim);\nconst float input_scale = static_cast<float>(qx.q_scale());\nconst uint32_t flags = 0;\n
const size_t output_stride = channels;\n\ninitQNNPACK();\npytorch_qnnp_operator_t softargmax = nullptr;\n\npytorch_qnnp_status status = pytorch_qnnp_create_softargmax_nc_q8(\nchannels,\ninput_scale,\n
#ifdef USE_PYTORCH_QNNPACK\n\nconst static float qnnpack_softmax_output_scale = 0x1.0p-8f;\nconst static int qnnpack_softmax_output_zero_point = 0;\n\nbool is_qnnpack_compatible(\nconst Tensor& qx,\nconst double output_scale,\n
\n#ifdef USE_PYTORCH_QNNPACK\n#include <ATen/native/quantized/cpu/init_qnnpack.h>\n#include <ATen/native/quantized/cpu/QnnpackUtils.h>\n#include <caffe2/utils/threadpool/pthreadpool-cpp.h>\n#include <pytorch_qnnpack.h>\n
}\nCUevent_st* cuda_event_ptr{nullptr};\nTORCH_CUDA_CHECK(cudaEventCreate(&cuda_event_ptr));\n*event = std::shared_ptr<CUevent_st>(cuda_event_ptr, [](CUevent_st* ptr) {\nTORCH_CUDA_CHECK(cudaEventDestroy(ptr));\n
auto event = (const ProfilerEventStub*)(event_);\nauto event2 = (const ProfilerEventStub*)(event2_);\nTORCH_CUDA_CHECK(cudaEventSynchronize(event->get()));\nTORCH_CUDA_CHECK(cudaEventSynchronize(event2->get()));\n
#include <nvToolsExt.h>\n\n#include <c10/cuda/CUDAGuard.h>\n#include <c10/util/ApproximateClock.h>\n#include <c10/util/irange.h>\n#include <torch/csrc/profiler/stubs/base.h>\n#include <torch/csrc/profiler/util.h>\n
<< "is currently unsupported; potential workarounds are: "\n<< "(1) don't use the profiler in CUDA mode or (2) use num_workers=0 "\n<< "in the DataLoader or (3) Don't profile the data loading portion "\n
\n// indexing by tensors ("advanced" indexing)\nreturn THPVariable_Wrap(([&]() {\npybind11::gil_scoped_release no_gil;\nreturn at::indexing::dispatch_index(sliced, std::move(variableIndices));\n})());\n
}\nif (THPVariable_Check(index)) {\nreturn false;\n}\n//  Allow indexing with ndarray if numpy compilation is enabled. An ndarray\n//  index should not be treated as a tuple since the indexing has a different\n
}\nreturn THPVariable_Wrap(std::move(sliced));\n}\n\n// indexing by tensors ("advanced" indexing)\nreturn THPVariable_Wrap(([&]() {\npybind11::gil_scoped_release no_gil;\nreturn at::indexing::dispatch_index(sliced, std::move(variableIndices));\n
/*is_tracing=*/is_tracing,\nself_.device(),\nself_.ndimension(),\nspecified_dims);\nif (variableIndices.empty()) {\nif (sliced.is_same(self_)) {\n// ensure we return a shallow copy for things like x[...]\n
auto fut2 = at::intraop_launch_future([&v2](){\nv2 = 2;\n});\n\nfut1->wait();\nfut2->wait();\n\nASSERT_TRUE(v1 == 1 && v2 == 2);\n}\n
at::parallel_for(0, 10, 1, [&](int64_t begin, int64_t end) {\n// Nested parallel regions execute on a single thread\nASSERT_EQ(begin, 0);\nASSERT_EQ(end, 10);\n\n// Thread id reflects inner parallel region\n
TEST(TestParallel, IntraOpLaunchFuture) {\nint v1 = 0;\nint v2 = 0;\n\nauto fut1 = at::intraop_launch_future([&v1](){\nv1 = 1;\n});\n\nauto fut2 = at::intraop_launch_future([&v2](){\nv2 = 2;\n});\n\nfut1->wait();\n
\nTEST(TestParallel, IntraOpLaunchFuture) {\nint v1 = 0;\nint v2 = 0;\n\nauto fut1 = at::intraop_launch_future([&v1](){\nv1 = 1;\n});\n\nauto fut2 = at::intraop_launch_future([&v2](){\nv2 = 2;\n});\n\n
register_module(name, torch::nn::Linear(fc->options));\n}\n\nint64_t in_, out_;\ntorch::nn::Linear fc{nullptr};\ntorch::Tensor buffer;\n};\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\ntorch::python::bind_module<Net>(m, "Net")\n
fc = register_module("fc", torch::nn::Linear(in_, out_));\nbuffer = register_buffer("buf", torch::eye(5));\n}\n\ntorch::Tensor forward(torch::Tensor x) {\nreturn fc->forward(x);\n}\n\nvoid set_bias(torch::Tensor bias) {\n
torch::python::bind_module<Net>(m, "Net")\n.def(py::init<int64_t, int64_t>())\n.def("set_bias", &Net::set_bias)\n.def("get_bias", &Net::get_bias)\n.def("add_new_parameter", &Net::add_new_parameter)\n.def("add_new_buffer", &Net::add_new_buffer)\n
torch::python::bind_module<Net>(m, "Net")\n.def(py::init<int64_t, int64_t>())\n.def("set_bias", &Net::set_bias)\n.def("get_bias", &Net::get_bias)\n.def("add_new_parameter", &Net::add_new_parameter)\n.def("add_new_buffer", &Net::add_new_buffer)\n
fuseStaticSubgraphs(g, min_size);\n},\npy::arg("graph"),\npy::arg("min_size") = DEFAULT_FUSION_SIZE);\n}\n\n} // namespace torch::jit\n
.def(\n"_fuse_to_static_module",\n[](torch::jit::Module& module, size_t min_size) {\nmodule.eval();\nmodule = freeze_module(module);\n\nMethod method = module.get_method("forward");\nauto graph = method.graph();\n
kwarg_ivalues.reserve(kwargs.size());\nfor (const auto& arg : args) {\nauto ivalue = torch::jit::toIValue(arg, c10::AnyType::get());\narg_ivalues.push_back(std::move(ivalue));\n}\nfor (const auto& kv : kwargs) {\n
{arg_ivalues}, {kwarg_ivalues}, warmup_runs, main_runs);\n})\n.def(\n"runAsync",\n[](StaticModule& self,\nconst py::tuple& args,\nconst py::dict& kwargs) {\nstd::vector<c10::IValue> arg_ivalues;\narg_ivalues.reserve(args.size());\n
void XPUHooks::initXPU() const {\nC10_LOG_API_USAGE_ONCE("aten.init.xpu");\nconst auto device_count = c10::xpu::device_count_ensure_non_zero();\nc10::xpu::XPUCachingAllocator::init(device_count);\n}\n
}\n\nbool XPUHooks::hasXPU() const {\nreturn true;\n}\n\nstd::string XPUHooks::showConfig() const {\nreturn "XPU backend";\n}\n\nint32_t XPUHooks::getGlobalIdxFromDevice(const at::Device& device) const {\n
\nbool XPUHooks::hasXPU() const {\nreturn true;\n}\n\nstd::string XPUHooks::showConfig() const {\nreturn "XPU backend";\n}\n\nint32_t XPUHooks::getGlobalIdxFromDevice(const at::Device& device) const {\n
Device XPUHooks::getDeviceFromPtr(void* data) const {\nreturn at::xpu::getDeviceFromPtr(data);\n}\n\nc10::DeviceIndex XPUHooks::getNumGPUs() const {\nreturn at::xpu::device_count();\n}\n\nDeviceIndex XPUHooks::current_device() const {\n
);\nTORCH_INTERNAL_ASSERT(tensor_example_dim <= requested_example_dim);\n\nif (tensor_levels == requested_levels && tensor_example_dim == requested_example_dim) {\n// Optimization: no need to do another view if the physical tensor is\n
const Tensor& self,\nstd::bitset<kVmapNumLevels> requested_levels,\nint64_t requested_example_dim) {\nauto [physical_tensor, tensor_levels] = getPhysicalTensorAndLevels(self);\n\nTORCH_INTERNAL_ASSERT(\n
// This contains a list of regular (non-Batched) Tensors where all of the\n// batch dims have been moved to the front of the tensor. Any previously\n// non-existing batch dims get added to the tensors as new dimensions of size 1.\n
\n// Given a Tensor or a BatchedTensor, creates a physical view of the tensor\n// such that it has a batch dimension for each level in `requested_levels`\n// and `requested_example_dim` number of non-batch-dimensions.\n
if (opts_->prefer_ipv6()) {\nC10D_DEBUG("The server socket will attempt to listen on an IPv6 address.");\nif (tryListen(AF_INET6)) {\nreturn std::move(socket_);\n}\n\nC10D_DEBUG("The server socket will attempt to listen on an IPv4 address.");\n
} else {\nreturn ConnectResult::Success;\n}\n}\n\nvoid SocketConnectOp::throwTimeoutError() const {\nauto msg = fmt::format(\n"The client socket has timed out after {} while trying to connect to ({}, {}).",\n
#include <netdb.h>\n#include <netinet/tcp.h>\n#include <poll.h>\n#include <sys/socket.h>\n#include <sys/types.h>\n#include <unistd.h>\n#endif\n\nC10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED("-Wdeprecated")\n
// NOLINTNEXTLINE(bugprone-argument-comment)\nif (::listen(socket_->handle(), -1 /* backlog */) != 0) {\nrecordError(\n"The server socket has failed to listen on {} {}.",\naddr,\ngetSocketError());\n\n
at::Tensor add_tensor(at::Tensor z) {\nreturn (x + y) * z;\n}\nvoid increment(int64_t z) {\nthis->x += z;\nthis->y += z;\n}\nint64_t combine(c10::intrusive_ptr<Foo> b) {\nreturn this->info() + b->info();\n
input_names_ = std::move(input_names);\n}\n\n// Specify the output name for the function this interpreter represents. This\n// should match the "output" field of one of the instructions in the\n// instruction list, typically the last instruction.\n
x = scale * x + add;\nreturn x;\n}\nint64_t divide(std::optional<int64_t> factor) {\nif (factor) {\n// NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\nx = x / *factor;\n
} else {\nstd::stringstream err;\nerr << "Unknown operator " << op << "!";\nthrow std::runtime_error(err.str());\n}\n\n// Write back result into environment\nconst auto& output_name = std::get<2>(instr);\n
const AllToAllOptions& opts) {\n// alltoall supports uneven split, so don't enforce shape checking.\nrunCollectiveChecks(OpType::ALLTOALL, {});\nreturn backend_->alltoall(outputTensors, inputTensors, opts);\n
#include <c10/core/Allocator.h>\n#include <c10/core/DeviceType.h>\n#include <c10/core/ScalarType.h>\n#include <c10/core/TensorOptions.h>\n#include <c10/util/Exception.h>\n#include <c10/util/Optional.h>\n
\n// Serializes the information (op type, input shapes, data types, device\n// types) about the collective fingerprint into a tensor\nat::Tensor serialize_fingerprint() {\nauto data = std::make_unique<std::vector<int64_t>>();\n
", OpType=",\nop_type_str,\n", TensorShape=[",\nc10::Join(", ", size_strs),\n"], TensorDtypes=",\n(dtype_strs),\n", TensorDeviceTypes=",\n(device_type_strs),\n")");\n} else {\ncollectiveInfo = c10::str(\n
\nTEST(NamedTensorTest, alias) {\n// tensor.alias is not exposed in Python so we test its name propagation here\nauto N = dimnameFromString("N");\nauto C = dimnameFromString("C");\nstd::vector<Dimname> names = { N, C };\n
if (name.type() != other_name.type() || name.symbol() != other_name.symbol()) {\nreturn false;\n}\n}\nreturn true;\n}\n\nTEST(NamedTensorTest, attachMetadata) {\nauto tensor = at::zeros({3, 2, 5, 7});\n
auto H = dimnameFromString("H");\nauto W = dimnameFromString("W");\nstd::vector<Dimname> names = { N, C, H, W };\nat::internal_set_names_inplace(tensor, names);\nASSERT_TRUE(tensor.has_names());\n}\n\n
}\n\nTEST(NamedTensorTest, empty) {\nauto N = Dimname::fromSymbol(Symbol::dimname("N"));\nauto C = Dimname::fromSymbol(Symbol::dimname("C"));\nauto H = Dimname::fromSymbol(Symbol::dimname("H"));\nauto W = Dimname::fromSymbol(Symbol::dimname("W"));\n
TORCH_INTERNAL_ASSERT(\nbucket.future_work,\n"Expected bucket.future_work not to be null. "\n"This may indicate that communication hook was not properly installed.");\nbucket.future_work->wait();\nauto future_result = comm_hook_ == nullptr\n
buckets;\n\nfor (const auto i : c10::irange(tensors.size())) {\nconst auto& tensor = tensors[i];\nauto msg = std::string("No support for sparse tensors.");\nif (logger.has_value()) {\nREDUCER_CHECK(!tensor.is_sparse(), logger.value(), msg);\n
for (auto& bucket : buckets_) {\nbucket.gradients = bucket.gradients.to(dtype);\n}\n}\n\n// Right now delay_all_reduce is only called when static_graph_=true and\n// num_iterations_==1.\nvoid Reducer::delay_all_reduce() {\n
// these operations are implicitly sequenced, and we don't need to\n// do any extra synchronization here.\nconst auto& tensor = bucket.gradients;\n\n// TODO(@egienvalue): remove special case after view ops are fully\n
}\n\n// unify(A, A)\nif (name_ == other.name_) {\nreturn *this;\n}\n\n// unify(A, None)\nif (other.name_.isWildcard()) {\nconst auto it = std::find(other.origin_.begin(), other.origin_.end(), name_);\n
}\n}\n\n// Let's say the TensorName represents 'C' in ['N', 'C', 'H, 'W'].\n// It should print like:\n// 'C' (index 1 of ['N', 'C', 'H', 'W'])\nstd::ostream& operator<<(std::ostream& out, const TensorName& tensorname) {\n
TORCH_CHECK(it == other.origin_.end(),\nop_name, ":",\n" Cannot match ", *this, " with ", other,\n" because the latter names already have ", name_, ".",\n" Are your tensors misaligned?");\nreturn *this;\n
\n// unify(A, A)\nif (name_ == other.name_) {\nreturn *this;\n}\n\n// unify(A, None)\nif (other.name_.isWildcard()) {\nconst auto it = std::find(other.origin_.begin(), other.origin_.end(), name_);\nTORCH_CHECK(it == other.origin_.end(),\n
int numWorkers = 2; // thread 0 creates both server and client\n\n// server part\nc10d::TCPStoreOptions server_opts{\n0,\ntrue, // is master\nnumWorkers,\nfalse, // don't wait otherwise client thread won't spawn\n
TEST(TCPStoreTest, testMultiTenantStores) {\ntestMultiTenantStores(false);\n}\n\nTEST(TCPStoreTest, testMultiTenantStoresUV) {\ntestMultiTenantStores(true);\n}\n
clientStores.push_back(\nc10::make_intrusive<c10d::PrefixStore>(prefix, clientTCPStores[i]));\n}\n\nstd::string expectedCounterRes =\nstd::to_string(numThreads * numIterations + 1);\n\nfor (const auto i : c10::irange(numThreads)) {\n
numKeys = serverStore->getNumKeys();\nEXPECT_EQ(numKeys, 4);\nauto timeout = std::chrono::milliseconds(kShortStoreTimeoutMillis);\nserverStore->setTimeout(timeout);\nEXPECT_THROW(serverStore->get("key0"), c10::Error);\n
GRAPH_DEBUG(\n"Graph current scope after node process: ",\ngraph->current_scope()->namesFromRoot());\n}\n}\n\nScopePtr ONNXGraphTopLevelScope(Graph& graph) {\nif (graph.inputs().empty()) {\nreturn graph.current_scope();\n
std::string TidyClassNameFromTorchScript(\nconst std::optional<c10::QualifiedName>& class_name) {\nif (!class_name) {\nreturn "UNKNOWN_CLASS";\n}\nstd::string out = "";\nfor (const auto& atom : class_name->atoms()) {\n
if (!input_node_0->hasUses()) {\ninput_node_0->destroy();\n}\nNode* interpolate_node = block->owningGraph()->create(\nSymbol::fromQualString("aten::__interpolate"),\n{cur->inputs()},\ncur->outputs().size());\n
cur->removeAllInputs();\ncur->destroy();\nGRAPH_UPDATE(\n"ONNX function call substitution function: '",\nfun_type->function()->name(),\n"' to aten::__interpolate");\nGRAPH_UPDATE(\n"Function in ONNX function call substitution body: ",\n
bool unbiased = true, // correction=1 in version 2.0\nbool keepdim = false) {\nTORCH_CHECK(\nself_arg.dim() >= 2 && self_arg.dim() <= 4,\n"Vulkan var.dim_IntList only supports 2d, 3d, 4d tensors as input!");\n
} // namespace native\n} // namespace at\n
if (unbiased == true) {\noutput = output.mul(sample_size * 1.0 / (sample_size - 1));\n}\nreturn output;\n}\nreturn self;\n}\n\n#ifdef USE_VULKAN_API\n\nTORCH_LIBRARY_IMPL(aten, Vulkan, m) {\nm.impl(TORCH_SELECTIVE_NAME("aten::var.dim"), TORCH_FN(var_dim_IntList));\n
dims_set.insert(dim_normalized);\n\nsample_size *= self.sizes().vec()[dim_normalized];\n}\n\nat::Tensor self_mean = self.mean(opt_dim, true);\nat::Tensor self_minus_mean = self.sub(self_mean);\n// We write `self_minus_mean.mul(self_minus_mean)` instead of\n
/* The general implementation of the histogram kernel. Maps each element of the input tensor\n* to its corresponding bin by performing a binary search over the elements of bin_edges.\n*\n* Refer to histogramdd_out_cpu_template for more details.\n
}\n\nhist_index += hist_strides[dim] * pos;\n}\n\nif (!skip_elt) {\n// In the unweighted case, the default weight is 1\ninput_t wt = accessor_wt.has_value() ? accessor_wt.value()[i] : static_cast<input_t>(1);\n
\nif (algorithm == BINARY_SEARCH) {\n// Handles the general case via binary search on the bin edges.\npos = std::upper_bound(bin_seq[dim], bin_seq[dim] + num_bin_edges[dim], elt)\n- bin_seq[dim] - 1;\n
#include <ATen/ops/aminmax.h>\n#include <ATen/ops/sum.h>\n#include <ATen/ops/zeros.h>\n#include <ATen/ops/zeros_like_ops.h>\n#endif\n\n#include <algorithm>\n#include <numeric>\n#include <functional>\n
#include <torch/csrc/jit/ir/ir.h>\n#include <torch/csrc/jit/runtime/custom_operator.h>\n#include <torch/csrc/jit/runtime/interpreter.h>\n\n#include <stdexcept>\n\nnamespace torch {\nnamespace jit {\nnamespace fuser {\n
dim);\ndrop(stack, num_inputs);\npack(stack, std::move(result));\n};\n},\naliasAnalysisIsSpecialCase())});\n\nvoid runFallback(int64_t key, Stack& stack) {\nauto maybe_spec = retrieve(key);\nif (!maybe_spec)\n
#include <stdexcept>\n\nnamespace torch {\nnamespace jit {\nnamespace fuser {\n\nnamespace {\nc10::AliasAnalysisKind aliasAnalysisIsSpecialCase() {\nreturn AliasAnalysisKind::INTERNAL_SPECIAL_CASE;\n}\n
namespace jit {\nnamespace fuser {\n\nnamespace {\nc10::AliasAnalysisKind aliasAnalysisIsSpecialCase() {\nreturn AliasAnalysisKind::INTERNAL_SPECIAL_CASE;\n}\n} // namespace\n\n// Registers fused operators so that fused graphs can properly generate fallback\n
\ndef forward(self, x, y):\nreturn x + y\n~~~~~ <--- HERE\n)";\nASSERT_THROWS_WITH_MESSAGE(c_loaded.forward(inputs), error_pattern);\n}\n\n} // namespace jit\n} // namespace torch\n
*\n*    def forward(self, x, y):\n*      return self.B0.forward(x, y) + 3\n*             ~~~~~~~~~~~~~~~ <--- HERE\n*\n*  File "<string>", line 3, in FunctionName_UNKNOWN\n*\n*    def forward(self, x, y):\n
std::shared_ptr<CompilationUnit> cu = std::make_shared<CompilationUnit>();\nModule aa("AA");\naa.define(R"(\ndef forward(self, x, y):\nreturn x + y\n)");\nModule a("A");\na.register_module("AA0", aa);\n
compile_spec.insert("forward", fake_dict);\nauto any_dict_ty = DictType::create(StringType::get(), AnyType::get());\n// lowered module\nauto lm = torch::jit::detail::codegen_backend_module(\n"backend_with_compiler_demo", m, compile_spec, any_dict_ty);\n
}\n\nTensor mkldnn_relu_backward(const Tensor& grad_output, const Tensor& input, const Scalar& threshold) {\nideep::tensor& x = itensor_from_mkldnn(input);\nideep::tensor grady = itensor_from_mkldnn(grad_output);\n
ideep::eltwise_forward::compute(\nx, x, ideep::algorithm::eltwise_relu, ideep::prop_kind::forward_training, /*alpha*/ 0.0);\nreturn input;\n}\n\nTensor mkldnn_relu_backward(const Tensor& grad_output, const Tensor& input, const Scalar& threshold) {\n
#include <ATen/native/mkldnn/MKLDNNCommon.h>\n#include <ATen/native/mkldnn/Utils.h>\n\nnamespace at { namespace native {\n\nTensor mkldnn_relu(const Tensor& input) {\nif (input.scalar_type() == ScalarType::BFloat16) {\n
#ifndef AT_PER_OPERATOR_HEADERS\n#include <ATen/NativeFunctions.h>\n#else\n#include <ATen/ops/relu_native.h>                // for mkldnn_relu, mkldnn_...\n#include <ATen/ops/threshold_backward_native.h>  // for mkldnn_relu_backward\n
}\n\nstd::ostream& operator<<(std::ostream& os, DispatchKeySet ts) {\nif (ts.empty()) {\nos << "DispatchKeySet()";\nreturn os;\n}\nos << "DispatchKeySet(";\nbool first = true;\nfor (auto k : ts) {\nif (!first) {\n
// backend keys. for a non-autograd key, return the empty keyset.\nDispatchKeySet getBackendKeySetFromAutograd(DispatchKey t) {\nswitch (t) {\ncase DispatchKey::AutogradCPU:\nreturn DispatchKeySet(DispatchKey::CPU);\n
next_backend_ = first_backendcomponent_idx + 1;\n}\n} else {\n// Functionality bits that aren't per backend are simpler to handle. We can\n// ignore the backend bits.\nTORCH_INTERNAL_ASSERT(next_backend_ == 0);\n
{DispatchKey::AutogradNestedTensor, DispatchKey::NestedTensor}) |\nDispatchKeySet(DispatchKeySet::RAW, full_backend_mask);\n\nDispatchKeySet getRuntimeDispatchKeySet(DispatchKey t) {\nTORCH_INTERNAL_ASSERT(t != DispatchKey::Undefined);\n
transforms::Normalize<int>({0.5, 1.5, -1.5}, {0.1, 0.2, 0.2}));\noutput = dataset.get_batch(3);\nASSERT_EQ(output.size(), 1);\nASSERT_EQ(output[0].data.size(0), 3);\nASSERT_TRUE(output[0]\n.data.slice(/*dim=*/0, /*start=*/0, /*end=*/1)\n
}\n\nTEST(DataTest, OrderedSequencerIsSetUpWell) {\nusing namespace torch::data::detail::sequencers; // NOLINT\nstruct S {\nsize_t sequence_number;\n};\nconst size_t kMaxJobs = 5;\nOrderedSequencer<S> sequencer(kMaxJobs);\n
std::mutex mutex_;\n};\n\n// On the OrderingTest: This test is intended to verify that the\n// `enforce_ordering` option of the dataloader works correctly. The reason this\n// flag exists is because when the dataloader has multiple workers (threads)\n
}\n\nTEST(DataLoaderTest, StatefulDatasetWithNoWorkers) {\nconst int kNumberOfExamplesAfterWhichTheDatasetExhausts = 10;\n\nstruct D : datasets::StatefulDataset<D, int, size_t> {\ntorch::optional<int> get_batch(size_t) override {\n
// can reduce the memory use.\nif (ideep::algorithm::pooling_max == algo\n&& !((input.requires_grad() && at::GradMode::is_enabled()) || input._fw_grad(/*level */ 0).defined())) {\naprop_kind = ideep::prop_kind::forward_inference;\n
TORCH_CHECK(false, "mkldnn_max_pool2d: ATen not compiled with MKLDNN support");\n}\n\nTensor mkldnn_max_pool3d(\nconst Tensor& self,\nIntArrayRef kernel_size,\nIntArrayRef stride,\nIntArrayRef padding,\n
IntArrayRef stride,\nIntArrayRef padding,\nIntArrayRef dilation,\nbool ceil_mode) {\nreturn _mkldnn_pooling_backward(\ngrad_output,\noutput,\ninput,\nkernel_size,\nstride,\npadding,\ndilation,\nceil_mode,\n
Tensor& mkldnn_adaptive_avg_pool2d_out(const Tensor& input,\nIntArrayRef output_size,\nTensor& output) {\nTORCH_CHECK(false, "mkldnn_adaptive_avg_pool2d_out: ATen not compiled with MKLDNN support");\n
const int CONSUMED = -2;\n\n// Number of inter-op threads set by the user;\n// NOT_SET -> positive value -> CONSUMED\n// (CONSUMED - thread pool is initialized)\n// or\n// NOT_SET -> CONSUMED\nstd::atomic<int> num_interop_threads{NOT_SET};\n
ThreadPoolRegistry()->Create(\n"C10",\n/* device_id */ 0,\n/* pool_size */ num_interop_threads.exchange(CONSUMED),\n/* create_new */ true);\nreturn *pool;\n}\n\n// Factory function for ThreadPoolRegistry\n
TORCH_CHECK(nthreads > 0, "Expected positive number of threads");\n\nint no_value = NOT_SET;\nTORCH_CHECK(num_interop_threads.compare_exchange_strong(no_value, nthreads),\n"Error: cannot set number of interop threads after parallel work "\n
get_pool().run(std::move(fn));\n#endif\n}\n} // namespace internal\n\nvoid launch(std::function<void()> func) {\n// NOLINTNEXTLINE(modernize-avoid-bind)\ninternal::launch_no_thread_state(std::bind([](\n
api::utils::ivec4& input_tensor_dims) {\nif (num_dims == 1) {\nearly_exit.data[0u] = 1;\ninput_dim_stride.data[0u] = 1;\nshader_descriptor = VK_KERNEL(softmax_batch_height_width);\n} else if (num_dims == 2) {\n
shader_descriptor,\n// pipeline barrier\npipeline_barrier,\n// global work group size\nglobal_workgroup_extents,\n// local work group size\nadaptive_work_group_size(global_workgroup_extents),\n// fence handle\n
// pipeline barrier\npipeline_barrier,\n// global work group size\nglobal_workgroup_extents,\n// local work group size\nadaptive_work_group_size(global_workgroup_extents),\n// fence handle\nVK_NULL_HANDLE,\n
shader_descriptor = VK_KERNEL(softmax_batch_height_width);\n}\n}\n}\n\nTensor softmax_internal(\nconst at::Tensor& input_arg,\nconst int64_t dim_arg,\nconst bool half_to_float) {\nTORCH_CHECK(\ninput_arg.dim() >= 1 && input_arg.dim() <= 4,\n
\nx1 = x1.contiguous();\nx2 = x2.contiguous();\nauto cdist = _cdist.contiguous();\nauto grad = _grad.contiguous();\nint64_t n = x1.size(-2);\nint64_t m = x1.size(-1);\nauto device1 = x1.device().type();\n
*    the current method this is the only source of the floating point imprecision.\n* 3. Makes sure |cosing_similarity(x1, x2)| <= 1.0.\n*\n*/\n\nauto commonDtype = at::result_type(x1_, x2_);\nTORCH_CHECK(at::isFloatingType(commonDtype), "expected common dtype to be floating point, yet common dtype is ", commonDtype);\n
// Use x1.size() here and not the original size of _x1.size() as this gradient is not taking broadcasting into account\n// Broadcasting will be handled automatically by the autograd engine\nreturn grad_x1.view(x1.sizes());\n
std::vector<int64_t> tensor1_expand_size(expand_batch_portion);\ntensor1_expand_size.insert(tensor1_expand_size.end(), {r1, c1});\nstd::vector<int64_t> tensor2_expand_size(expand_batch_portion);\ntensor2_expand_size.insert(tensor2_expand_size.end(), {r2, c2});\n
{2263275., 2273076., 2282877.},\n{2312280., 2322081., 2331882.}}}}},\ntorch::kFloat);\nASSERT_TRUE(torch::allclose(y, expected));\n\nauto y_no_options = F::conv3d(x, weight);\nASSERT_TRUE(torch::allclose(y_no_options, expected));\n
auto input =\ntorch::arange(9, torch::kFloat).view(std::vector<int64_t>({1, 1, 3, 3}));\nauto grid = torch::tensor(\n{{{{-2., -1.}, {-1., -1.}, {0., -1.}},\n{{-1., 0.}, {0., 0.}, {1., 0.}},\n{{0., 1.}, {1., 1.}, {2., 1.}}}},\n
\nTEST_F(FunctionalTest, SoftMarginLossDefaultOptions) {\nauto input = torch::tensor(\n{2., 4., 1., 3.}, torch::dtype(torch::kFloat).requires_grad(true));\nauto target = torch::tensor({-1., 1., 1., -1.}, torch::kFloat);\n
{\nconst double scale = 1.0507009873554804934193349852946;\nconst double alpha = 1.6732632423543772848170429916717;\nfor (const auto inplace : {false, true}) {\nauto input = torch::randn({5, 5});\nauto input_bf16 = input.clone().to(torch::kBFloat16);\n
caffe2::TypeMeta::Make<float>(),\nat::Device(DeviceType::XLA, 0));\nat::Tensor t(std::move(tensor_impl));\nASSERT_TRUE(t.device() == at::Device(DeviceType::XLA, 0));\n}\n\nTEST(XlaTensorTest, test_allocator_clone) {\n
}\n};\n\nTEST(XlaTensorTest, TestNoStorage) {\nXLAAllocator allocator;\nauto tensor_impl = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(\nDispatchKey::XLA,\ncaffe2::TypeMeta::Make<float>(),\nat::Device(DeviceType::XLA, 0));\n
}\n\nvoid* XLAMalloc(ptrdiff_t size) {\n// NOLINTNEXTLINE(cppcoreguidelines-no-malloc)\nreturn malloc(size);\n}\n\nstruct XLAAllocator final : public at::Allocator {\nat::DataPtr allocate(size_t size) override {\n
default_copy_data(dest, src, count);\n}\n};\n\nTEST(XlaTensorTest, TestNoStorage) {\nXLAAllocator allocator;\nauto tensor_impl = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(\nDispatchKey::XLA,\n
cpu_kernel_vec(\niter,\n[=](scalar_t self_val, scalar_t t1_val, scalar_t t2_val) -> scalar_t {\nreturn float(self_val) + float_val * float(t1_val) / float(t2_val);\n},\n[=](Vectorized<scalar_t> self_vec,\n
const auto neg_1_vec = Vectorized<scalar_t>(-1);\nconst auto zero_vec = Vectorized<scalar_t>(0);\nconst auto pos_1_vec = Vectorized<scalar_t>(1);\ncpu_kernel_vec(iter,\n[=](scalar_t input, scalar_t target, scalar_t grad_output) -> scalar_t {\n
const auto x = input - target;\nif (x <= -delta) {\nreturn -norm_val * grad_output * delta;\n} else if (x >= delta) {\nreturn norm_val * grad_output * delta;\n} else {\nreturn norm_val * x * grad_output;\n
Vectorized<scalar_t> input, Vectorized<scalar_t> target, Vectorized<scalar_t> grad_output) -> Vectorized<scalar_t> {\n// using two blendv calls to simulate the 3 cases\n// delta     if  x >= delta\n// -delta    if x <= -delta\n
#include <cstring>\n#include <iostream>\n\nnamespace torch::jit {\nstatic std::ostream& operator<<(std::ostream& out, OpCode op) {\nswitch (op) {\n#define OP_STRING(x, _) \\ncase x:               \\nreturn out << #x;\n
}\n\nstatic constexpr size_t instruction_size = 8;\nstatic_assert(\nsizeof(Instruction) == instruction_size,\n"Instructions should be 8 bytes");\nstd::ostream& operator<<(std::ostream& out, Instruction inst) {\n
\nbool isOpSupportedInMobile(OpCode op) {\n// clang-format off\n// NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays)\nstatic constexpr OpCode supported_ops_in_mobile[] {\nOP, OPN, LOAD, MOVE, STOREN, STORE, DROP, DROPR, LOADC, JF, JMP, LOOP,\n
\nOpCode parseOpCode(const char* str) {\nconst int n = sizeof(strOpCode) / sizeof(strOpCode[0]);\nfor (const auto i : c10::irange(n)) {\nif (strcmp(strOpCode[i], str) == 0)\nreturn (OpCode)i;\n}\nreturn OP;\n
parseIR(graph_string, &*graph);\n\nauto x1 = at::rand({2, 2}, TensorOptions(kCPU).dtype(at::kFloat));\nauto q1 = at::quantize_per_tensor(x1, 0.1f, 13, at::kQUInt8);\nauto qs = at::sigmoid(q1);\nauto y_expected = at::dequantize(qs);\n
bool check = at::allclose(y_expected, y);\nif (!check) {\nstd::cout << "y_expected:\n" << y_expected << std::endl;\nstd::cout << "y:\n" << y << std::endl;\n}\nTORCH_CHECK_EQ(check, 1);\n}\n\nTEST_F(Quantization, QuantDequantUInt8) {\n
std::vector<IValue> stack = fmap<IValue>(inputs);\nk.run(stack);\nauto y = stack[0].toTensor();\nbool check = at::allclose(y_expected, y);\nif (!check) {\nstd::cout << "x1:\n" << x1 << std::endl;\nstd::cout << "q1:\n" << q1 << std::endl;\n
at::Tensor x1,\nat::Tensor x2,\ndouble scale,\nint64_t zero) {\nconst auto qadd_op =\nc10::Dispatcher::singleton()\n.findSchemaOrThrow("quantized::add", "")\n.typed<at::Tensor(at::Tensor, at::Tensor, double, int64_t)>();\n
}\n\ntmp_vjp_prev->replaceAllUsesWith(new_vjp);\nnew_vjp->node()->replaceInput(1, tmp_vjp_prev);\nGRAPH_DEBUG("grad_map[", tmp->debugName(), "] = ", *new_vjp->node());\ngrad_desc.df_input_vjps.emplace_back(i);\n
\nstatic void liftConstants(Block* block, Block* move_to_this_block) {\nfor (Node* node : block->nodes()) {\nliftConstants(node, move_to_this_block);\n}\nliftConstants(block->return_node(), move_to_this_block);\n
}\n}\n\nauto inputs = graph.inputs();\nfor (size_t i = 0, num_inputs = inputs.size(); i < num_inputs; ++i) {\nValue* input = inputs[i];\nif (!input->requires_grad())\ncontinue;\n// NB: Not having a gradient defined w.r.t. an input to the graph which\n
lifted_constant->output()->debugName(),\n" to the backprop block");\nnode->replaceInputWith(input, lifted_constant->output());\n}\nfor (Block* sub : node->blocks()) {\nliftConstants(sub, move_to_this_block);\n
#include <ATen/core/Tensor.h>\n#include <ATen/Dispatch.h>\n#include <ATen/Parallel.h>\n#include <ATen/native/UpSample.h>\n#include <ATen/native/cpu/utils.h>\n\n#ifndef AT_PER_OPERATOR_HEADERS\n#include <ATen/Functions.h>\n
\n// at::native functions for the native_functions.yaml\ntemplate <typename scalar_t, nn_compute_source_index_fn_t nn_compute_source_index_fn>\nstatic void upsample_nearest2d_out_frame(\nscalar_t* odata,\n
\nint64_t output_height = output_size[0];\nint64_t output_width = output_size[1];\n\nint64_t nbatch = input.size(0);\nint64_t channels = input.size(1);\nint64_t input_height = input.size(2);\nint64_t input_width = input.size(3);\n
std::optional<double> scales_w) {\nTORCH_CHECK(\noutput_size.size() == 2,\n"It is expected output_size equals to 2, but got size ",\noutput_size.size());\n\nTORCH_CHECK(\ninput.dim() == 4,\n"Non-empty 4D data tensor expected but got a tensor with sizes ",\n
indent();\nbody_ << "for " << useOf(stmt.currentTripCount()) << " in range("\n<< useOf(stmt.maxTripCount()) << "):\n";\n} else {\n// note: trip_count_in_block is unused because this is a while loop,\n
}\n}\n\n~WithSourceRange() {\nstack->pop_back();\n}\n\nSourceRangeStack* stack;\n};\n\nclass TaggedStringStream {\npublic:\nTaggedStringStream(const SourceRangeStack* srs) : srs_(srs) {}\n\nTaggedStringStream& operator<<(const std::string& s) {\n
}\nstmt << ")";\n} break;\ncase prim::CallMethod: {\nconst auto& self = node->inputs().at(0);\nconst auto& methodName = node->s(attr::name);\nstmt << "(" << useOf(self) << ")"\n<< "." << methodName << "(";\n
while (!stack.empty()) {\nIValue head = stack.back();\nstack.pop_back();\nif (head.isObject()) {\nresult.push_back(head);\nauto obj = head.toObject();\nClassTypePtr type = obj->type();\nif (type->hasMethod("__getstate__")) {\n
// (original_storage_size is only used for storage resizing in fsdp anyway, which does not apply to sparse)\n// Same for XLA\nif (base.unsafeGetTensorImpl()->has_storage() && base.device().type() != c10::DeviceType::XLA) {\n
// Each view inverse is implemented in ViewInverses.cpp.\nt = update.view_metas[i].reverse_fn(tmp_values[i], t, out_idx);\n}\nTORCH_INTERNAL_ASSERT(!at::functionalization::impl::isFunctionalTensor(t));\n
//\n// base = ...\n// a = base.view1()\n// b = a.view2()\n// c = b.view3()\n// c.add_(3)\n//\n// Then the functionalization pass will queue an update as follows:\n//\n// update.new_val = c  # the updated value of c\n
: c10::StorageImpl(\nc10::StorageImpl::use_byte_size_t(),\nget_nbytes(base),\nDataPtr{nullptr, base.device()},\nGetAllocator(kMeta),\n/*resizable=*/true\n),\nbase_(base)\n{\n// SparseTensorImpl has no storage, so we cannot query its nbytes.\n
} // namespace c10::impl\n
}\n\nvoid SetPythonGILHooks(PythonGILHooks* hooks) {\nif (disable_detection()) {\nreturn;\n}\nTORCH_INTERNAL_ASSERT(!hooks || !python_gil_hooks);\npython_gil_hooks = hooks;\n}\n\n} // namespace c10::impl\n
}\nTORCH_INTERNAL_ASSERT(!hooks || !python_gil_hooks);\npython_gil_hooks = hooks;\n}\n\n} // namespace c10::impl\n
return std::getenv("TORCH_DISABLE_DEADLOCK_DETECTION") != nullptr;\n}\n} // namespace\n\nbool check_python_gil() {\nif (!python_gil_hooks) {\nreturn false;\n}\nreturn python_gil_hooks->check_python_gil();\n
num_iterations_stats_recorded_) != std::end(LoggingIterations)) {\nat::LogPyTorchDDPUsage(*ddp_logging_data_);\n}\n}\n\nat::DDPLoggingData Logger::get_ddp_logging_data() {\nstd::lock_guard<std::mutex> lock(reducer_->mutex_);\n
std::stringstream ddpLoggingDataInfo;\nfor (const auto& intItem : ddp_logging_data_->ints_map) {\nddpLoggingDataInfo << intItem.first << ": " << intItem.second << "\n";\n}\nfor (const auto& strItem : ddp_logging_data_->strs_map) {\n
\nat::DDPLoggingData Logger::get_ddp_logging_data() {\nstd::lock_guard<std::mutex> lock(reducer_->mutex_);\nreturn *ddp_logging_data_;\n}\n\n// initialization of static variables in C10dLogger\nstd::unique_ptr<C10dLogger> C10dLogger::logger_ = nullptr;\n
\n#ifdef USE_C10D_GLOO\nauto gloo_pg = static_cast<c10d::ProcessGroupGloo*>(\nreducer_->process_group_\n->getBackend(c10d::ProcessGroup::BackendType::GLOO)\n.get());\nauto n_threads = gloo_pg->getNumThreads();\n
namespace torch {\nnamespace mps {\n\nbool is_available() {\nreturn at::detail::getMPSHooks().hasMPS();\n}\n\n/// Sets the seed for the MPS's default generator.\nvoid manual_seed(uint64_t seed) {\nif (is_available()) {\n
void synchronize() {\nat::detail::getMPSHooks().deviceSynchronize();\n}\n\nvoid commit() {\nat::detail::getMPSHooks().commitStream();\n}\n\nMTLCommandBuffer_t get_command_buffer() {\nreturn at::detail::getMPSHooks().getCommandBuffer();\n
{\n// See Note [Acquire lock when using random generators]\nstd::lock_guard<std::mutex> lock(gen.mutex());\ngen.set_current_seed(seed);\n}\n}\n}\n\nvoid synchronize() {\nat::detail::getMPSHooks().deviceSynchronize();\n
\nvoid synchronize() {\nat::detail::getMPSHooks().deviceSynchronize();\n}\n\nvoid commit() {\nat::detail::getMPSHooks().commitStream();\n}\n\nMTLCommandBuffer_t get_command_buffer() {\nreturn at::detail::getMPSHooks().getCommandBuffer();\n
case Tag::None:\nreturn DynamicTypeTrait<NoneType>::getBaseType();\ncase Tag::Tensor:\nreturn DynamicTypeTrait<TensorType>::getBaseType();\ncase Tag::Double:\nreturn DynamicTypeTrait<FloatType>::getBaseType();\n
return dyn;\n}\nreturn std::shared_ptr<const DynamicType>(new DynamicType{other});\n}\n\nDynamicTypePtr DynamicType::create(Type& other) {\nif (auto dynRaw = other.castRaw<DynamicType>()) {\nTORCH_INTERNAL_ASSERT(!dynRaw->weak_from_this().expired(),\n
\nbool contains(DynamicType::Tag lhs, DynamicTypeBits rhs) {\nreturn (static_cast<DynamicTypeBits>(lhs) | rhs) ==\nstatic_cast<DynamicTypeBits>(lhs);\n}\n\nbool contains(DynamicType::Tag lhs, DynamicType::Tag rhs) {\n
}\n}\n\nbool DynamicType::equals(const Type& rhs) const {\nreturn equals(*create(rhs));\n}\n\nbool DynamicType::isSubtypeOfExt(const Type& rhs, std::ostream*) const {\nauto other = create(rhs);\nif (tag_ == other->tag_) {\n
\nvoid GraphTask::mark_as_completed_and_run_post_processing() {\n// Allow only one thread one attempt to process this logic.\nif (future_completed_.exchange(true)) {\n// Future is already marked complete, or being marked as such.\n
graph_task->owner_ = worker_device;\n\n// Now that all the non-thread safe fields of the graph_task have been\n// populated, we can enqueue it.\nqueue->push(\nNodeTask(graph_task, std::move(graph_root), std::move(input_buffer)));\n
--thread_pool_shared_->num_workers_;\nauto task = tp_shared->graphtasks_queue_.front();\ntp_shared->graphtasks_queue_.pop();\nlk.unlock();\nstd::shared_ptr<GraphTask> graph_task = task.lock();\nif (!graph_task) {\n
\n// Queue the root\nif (skip_dummy_node) {\nInputBuffer input_buffer(root_edges.at(0).function->num_inputs());\nauto input = inputs.at(0);\n\nconst auto input_stream = InputMetadata(input).stream();\n
model->to(torch::kCUDA);\n}\n\n// transformer with customized encoder/decoder\nLayerNorm enorm(LayerNormOptions({4}));\nTransformerEncoder encoder(\nTransformerEncoderOptions(\nTransformerEncoderLayerOptions(4, 2).dim_feedforward(16).dropout(0.0),\n
mask[1][4] = 1;\nresult = model(\nencoder_input,\n/*src_mask=*/torch::Tensor{},\n/*src_key_padding_mask=*/mask)\n.detach();\nref_output = torch::tensor(\n{{{2.429026, 0.020793, -0.601741, -0.085642},\n
torch::Tensor decoder_input =\ntorch::tensor({{{20, 30, 40, 50}}}, tensor_options);\ntorch::Tensor memory_input =\ntorch::tensor({{{60, 70, 80, 90}}}, tensor_options);\ntorch::Tensor result = model(decoder_input, memory_input).detach();\n
{{0.8335, 0.2799, 0.5031, 0.2947}, {0.1402, 0.0318, 0.7636, 0.1346}},\n{{0.6333, 0.9344, 0.1376, 0.9938}, {0.8924, 0.2872, 0.6692, 0.2944}},\n{{0.9897, 0.6915, 0.3154, 0.1733}, {0.8645, 0.3513, 0.3064, 0.0767}},\n
#include <ATen/NativeFunctions.h>\n#else\n#include <ATen/ops/_amp_foreach_non_finite_check_and_unscale.h>\n#include <ATen/ops/_amp_foreach_non_finite_check_and_unscale_native.h>\n#include <ATen/ops/_amp_update_scale.h>\n
\nat::Tensor& _amp_update_scale_cpu_ (\nat::Tensor& current_scale,\nat::Tensor& growth_tracker,\nconst at::Tensor& found_inf,\ndouble growth_factor,\ndouble backoff_factor,\nint64_t growth_interval) {\n
at::Tensor& found_inf,\nconst at::Tensor& inv_scale) {\n_amp_foreach_non_finite_check_and_unscale_cpu_stub(\nfound_inf.device().type(), scaled_grads, found_inf, inv_scale);\n}\n\nat::Tensor& _amp_update_scale_cpu_ (\n
DEFINE_DISPATCH(_amp_foreach_non_finite_check_and_unscale_cpu_stub);\nDEFINE_DISPATCH(_amp_update_scale_cpu_stub);\n\n} // namespace at::native\n
#include "torch/csrc/autograd/utils/wrap_outputs.h"\n#include "torch/csrc/autograd/utils/python_arg_parsing.h"\n#include "torch/csrc/utils/pycfunction_helpers.h"\n#include "torch/csrc/utils/python_arg_parser.h"\n
#include "torch/csrc/DynamicTypes.h"\n#include "torch/csrc/Exceptions.h"\n#include "torch/csrc/autograd/python_linalg_functions.h"\n#include "torch/csrc/autograd/generated/python_return_types.h"\n#include "torch/csrc/autograd/python_variable.h"\n
}\n\n// generated methods start here\n\n${py_methods}\n\n} // namespace torch::autograd\n
\n#include "torch/csrc/Device.h"\n#include "torch/csrc/DynamicTypes.h"\n#include "torch/csrc/Exceptions.h"\n#include "torch/csrc/autograd/python_linalg_functions.h"\n#include "torch/csrc/autograd/generated/python_return_types.h"\n
const auto relu_int_repr = at::where(mask, int_repr, zero_point);\nreturn at::_make_per_tensor_quantized_tensor(relu_int_repr, self.q_scale(), zero_point);\n}\n\n}  // namespace at::native\n}  // namespace at\n
Tensor gelu_quantized_cuda(const Tensor& qx, c10::string_view approximate) {\n(void)approximate; // suppress unused variable lint warning\nif (qx.numel() == 0) {\nreturn Tensor{};\n}\nauto x_fp32 = at::dequantize(qx);\n
auto zero_point = self.q_zero_point();\nauto int_repr = self.int_repr();\nauto mask = (int_repr > zero_point);\nconst auto relu_int_repr = at::where(mask, int_repr, zero_point);\nreturn at::_make_per_tensor_quantized_tensor(relu_int_repr, self.q_scale(), zero_point);\n
}  // namespace at::native\n}  // namespace at\n
namespace torch::jit {\n\nnamespace {\nsize_t hashType(const Type& type) {\nif (auto named_type = type.castRaw<ClassType>()) {\nreturn c10::get_hash(\nnamed_type->name().value(), named_type->compilation_unit());\n
size_t HashType::operator()(const TypePtr& type) const {\nreturn hashType(*type);\n}\n\nsize_t HashType::operator()(const c10::ConstTypePtr& type) const {\nreturn hashType(*type);\n}\n\nbool EqualType::operator()(const TypePtr& a, const TypePtr& b) const {\n
#include <torch/csrc/jit/ir/ir.h>\n\nnamespace torch::jit {\n\nnamespace {\nsize_t hashType(const Type& type) {\nif (auto named_type = type.castRaw<ClassType>()) {\nreturn c10::get_hash(\nnamed_type->name().value(), named_type->compilation_unit());\n
size_t hash = 0;\nfor (const auto& containedType : type.containedTypes()) {\nhash = at::hash_combine(hash, hashType(*containedType));\n}\nhash = at::hash_combine(hash, get_hash(type.kind()));\nreturn hash;\n
is_script,\nopset_version);\n}))\n.def(\n"_jit_pass_onnx_function_substitution",\nwrap_pybind_function(ONNXFunctionCallSubstitution))\n.def(\n"_jit_pass_onnx_autograd_function_process",\nwrap_pybind_function(ONNXAutogradFunctionProcess))\n
.value("INT8", ::ONNX_NAMESPACE::TensorProto_DataType_INT8)\n.value("UINT16", ::ONNX_NAMESPACE::TensorProto_DataType_UINT16)\n.value("INT16", ::ONNX_NAMESPACE::TensorProto_DataType_INT16)\n.value("INT32", ::ONNX_NAMESPACE::TensorProto_DataType_INT32)\n
} // namespace torch::onnx\n
}),\npy::arg("proto_string"));\n\nauto onnx = m.def_submodule("_onnx");\npy::enum_<::ONNX_NAMESPACE::TensorProto_DataType>(onnx, "TensorProtoDataType")\n.value("UNDEFINED", ::ONNX_NAMESPACE::TensorProto_DataType_UNDEFINED)\n
KernelRuntimeContext context{};\nop(context, kernel_values);\nat::Tensor expected = at::ones({2, 3});\nexpected = at::fill(expected, 2);\nASSERT_TRUE(expected.equal(kernel_values[3]->toTensor()));\n\n
values[0] = EValue(at::ones({2, 3}));\nvalues[1] = EValue(at::ones({2, 3}));\nvalues[2] = EValue(at::ones({2, 3}));\nvalues[3] = EValue(at::zeros({2, 3}));\nASSERT_TRUE(hasKernelFn("custom::add_3.out"));\n
\n// add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)\nTEST(OperatorRegistrationTest, Add) {\nEValue values[4];\nvalues[0] = EValue(at::ones({2, 3}));\nvalues[1] = EValue(at::ones({2, 3}));\n
\n// add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)\nTEST(OperatorRegistrationTest, Add) {\nEValue values[4];\nvalues[0] = EValue(at::ones({2, 3}));\nvalues[1] = EValue(at::ones({2, 3}));\n
for (auto dk1: DispatchKeySet(DispatchKeySet::FULL)) {\nauto dks = getRuntimeDispatchKeySet(dk1);\nfor (auto dk2: DispatchKeySet(DispatchKeySet::FULL)) {\nASSERT_EQ(dks.has(dk2), runtimeDispatchKeySetHas(dk1, dk2));\n
using at::DispatchKey;\nusing at::DispatchKeySet;\n\nTEST(DispatchKeySetTest, TestGetRuntimeDispatchKeySet) {\n// Check if getRuntimeDispatchKeySet and runtimeDispatchKeySetHas agree.\nfor (auto dk1: DispatchKeySet(DispatchKeySet::FULL)) {\n
for (auto dk2: DispatchKeySet(DispatchKeySet::FULL)) {\nASSERT_EQ(dks.has(dk2), runtimeDispatchKeySetHas(dk1, dk2));\n}\n}\n}\n
ASSERT_EQ(dks.has(dk2), runtimeDispatchKeySetHas(dk1, dk2));\n}\n}\n}\n
int64_t num_layers)\n: decoder_layer_(std::move(decoder_layer)), num_layers_(num_layers) {}\n\nTransformerDecoderOptions::TransformerDecoderOptions(\nconst TransformerDecoderLayerOptions& decoder_layer_options,\n
int64_t num_layers)\n: encoder_layer_(std::move(encoder_layer)), num_layers_(num_layers) {}\n\nTransformerEncoderOptions::TransformerEncoderOptions(\nconst TransformerEncoderLayerOptions& encoder_layer_options,\n
\nTransformerEncoderOptions::TransformerEncoderOptions(\nconst TransformerEncoderLayerOptions& encoder_layer_options,\nint64_t num_layers)\n: encoder_layer_(encoder_layer_options), num_layers_(num_layers) {}\n
\nTransformerEncoderLayerOptions::TransformerEncoderLayerOptions(\nint64_t d_model,\nint64_t nhead)\n: d_model_(d_model), nhead_(nhead) {}\n\nTransformerDecoderLayerOptions::TransformerDecoderLayerOptions(\n
using at::DeviceGuard;\nusing at::TensorOptions;\nusing at::IntArrayRef;\nusing at::Generator;\nusing at::TensorList;\nusing at::Dimname;\nusing at::DimnameList;\n\nusing torch::utils::check_out_type_matches;\n
THPSpecialVariableFunctionsModule = special;\nif (!special) {\nthrow python_error();\n}\n// steals a reference to special\nif (PyModule_AddObject(module, "_special", special) != 0) {\nthrow python_error();\n
// generated forward declarations start here\n\n${py_forwards}\n\nstatic PyMethodDef special_functions[] = {\n${py_method_defs}\n{NULL}\n};\n\nstatic PyObject* THPSpecialVariableFunctionsModule = NULL;\n
// generated methods start here\n\n${py_methods}\n\n} // namespace torch::autograd\n
}\n});\nreturn qy;\n}\n\ntemplate <bool ReLUFused>\nTensor quantized_cat_impl(\nITensorListRef qxs,\nint64_t dim,\ndouble scale,\nint64_t zero_point) {\nreturn quantized_cat_impl<ReLUFused>(qxs.materialize(), dim, scale, zero_point);\n
"Only per-tensor quantization is supported in 'cat'!")\ndouble _scale = scale.has_value() ? scale.value() : qxs.get(0).q_scale();\nint64_t _zero_point =\nzero_point.has_value() ? zero_point.value() : qxs.get(0).q_zero_point();\n
auto out_ = quantized_cat_impl<false>(qxs, dim, out.q_scale(), out.q_zero_point());\nat::native::copy_(out, out_, /*non_blocking=*/false);\nreturn out;\n}\n\n}  // namespace native\n}  // namespace at\n
m.impl(TORCH_SELECTIVE_NAME("quantized::cat_relu_out"), TORCH_FN(qcat_out<true>));\n}\n\nTensor cat_quantized_cpu(const ITensorListRef& qxs, int64_t dim) {\nauto materialized = qxs.materialize();\nTORCH_CHECK(is_valid_quantization_scheme(materialized[0]),\n
\nstd::optional<BackendDevice> GetBackendDevice() {\nreturn c10::nullopt;\n}\n\n} // namespace lazy\n} // namespace torch\n
std::optional<BackendDevice> GetBackendDevice(const at::Tensor& tensor) {\nif (auto lt = TryGetLtcTensor(tensor)) {\nreturn lt->GetDevice();\n}\nreturn c10::nullopt;\n}\n\nstd::optional<BackendDevice> GetBackendDevice(\n
\nstd::optional<BackendDevice> GetBackendDevice(\nconst std::optional<c10::Device>& device) {\nif (device) {\nreturn c10::make_optional(atenDeviceToBackendDevice(*device));\n}\nreturn c10::nullopt;\n}\n
const std::optional<c10::Device>& device) {\nif (device) {\nreturn c10::make_optional(atenDeviceToBackendDevice(*device));\n}\nreturn c10::nullopt;\n}\n\nstd::optional<BackendDevice> GetBackendDevice() {\n
ASSERT_TRUE(torch::allclose(\npacked_enforce_sorted.batch_sizes(), torch::tensor({3, 2, 1})));\nASSERT_TRUE(torch::allclose(packed_enforce_sorted.data(), expected));\nASSERT_FALSE(packed_enforce_sorted.sorted_indices().defined());\n
sequences, torch::tensor(lengths_vec), batch_first, enforce_sorted);\n}\n_compatibility_test(\nunsorted_sequences,\ntorch::tensor(unsorted_sequences_lengths_vec),\nbatch_first);\n}\n}\n}\n\nTEST_F(NNUtilsTest, PackPaddedSequence) {\n
}\nreturn total_norm;\n}\n};\nauto compare_scaling =\n[&](const std::vector<torch::Tensor>& grads) -> torch::Tensor {\nstd::vector<torch::Tensor> p_scale;\nfor (const auto i : c10::irange(grads.size())) {\n
ASSERT_TRUE(a.data().is_same(b.data()));\nASSERT_TRUE(a.batch_sizes().is_same(b.batch_sizes()));\nASSERT_TRUE(a.sorted_indices().is_same(b.sorted_indices()));\nASSERT_TRUE(a.unsorted_indices().is_same(b.unsorted_indices()));\n
\n\ntorch::Tensor noop_cusolver_function(torch::Tensor x) {\ncusolverDnHandle_t handle;\nTORCH_CUSOLVER_CHECK(cusolverDnCreate(&handle));\nTORCH_CUSOLVER_CHECK(cusolverDnDestroy(handle));\nreturn x;\n
\n\ntorch::Tensor noop_cusolver_function(torch::Tensor x) {\ncusolverDnHandle_t handle;\nTORCH_CUSOLVER_CHECK(cusolverDnCreate(&handle));\nTORCH_CUSOLVER_CHECK(cusolverDnDestroy(handle));\nreturn x;\n
\ntorch::Tensor noop_cusolver_function(torch::Tensor x) {\ncusolverDnHandle_t handle;\nTORCH_CUSOLVER_CHECK(cusolverDnCreate(&handle));\nTORCH_CUSOLVER_CHECK(cusolverDnDestroy(handle));\nreturn x;\n}\n
return x;\n}\n\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\nm.def("noop_cusolver_function", &noop_cusolver_function, "a cusolver function");\n}\n
namespace {\nJitDecompInterface* impl = nullptr;\n}\n\nvoid setJitDecompImpl(JitDecompInterface* impl_) {\nimpl = impl_;\n}\n\nJitDecompInterface* getJitDecompImpl() {\nreturn impl;\n}\n\n} // namespace impl\n
\nnamespace torch {\nnamespace autograd {\nnamespace impl {\n\nnamespace {\nJitDecompInterface* impl = nullptr;\n}\n\nvoid setJitDecompImpl(JitDecompInterface* impl_) {\nimpl = impl_;\n}\n\nJitDecompInterface* getJitDecompImpl() {\n
\nnamespace torch {\nnamespace autograd {\nnamespace impl {\n\nnamespace {\nJitDecompInterface* impl = nullptr;\n}\n\nvoid setJitDecompImpl(JitDecompInterface* impl_) {\nimpl = impl_;\n}\n\nJitDecompInterface* getJitDecompImpl() {\n
}\n\nvoid setJitDecompImpl(JitDecompInterface* impl_) {\nimpl = impl_;\n}\n\nJitDecompInterface* getJitDecompImpl() {\nreturn impl;\n}\n\n} // namespace impl\n} // namespace autograd\n} // namespace torch\n
std::unordered_map<std::string, Value*> vmap;\ntorch::jit::parseIR(graph_string, graph.get(), vmap);\nauto get_name = invert_map(vmap);\n\nstd::vector<std::string> result;\nDepthFirstGraphNodeIterator graph_it(graph);\n
%2 : int = prim::Constant[value=10]()\n%3 : int = prim::Constant[value=20]()\n%4 : int = prim::Constant[value=30]()\n%5 : int = prim::Constant[value=40]()\n%6 : bool = aten::Bool(%a.1)\n%7 : int = prim::If(%6)\n
"%7 : Tensor = aten::add_(%b.10, %3, %3)",\n"%8 : Tensor = prim::Loop(%2, %1, %a.1)"});\n}\n\n} // namespace jit\n} // namespace torch\n
"%3 : bool = aten::Bool(%1)",\n"%4 : int = prim::If(%3)",\n"%5 : int = prim::Constant[value=20]()"});\n}\n\nTEST(GraphIteratorTest, GraphWithNestedIf) {\nconst auto graph_string = R"IR(\ngraph(%a.1 : Tensor,\n
kernel_width,\nstride_height,\nstride_width,\npad_height,\npad_width,\nuse_channels_last);\n}\n});\n});\n}\n\ntemplate <typename scalar_t>\nvoid slow_conv2d_backward_weight_frame(\nTensorAccessor<scalar_t, 2> grad_weight,\n
if (output_mask[1]) {\ngrad_weight = at::empty({0}, grad_output.options());\n}\n\nif (output_mask[2]) {\ngrad_bias = at::empty({0}, grad_output.options());\n}\n\nat::native::slow_conv2d_backward_out_cpu(\n
\nreturn columns.contiguous();\n}\n\nstatic inline void slow_conv2d_shape_check(\nconst Tensor& input,\nconst Tensor& grad_output,\nconst Tensor& weight,\nconst Tensor& bias,\nint64_t kernel_height,\n
\nif (grad_output.defined()) {\nif (weight.defined()) {\nint64_t n_output_plane = weight.size(0);\ncheck_dim_size(grad_output, ndim, dim_planes, n_output_plane);\n} else if (bias.defined()) {\nTORCH_CHECK(bias.numel() > 0, "non-empty bias tensor expected");\n
} else if (q_scheme == c10::kPerChannelAffine) {\n// Process the per channel quantization.\n//\n// After the uint8 * int8 matrix multiplication is performed, this\n// operation does:\n//  1) Add in row and column offsets to the rows and columns,\n
//  respectively.\n//  2) Add in the bias term.\nfbgemm::ReQuantizeForFloat<\nReluFused,\nfbgemm::QuantizationGranularity::OUT_CHANNEL>\noutputProcObj(\ndoNothingObj,\ninput_scale_float,\nw_scale.data(),\n
fbgemm::QuantizationGranularity::OUT_CHANNEL>\noutputProcObj(\ndoNothingObj,\ninput_scale_float,\nw_scale.data(),\ninput_zero_point_int32,\nw_zp.data(),\npackA.getRowOffsetBuffer(),\ncol_offsets.data(),\n
auto packed_weight = at::native::itensor_from_mkldnn(onednn_weight);\nint64_t K = input.size(dim - 1), M = input.numel() / K, N = packed_weight.get_dim(1);\n\nauto output_size = input.sizes().vec();\n
pass\ndef forward(self, x: Tensor) -> Tensor:\npass\n)JIT";\nauto interfaceType2 = importType(cu2, "__torch__.OneForward", interfaceSrc2);\n\nEXPECT_NE(*interfaceType, *interfaceType2);\n}\n\nTEST(TypeEquality, TupleEquality) {\n
\nauto differentName = TupleType::createNamed(\n"WowSoDifferent",\nfields,\n{IntType::get(), TensorType::get(), FloatType::get(), ComplexType::get()});\nEXPECT_NE(*type, *differentName);\n\nauto differentField = TupleType::createNamed(\n
auto classType2 = cu->get_type("__torch__.First");\n// Trivially these should be equal\nEXPECT_EQ(*classType, *classType2);\n}\n\nTEST(TypeEquality, ClassInequality) {\n// Even if classes have the same name across two compilation units, they\n
"my.named.tuple", field_names, {type, IntType::get()});\nEXPECT_EQ(namedTupleType->annotation_str(printer), "Rewritten");\n\n// Put it inside another tuple, should still work\nconst auto outerTupleType = TupleType::create({IntType::get(), namedTupleType});\n
#else\n#include <ATen/ops/addcdiv_native.h>\n#include <ATen/ops/addcmul_native.h>\n#endif\n\nnamespace at::meta {\n\nTORCH_META_FUNC(addcmul)\n(const Tensor& self,\nconst Tensor& tensor1,\nconst Tensor& tensor2,\n
"Integer division with addcdiv is no longer supported, and in a future  ",\n"release addcdiv will perform a true division of tensor1 and tensor2. ",\n"The historic addcdiv behavior can be implemented as ",\n
} // namespace at::native\n
\nTORCH_META_FUNC(addcmul)\n(const Tensor& self,\nconst Tensor& tensor1,\nconst Tensor& tensor2,\nconst Scalar& value) {\nbuild_ternary_op(maybe_get_output(), self, tensor1, tensor2);\n}\n\nTORCH_META_FUNC(addcdiv)\n
{'*', 10},\n};\n\nbool SharedParserData::isUnary(int kind, int* prec) {\nauto it = unary_prec.find(kind);\nif (it != unary_prec.end()) {\n*prec = it->second;\nreturn true;\n}\nreturn false;\n}\nbool SharedParserData::isBinary(int kind, int* prec) {\n
#undef DEFINE_CASE\ndefault:\nthrow std::runtime_error("Unknown kind: " + std::to_string(kind));\n}\n}\n\nC10_EXPORT SharedParserData& sharedParserData() {\nstatic SharedParserData data; // safely handles multi-threaded init\n
if (it != unary_prec.end()) {\n*prec = it->second;\nreturn true;\n}\nreturn false;\n}\nbool SharedParserData::isBinary(int kind, int* prec) {\nauto it = binary_prec.find(kind);\nif (it != binary_prec.end()) {\n
\nnamespace torch::jit {\n\nstatic const std::unordered_map<int, int> binary_prec = {\n{TK_IF, 1},\n{TK_FOR, 1},\n{TK_AND, 2},\n{TK_OR, 2},\n// reserve a level for unary not\n{TK_IN, 4},\n{TK_NOTIN, 4},\n
if (!result.is_contiguous()) {\nout = result.contiguous();\n}\nif (sorted_sequence.is_contiguous() && self.is_contiguous() && sorted_sequence.dtype() == self.dtype() && sorter.is_contiguous()) {\ndispatch(out, self, sorted_sequence, out_int32, is_right, sorter);\n
ScalarType::Half,\nScalarType::BFloat16,\ninput.scalar_type(),\n"searchsorted_out_cpu",\n[&] {\nsearchsorted_cpu_contiguous<scalar_t, int64_t>(\nresult, input, boundaries, right, sorter);\n});\n}\nelse {\n
const Tensor& self,\nbool out_int32,\nbool right,\nconst std::optional<c10::string_view> side_opt,\nconst std::optional<Tensor>& sorter_opt) {\nScalarType scalar_type = out_int32 ? ScalarType::Int : ScalarType::Long;\n
\nTensor& bucketize_out_cpu(const Tensor& self, const Tensor& boundaries, bool out_int32, bool right, Tensor& result) {\nTORCH_CHECK(boundaries.dim() == 1, "boundaries tensor must be 1 dimension, but got dim(", boundaries.dim(), ")");\n
#include <ATen/NativeFunctions.h>\n#else\n#include <ATen/ops/eq.h>\n#include <ATen/ops/eq_native.h>\n#include <ATen/ops/ge.h>\n#include <ATen/ops/ge_native.h>\n#include <ATen/ops/gt.h>\n#include <ATen/ops/gt_native.h>\n
#endif\n\nnamespace at {\nnamespace native {\n\n/*\nAll comparator operators will be named "<aten op name>_quantized_cpu".\n'_out' will be appended for the 'out' variant of the op.\n\nTODO: This is an inefficient implementation that uses `.dequantize`.\n
#ifndef AT_PER_OPERATOR_HEADERS\n#include <ATen/Functions.h>\n#include <ATen/NativeFunctions.h>\n#else\n#include <ATen/ops/eq.h>\n#include <ATen/ops/eq_native.h>\n#include <ATen/ops/ge.h>\n#include <ATen/ops/ge_native.h>\n
// Nondeterministic because if storage is resized, new elements are uninitialized\nglobalContext().alertNotDeterministic("quantized_resize_cpu_");\nTORCH_CHECK(\n!optional_memory_format.has_value(),\n
auto oo = n->outputs()[i];\nauto no = copy->outputs()[i];\nno->copyMetadata(oo);\nGRAPH_DEBUG("Mapping %", oo->debugName(), " to %", no->debugName());\nlocal_map[oo] = no;\n}\n\nsubgraphNode->owningGraph()->insertNode(copy);\n
std::optional<Node*> existing,\nAliasDb& db,\nconst std::function<Node*(void)>& merge_fn) {\n// When we merge a node into a subgraph, the new subgraph outputs\n// have the same aliasing properties as the original node's outputs.\n
const auto hasUsesOutsideSubgraph = [&](Value* v) {\nreturn std::any_of(\nv->uses().cbegin(), v->uses().cend(), [&](const Use& use) {\nreturn use.user->isAfter(subgraphNode);\n});\n};\n\nfor (int64_t i = subgraphNode->outputs().size() - 1; i >= 0; i--) {\n
orderedSeenValues.insert(closedValue);\n}\n}\n\nfor (auto input : orderedClosedValues) {\nif (externalValuesMap.count(input) == 0) {\n// Clone constants inside the subgraph instead of referencing them, to\n
return node;\n}\n}\n}\n\n// Ideally, there will be only one ATen operator that has tensor outputs in\n// the graph. Let's use that as the last resolve to make checkAliasAnnotation\n// more robust.\nfor (const auto* node : g.nodes()) {\n
const Node* findNodeForOp(\nconst Graph& g,\nconst std::string& unqualifiedOpName) {\nconst auto opName = Symbol::fromQualString("aten::" + unqualifiedOpName);\nfor (const auto* node : g.nodes()) {\nif (node->kind() == opName) {\n
#include <c10/util/irange.h>\n\nnamespace torch {\nnamespace jit {\nnamespace {\n\nIValue deepCopy(const IValue& self) {\n// primitive types can be copied directly\nif (!self.isPtrType()) {\nreturn self;\n
auto inputValue = toIValue(input);\nif (!inputValue) {\ninputValue = toIValueProp(input);\n}\n\nif (inputValue) {\npush(stack, *inputValue);\n} else {\nAT_ASSERT(input->type()->kind() == TypeKind::OptionalType);\n
if (sizes[i] != 1 && strides[i] != expected_stride) {\ncontig_if_nonempty = false;\n}\nexpected_stride *= sizes[i];\n}\n}\nreturn contig_if_nonempty;\n}\n\nbool geometry_is_contiguous(IntArrayRef sizes, IntArrayRef strides) {\n
T expected_stride = 1;\nbool contig_if_nonempty = true;\nfor (int64_t i = dim - 1; i >= 0; i--) {\nif (sizes[i] == 0) {\nreturn true;\n}\nif (contig_if_nonempty) {\nif (sizes[i] != 1 && strides[i] != expected_stride) {\n
\nbool geometry_is_contiguous(IntArrayRef sizes, IntArrayRef strides) {\nreturn _geometry_is_contiguous(sizes, strides);\n}\n\nbool TensorGeometry::is_contiguous() const {\nif (numel_ == 0) {\nreturn true;\n
auto dim = static_cast<std::int64_t>(sizes.size());\nT expected_stride = 1;\nbool contig_if_nonempty = true;\nfor (int64_t i = dim - 1; i >= 0; i--) {\nif (sizes[i] == 0) {\nreturn true;\n}\nif (contig_if_nonempty) {\n
\nvoid LRScheduler::set_optimizer_lrs(const std::vector<double>& learning_rates) {\n// Check the number of learning rates is equal to the number of parameters\n// groups in the optimizer\nTORCH_CHECK(\n
void LRScheduler::step() {\nstd::vector<double> learning_rates = get_lrs();\nset_optimizer_lrs(learning_rates);\nstep_count_++;\n}\n\nvoid LRScheduler::set_optimizer_lrs(const std::vector<double>& learning_rates) {\n
}\n}\nreturn learnings_rates;\n}\n\n} // namespace optim\n} // namespace torch\n
LRScheduler::LRScheduler(torch::optim::Optimizer& optimizer)\n: optimizer_(optimizer) {}\n\nvoid LRScheduler::step() {\nstd::vector<double> learning_rates = get_lrs();\nset_optimizer_lrs(learning_rates);\n
namespace torch { namespace autograd { namespace generated {\n\n${py_return_types}\n\n}}}\n\nnamespace torch::autograd {\n\nstatic void addReturnType(\nPyObject* module,\nconst char* name,\nPyTypeObject* type) {\n
}\n}\n\n} // namespace torch::autograd\n
(PyObject*)type) != 0) {\nPy_DECREF(type);\nthrow python_error();\n}\n}\n\nvoid initReturnTypes(PyObject* module) {\nstatic struct PyModuleDef def = {\nPyModuleDef_HEAD_INIT, "torch._C._return_types", nullptr, -1, {}};\n
\nstatic void addReturnType(\nPyObject* module,\nconst char* name,\nPyTypeObject* type) {\n// hold onto the TypeObject for the unlikely case of user\n// deleting or overriding it.\nPy_INCREF(type);\nif (PyModule_AddObject(\n
(*type_).ptr() = nullptr;\n}\n}\n\nc10::intrusive_ptr<JitFuture> PyRRef::getFuture() const {\n// Marking hasValue to false, as this Future is only used for signaling\n// profiler to update profiling result and the profiler does not retrieve\n
const RRefProxyType& type,\nfloat timeoutSeconds) const {\nauto& pythonRpcHandler = PythonRpcHandler::getInstance();\npybind11::gil_scoped_acquire ag;\nauto& functions = pythonRpcHandler.getRRefProxyFunctions();\n
}\n// Returns py::object that can be Python type or future.\nreturn *type_;\n}\n\npy::tuple PyRRef::pickle() const {\nauto& ctx = RRefContext::getInstance();\nauto rrefForkData = ctx.prepareChildFork(rref_);\n
return PyRRef(std::move(rref));\n}\n\nc10::IValue PyRRef::toIValue() const {\n// cast to RRefInterface to hold it into IValue\nauto rrefPtr = c10::static_intrusive_pointer_cast<c10::RRefInterface>(rref_);\n
std::shared_ptr<fe::graph::Tensor_attributes>, // O,\nstd::shared_ptr<fe::graph::Tensor_attributes>, // dO,\nstd::shared_ptr<fe::graph::Tensor_attributes>, // stats,\nstd::shared_ptr<fe::graph::Tensor_attributes>, // dQ,\n
} // namespace native\n} // namespace at\n\n#else // AT_CUDNN_ENABLED && defined(CUDNN_VERSION) && CUDNN_VERSION >= 8900\n#include <ATen/cudnn/Descriptors.h>\n#include <ATen/cudnn/Types.h>\n#include <ATen/cudnn/Utils.h>\n
mha_graph->execute(handle, variant_pack, workspace_ptr.get()).is_good());\nmhagraphcache.update(key, graph_and_tensors_values);\n}\n\nvoid run_cudnn_SDP_bprop(\nint64_t b,\nint64_t h,\nint64_t s_q,\nint64_t s_kv,\n
softmaxstats = at::empty({b, h, s_q}, q.options().dtype(kFloat));\n}\n\nauto key = MHACacheKeyWrapper(\nb,\nh,\ns_q,\ns_kv,\nd,\nq,\nk,\nv,\ndropout_probability,\nis_causal,\nreturn_softmaxstats);\nauto graph_and_tensors_ptr = mhagraphcache.find(key);\n
template struct AdaptiveMaxPoolOptions<ExpandingArray<1>>;\ntemplate struct AdaptiveMaxPoolOptions<ExpandingArrayWithOptionalElem<2>>;\ntemplate struct AdaptiveMaxPoolOptions<ExpandingArrayWithOptionalElem<3>>;\n
} // namespace torch\n
template struct AdaptiveMaxPoolOptions<ExpandingArrayWithOptionalElem<3>>;\n\ntemplate struct AdaptiveAvgPoolOptions<ExpandingArray<1>>;\ntemplate struct AdaptiveAvgPoolOptions<ExpandingArrayWithOptionalElem<2>>;\n
\n} // namespace nn\n} // namespace torch\n
Node* n = to_clean.back();\nAT_ASSERT(!n->hasUses());\nn->destroy();\nto_clean.pop_back();\n}\nAT_ASSERT(!self_value->hasUses());\ng->eraseInput(self_offset);\n\nreturn std::make_pair(std::move(g), std::move(extra_ivalues));\n
}\nreturn result;\n}\n\nstd::pair<std::shared_ptr<Graph>, std::vector<IValue>> LowerGraph(\nGraph& graph,\nconst ModulePtr& self) {\nauto result = lower_graph(self, graph);\nreturn std::make_pair(result.first, loadTensors(result.second));\n
}\ne.n->output()->replaceAllUsesWith(getOrAddSlot({e.mod, slot_idx}));\ne.n->destroy();\n}\n\nwhile (!to_clean.empty()) {\nNode* n = to_clean.back();\nAT_ASSERT(!n->hasUses());\nn->destroy();\nto_clean.pop_back();\n
}\nreturn result;\n}\n\nstd::pair<std::shared_ptr<Graph>, std::vector<IValue>> LowerGraph(\nGraph& graph,\nconst ModulePtr& self) {\nauto result = lower_graph(self, graph);\nreturn std::make_pair(result.first, loadTensors(result.second));\n
\nTORCH_META_FUNC(nll_loss_backward)\n(const Tensor& grad_output,\nconst Tensor& self,\nconst Tensor& target,\nOptionalTensorRef weight_opt,\nint64_t reduction,\nint64_t ignore_index,\nconst Tensor& total_weight) {\n
++num_ignored;\ncontinue;\n}\n\nTORCH_CHECK_INDEX(\ncur_target >= 0 && cur_target < n_classes,\n"Target ",\ncur_target,\n" is out of bounds.");\n\nconst auto data = input_data[b * n_classes + cur_target];\n
ret = smooth_loss.sum() /\nweight.gather(0, target.masked_select(~ignore_mask).flatten())\n.sum();\n}\n} else {\nauto true_mask = ~ignore_mask;\nret = smooth_loss.sum()/ true_mask.sum();\n}\nbreak;\ncase Reduction::Sum:\n
ret = at::nll_loss2d_symint(input_, target_, weight, reduction, std::move(ignore_index));\n} else {\n// dim == 3 or dim > 4\nauto n = input_.sym_sizes()[0];\nauto c = input_.sym_sizes()[1];\nauto out_size = input_.sym_sizes().slice(2).vec();\n
});\n} else {\nif (iter.dtype(0) == kHalf) {\nreturn norm_kernel_cpu_impl<at::Half, float>(iter, val);\n} else if (iter.input_dtype() == kHalf && iter.dtype(0) == kFloat) {\n// type promotion that does cast and reduction in a single kernel\n
using Vec = Vectorized<scalar_t>;\nusing fVec = Vectorized<acc_t>;\nfVec acc_vec{acc_t(0)};\nacc_t buffer[fVec::size()];\nint64_t d = 0;\nfor (; d < size - (size % Vec::size()); d += Vec::size()) {\nVec data_vec = Vec::loadu(self_data + d);\n
.declare_static_shape(self.sizes(), /*squash_dim=*/dim)\n.add_output(result)\n.add_const_input(self)\n.build();\n\nauto result_dim_stride = ensure_nonempty_stride(result, dim);\nauto self_dim_stride = ensure_nonempty_stride(self, dim);\n
\n#ifndef AT_PER_OPERATOR_HEADERS\n#include <ATen/Functions.h>\n#else\n#include <ATen/ops/imag.h>\n#endif\n\n#include <c10/util/Optional.h>\n#include <c10/util/irange.h>\n#include <ATen/AccumulateType.h>\n
\nTensor& hardtanh_quantized_cpu_(\nTensor& self,\nconst Scalar& min,\nconst Scalar& max) {\nTensor qy;\nqy = quantized_clamp_impl(self, min, max);\n// This can be optimized in a future PR if it becomes a bottleneck.\n
Tensor quantized_clamp_impl(\nconst Tensor& qx,\nconst optional<Scalar>& min,\nconst optional<Scalar>& max) {\nTensor qy;\nif (min && max) {\n#ifdef USE_PYTORCH_QNNPACK\nif (at::globalContext().qEngine() == at::QEngine::QNNPACK &&\n
uint8_t min_q =\nat::native::quantize_val<quint8>(input.q_scale(), input.q_zero_point(), min_f).val_;\nuint8_t max_q =\nat::native::quantize_val<quint8>(input.q_scale(), input.q_zero_point(), max_f).val_;\n
});\nreturn qy;\n}\n\n// hardtanh is clamp with default min==-1.0f and default max==1.0f\nTensor hardtanh_quantized_cpu(\nconst Tensor& qx,\nconst Scalar& min,\nconst Scalar& max) {\nTensor qy;\nqy = quantized_clamp_impl(qx, min, max);\n
op.callBoxed(stack);\n}\n\nvoid VmapInterpreterPtr::sendToNextInterpreterImpl(\nconst c10::OperatorHandle& op,\ntorch::jit::Stack* stack,\nbool grad_special_case) {\n// Re-dispatch\nif (getDynamicLayerStack().empty()) {\n
sanityCheckStack(op, stack);\n}\nop.callBoxed(stack);\n}\n\n} // namespace at::functorch\n
const c10::OperatorHandle& op,\ntorch::jit::Stack* stack,\nbool grad_special_case) {\n// Re-dispatch\nif (getDynamicLayerStack().empty()) {\nsanityCheckStack(op, stack);\n}\nop.callBoxed(stack);\n}\n\n
\nvoid VmapInterpreterPtr::processImpl(\nconst c10::OperatorHandle& op,\ntorch::jit::Stack* stack) {\nsetup_dispatch_key_tls(TransformType::Vmap, DispatchKeySet(DispatchKey::FuncTorchVmapMode));\nop.callBoxed(stack);\n
const xnn_status create_status = xnn_create_global_average_pooling_nwc_f32(\n-std::numeric_limits<float>::infinity(),\nstd::numeric_limits<float>::infinity(),\n0 /* flags */,\n&global_average_pooling_op);\n
xnn_status_success == create_status,\n"xnn_create_global_average_pooling_nwc_f32 failed!");\n\nOperator global_avg_pool_scoped_op(global_average_pooling_op);\n\nsize_t workspace_size = 0;\nsize_t workspace_alignment = 0;\n
\nTORCH_CHECK(\nxnn_status_success == create_status,\n"xnn_create_global_average_pooling_nwc_f32 failed!");\n\nOperator global_avg_pool_scoped_op(global_average_pooling_op);\n\nsize_t workspace_size = 0;\n
"xnn_create_global_average_pooling_nwc_f32 failed!");\n\nOperator global_avg_pool_scoped_op(global_average_pooling_op);\n\nsize_t workspace_size = 0;\nsize_t workspace_alignment = 0;\n\nconst xnn_status reshape_status = xnn_reshape_global_average_pooling_nwc_f32(\n
return false;\n}\n\nIValue& InterpreterState::reg(size_t reg) {\nTORCH_CHECK(\nreg > 0 && reg <= registers_.size(), "Invalid register index: ", reg);\nreturn *(registers_.end() - reg);\n}\n\n} // namespace mobile\n
registers_.size() - frames_.back().getCode().register_size_);\nframes_.pop_back();\n}\n\nvoid InterpreterState::saveExceptionDebugHandles() {\nstd::vector<DebugHandle> exception_debug_handles;\nfor (auto frame = frames_.crbegin(); frame != frames_.crend(); frame++) {\n
//   }\n// }\n// std::cout << std::endl;\n\n// TODO(iliacher): remove the workaround after RecordFunction is in\n// Dispatcher\n// Check with iliacher if has been done.\n// Plus this is not safe as if you throw exception record function will be\n
} // namespace\n\nusing namespace at;\n\nconst std::vector<DebugHandle>& getInterpretersExceptionDebugHandles() {\nreturn exception_debug_handles_;\n}\n\nvoid InterpreterState::enterFrame(const Code& code) {\n
return std::make_shared<SimpleValue>(n->output());\n}\n// Check and see if it's a getter attribute.\nauto prop = classType->getProperty(field);\nif (prop) {\nreturn MethodValue(value_, prop->getter->name())\n
->insertNode(\nm.graph()->createTupleIndex(value_, idx, out_type))\n->output();\nreturn std::make_shared<SimpleValue>(r);\n}\n}\n}\n} else if (auto awaitType = value_->type()->cast<AwaitType>()) {\nauto elType = awaitType->getElementType();\n
<< " does not have an __init__ function defined";\n}\n\n// Call the init function\nMethodValue(self, "__init__").call(loc, m, args, kwargs, n_binders);\n\nreturn std::make_shared<SimpleValue>(self);\n
} else {\nstep_ = g.insertConstant(1, loc);\n}\nhas_only_end_ = false;\n} else {\nthrow ErrorReport(loc) << "range expected at most 3 arguments, got "\n<< inputs.size();\n}\n\nstatic_len_ = static_len;\n
/// Like `try_make_tempdir`, but throws an exception if a temporary directory\n/// could not be returned.\nTempDir make_tempdir(std::string_view name_prefix) {\nif (auto tempdir = try_make_tempdir(name_prefix)) {\n
#else\nauto filename = make_filename(name_prefix);\n#endif\nif (filename.empty()) {\nreturn std::nullopt;\n}\n#if defined(_WIN32)\nreturn TempFile(std::move(filename));\n#else\nconst int fd = mkstemp(filename.data());\n
}\n\n#if defined(_WIN32)\nbool TempFile::open() {\nif (fd != -1) {\nreturn false;\n}\nauto err = _sopen_s(\n&fd,\nname.c_str(),\n_O_CREAT | _O_TEMPORARY | _O_EXCL | _O_BINARY | _O_RDWR,\n_SH_DENYNO,\n
if (!dirname) {\nreturn std::nullopt;\n}\nreturn TempDir(dirname);\n#endif // defined(_WIN32)\n}\n\n#if defined(_WIN32)\nbool TempFile::open() {\nif (fd != -1) {\nreturn false;\n}\nauto err = _sopen_s(\n
Tensor _upsample_nearest_exact3d(\nconst Tensor& input,\nat::OptionalIntArrayRef output_size,\nstd::optional<ArrayRef<double>> scale_factors) {\nauto osize = compute_output_size(input.sizes(), output_size, scale_factors);\n
set_output_raw_strided(0, full_output_size, {}, input.options().memory_format(input.suggest_memory_format()));\n}\n\nTORCH_META_FUNC(_upsample_nearest_exact3d) (\nconst Tensor& input,\nIntArrayRef output_size,\n
IntArrayRef output_size,\nIntArrayRef input_size,\nstd::optional<double> scales_d,\nstd::optional<double> scales_h,\nstd::optional<double> scales_w\n) {\nauto full_output_size = native::upsample_3d_common_check(input_size, output_size);\n
"Expected grad_output to be a tensor of dimension 5 but got: dimension ", grad_output.dim());\n\nfor (const auto i : c10::irange(5)) {\nTORCH_CHECK(\ngrad_output.size(i) == full_output_size[i],\n"Expected grad_output to have the same shape as output;",\n
\n} // namespace jit\n} // namespace torch\n
#include <ATen/core/jit_type.h>\n\n#include <string>\n#include <vector>\n\nnamespace torch {\nnamespace jit {\nnamespace mobile {\n\nchar const* toString(OpCode op);\n\nnamespace {\n\n/**\n* Serializes an IValue using Pickle, and puts it in a file named "data.pkl"\n
\n// Add the dict as an attribute.\nobject->setAttr(internal::kSavedParametersAttributeName, dict);\n\n// Wrap the Object in a Module.\nauto mcu = std::make_shared<mobile::CompilationUnit>();\nreturn mobile::Module(object, mcu);\n
std::ostream& out,\nbool use_flatbuffer) {\nauto dict = mobile::tensor_map_to_dict(map);\n\nauto write_func = [&out](const void* buf, size_t nbytes) -> size_t {\nout.write(\nstatic_cast<const char*>(buf), static_cast<std::streamsize>(nbytes));\n
experimental_config.adjust_timestamps = adjust_vulkan_timestamps;\n\ntorch::profiler::impl::ProfilerConfig config(\ntorch::profiler::impl::ProfilerState::KINETO,\nreport_input_shapes,\nprofile_memory,\n
#include <vector>\n\nnamespace torch {\nnamespace jit {\nnamespace mobile {\n\nthread_local KinetoEdgeCPUProfiler* tls_edge_profiler{nullptr};\n\nKinetoEdgeCPUProfiler::KinetoEdgeCPUProfiler(\nconst torch::jit::mobile::Module& m,\n
torch::autograd::profiler::reportBackendEventToActiveKinetoProfiler(\nstart_time_us,\nend_time_us,\ndebug_handle,\nat::RecordScope::LITE_INTERPRETER,\nevent_name,\nbackend_name);\n}\n\nconst std::unique_ptr<torch::autograd::profiler::ProfilerResult>&\n
\ntorch::profiler::impl::ProfilerConfig config(\ntorch::profiler::impl::ProfilerState::KINETO,\nreport_input_shapes,\nprofile_memory,\nwith_stack,\nwith_flops,\nwith_modules,\nexperimental_config);\ntorch::autograd::profiler::prepareProfiler(\n
graph->insertNode(graph->create(aten::type_as, 1));\ntype_as_node->addInput(add_mat);\ntype_as_node->addInput(type_as_mat);\nadd_mat = type_as_node->output();\nif (add_mat_type->isComplete()) {\nauto new_type =\n
// x.t().t() == x\nNode* input_node = node->input()->node();\nif (input_node->matches("aten::t(Tensor self) -> Tensor")) {\nGRAPH_UPDATE(\ngetHeader(node),\n" (x.t().t() == x) is replaced with ",\ninput_node->input()->debugName());\n
WithInsertPoint guard(node);\nstd::string type_str = node->inputs().at(0)->node()->s(attr::value);\nauto maybe_index = toIValue(node->inputs().at(1));\nint64_t index = 0;\nif (maybe_index) {\nindex = maybe_index->toInt();\n
bool optimizeBlock(Block* block) {\nbool changed = false;\nfor (auto it = block->nodes().begin(); it != block->nodes().end(); ++it) {\nauto* node = *it;\n\nfor (Block* sub_block : node->blocks()) {\nchanged |= optimizeBlock(sub_block);\n
// Vectorization was not successful.\nreturn false;\n}\n\nvoid LoopNest::initialize(\nconst std::vector<Tensor>& output_tensors,\nconst std::vector<Tensor>& tensors_to_compute) {\nfor (const auto& t : output_tensors) {\n
*   - The computation of `consumer` needs to be rewritten so that it uses\n*   `temp` instead of `producer`. The indices in the corresponding accesses\n*   also need to be offset.\n*/\nvoid LoopNest::computeAt(StmtPtr s, ForPtr f) {\n
if (parent == nullptr) {\nthrow malformed_input("parent of the loops must be a Block");\n}\n\n// Reorder the loops according to the permutation.\nstd::vector<ForPtr> result(loops.size());\nfor (size_t i = 0; i < loops.size(); ++i) {\n
auto it = root_block->begin();\nfor (; it != root_block->end(); ++it) {\nif (*it == loops.front()) {\nbreak;\n}\n}\nTORCH_INTERNAL_ASSERT(\nit != root_block->end(),\nbuildErrorMessage(\n"Could not find the given loop in the root stmt in unsafeFuseLoop the fuser."));\n
}\n\n// Move the constant definition to the beginning of the graph.\nauto first_node = node->owningGraph()->block()->nodes().front();\nif (node != first_node)\nnode->moveBefore(first_node);\n}\n}\n} // anonymous namespace\n
std::unordered_set<Node*, HashNode, EqualNode> constants;\nConstantPooling(graph->block(), constants, aliasDb);\n}\n} // namespace jit\n} // namespace torch\n
}\ncontinue;\n}\n\nif (node->kind() != prim::Constant) {\ncontinue;\n}\n\n// Check whether the same constant already exists.\nauto subit = constants.insert(node);\nif (!subit.second) {\nauto existing = *subit.first;\n
AliasDb aliasDb(graph);\nstd::unordered_set<Node*, HashNode, EqualNode> constants;\nConstantPooling(graph->block(), constants, aliasDb);\n}\n} // namespace jit\n} // namespace torch\n
#include <c10/core/impl/HermeticPyObjectTLS.h>\n\nnamespace c10::impl {\n\nthread_local std::atomic<bool> hermeticPyObjectState{false};\n\nstd::atomic<bool> HermeticPyObjectTLS::haveState_{false};\n\n
#include <c10/core/impl/HermeticPyObjectTLS.h>\n\nnamespace c10::impl {\n\nthread_local std::atomic<bool> hermeticPyObjectState{false};\n\nstd::atomic<bool> HermeticPyObjectTLS::haveState_{false};\n\n
std::atomic<bool> HermeticPyObjectTLS::haveState_{false};\n\nvoid HermeticPyObjectTLS::set_state(bool state) {\nhermeticPyObjectState = state;\n}\n\nbool HermeticPyObjectTLS::get_tls_state() {\nreturn hermeticPyObjectState;\n
std::atomic<bool> HermeticPyObjectTLS::haveState_{false};\n\nvoid HermeticPyObjectTLS::set_state(bool state) {\nhermeticPyObjectState = state;\n}\n\nbool HermeticPyObjectTLS::get_tls_state() {\nreturn hermeticPyObjectState;\n
return getKernelRegistry().hasKernelFn(name);\n}\n\nbool KernelRegistry::hasKernelFn(const char* name) {\nauto kernel = this->kernels_map_.find(name);\nreturn kernel != this->kernels_map_.end();\n}\n\n
}\n\nKernelFunction& getKernelFn(const char* name) {\nreturn getKernelRegistry().getKernelFn(name);\n}\n\nKernelFunction& KernelRegistry::getKernelFn(const char* name) {\nauto kernel = this->kernels_map_.find(name);\n
return kernel != this->kernels_map_.end();\n}\n\nKernelFunction& getKernelFn(const char* name) {\nreturn getKernelRegistry().getKernelFn(name);\n}\n\nKernelFunction& KernelRegistry::getKernelFn(const char* name) {\n
}\n\nKernelFunction& getKernelFn(const char* name) {\nreturn getKernelRegistry().getKernelFn(name);\n}\n\nKernelFunction& KernelRegistry::getKernelFn(const char* name) {\nauto kernel = this->kernels_map_.find(name);\n
#ifndef C10_MOBILE\nstatic void check(int64_t value) {\nconst auto i = SymInt(value);\nEXPECT_EQ(i.maybe_as_int(), c10::make_optional(value));\n}\n\nTEST(SymIntTest, ConcreteInts) {\ncheck(INT64_MAX);\n
EXPECT_NE(0 - y, 0);\n}\n\n#endif\n
check(INT64_MIN);\n}\n\nTEST(SymIntTest, CheckRange) {\nEXPECT_FALSE(SymInt::check_range(INT64_MIN));\n}\n\nTEST(SymIntTest, Overflows) {\nconst auto x = SymInt(INT64_MAX);\nEXPECT_NE(-(x + 1), 0);\n\n
\n#include <c10/core/SymInt.h>\n#include <c10/core/SymNodeImpl.h>\n\nusing namespace c10;\n#ifndef C10_MOBILE\nstatic void check(int64_t value) {\nconst auto i = SymInt(value);\nEXPECT_EQ(i.maybe_as_int(), c10::make_optional(value));\n
{padding_expanded[0], padding_expanded[1]},\n{output_padding_expanded[0], output_padding_expanded[1]},\n{stride_expanded[0], stride_expanded[1]},\n{dilation_expanded[0], dilation_expanded[1]},\ntransposed, groups\n
{dilation_expanded[0], dilation_expanded[1]},\ntransposed, groups\n};\n}\n\nTensor run(\nContextConv2D& context,\nconst Tensor& input) {\nusing namespace internal;\n\nconst Tensor padded_input_nhwc = mobile::allocate_padded_contiguous_if_needed(\n
TORCH_CHECK(\navailable(\nweight_nhwc,\n(bias.has_value() && bias->defined()) ? at::OptionalIntArrayRef(bias->sizes()) : c10::nullopt,\npadding_expanded,\nstride_expanded,\ndilation_expanded,\ngroups,\n
std::move(dilation),\ngroups,\noutput_min,\noutput_max);\n}\n\nTensor conv2d_clamp_run(\nconst Tensor& input,\nconst c10::intrusive_ptr<xnnpack::Conv2dOpContext>& op_context) {\nreturn op_context->run(input);\n
std::vector<int64_t> strides(sizes.size(), 1);\nfor (int64_t i = sizes.size(); i > 1; --i) {\nstrides[i - 2] = strides[i - 1] * sizes[i - 1];\n}\nreturn strides;\n}\n\nstd::vector<at::Tensor> DataHandlesToTensors(\n
c10::ArrayRef<BackendDataPtr> data_handles,\nat::ScalarType dest_element_type) {\nstd::vector<at::Tensor> tensors;\nfor (const auto& handle : data_handles) {\ntensors.push_back(\ngetBackend()->MakeTensorFromComputationData(handle, dest_element_type));\n
strides[i - 2] = strides[i - 1] * sizes[i - 1];\n}\nreturn strides;\n}\n\nstd::vector<at::Tensor> DataHandlesToTensors(\nc10::ArrayRef<BackendDataPtr> data_handles,\nat::ScalarType dest_element_type) {\n
#include <algorithm>\n#include <cstring>\n#include <functional>\n#include <list>\n#include <numeric>\n#include <thread>\n\nnamespace torch {\nnamespace lazy {\n\nstd::vector<int64_t> ComputeArrayStrides(c10::ArrayRef<int64_t> sizes) {\n
}\n\nstd::string to_string(const ScalarType& type) {\nstd::ostringstream oss;\noss << type;\nreturn oss.str();\n}\n\n} // namespace std\n
#include <torch/csrc/jit/tensorexpr/types.h>\n\n#include <torch/csrc/Export.h>\n#include <torch/csrc/jit/tensorexpr/exceptions.h>\n\n#include <c10/util/Logging.h>\n\nnamespace torch::jit::tensorexpr {\n
\nstd::string to_string(const Dtype& dtype) {\nstd::ostringstream oss;\noss << dtype;\nreturn oss.str();\n}\n\nstd::string to_string(const ScalarType& type) {\nstd::ostringstream oss;\noss << type;\nreturn oss.str();\n
// NOLINTNEXTLINE\n#define DTYPE_DEFINE(_1, n) TORCH_API Dtype k##n(ScalarType::n, 1);\n\nAT_FORALL_SCALAR_TYPES_AND7(\nBool,\nHalf,\nBFloat16,\nFloat8_e5m2,\nFloat8_e5m2fnuz,\nFloat8_e4m3fn,\nFloat8_e4m3fnuz,\n
\nstd::string get_cxx_flags() {\n#if defined(FBCODE_CAFFE2)\nTORCH_CHECK(\nfalse,\n"Buck does not populate the `CXX_FLAGS` field of Caffe2 build options. "\n"As a result, `get_cxx_flags` is OSS only."\n
ss << "CPU capability usage: " << get_cpu_capability();\nreturn ss.str();\n}\n\nstd::string show_config() {\nstd::ostringstream ss;\nss << "PyTorch built with:\n";\n\n// Reference:\n// https://blog.kowalczyk.info/article/j/guide-to-predefined-macros-in-c-compilers-gcc-clang-msvc-etc..html\n
#endif\n\n#if AT_BUILD_WITH_LAPACK()\n// TODO: Actually record which one we actually picked\nss << "  - LAPACK is enabled (usually provided by MKL)\n";\n#endif\n\n#if AT_NNPACK_ENABLED()\n// TODO: No version; c.f. https://github.com/Maratyszcza/NNPACK/issues/165\n
#include <dnnl.hpp>\n#include <ideep.hpp>\n#endif\n\n#include <caffe2/core/common.h>\n\n#include <ATen/native/DispatchStub.h>\n\n#include <sstream>\n\nnamespace at {\n\nstd::string get_mkl_version() {\n
\nif (subscript_exprs.size() != 1)\nthrow ErrorReport(subscript.subscript_exprs().range())\n<< "BroadcastingList/Optional[BroadcastingList] "\n"must be subscripted with a type";\n\nauto typ = subscript_exprs[0];\n
const Decl& decl,\nbool skip_self) {\nauto params_begin = decl.params().begin();\nauto params_end = decl.params().end();\nif (skip_self) {\n++params_begin;\n}\nstd::vector<Argument> retval;\n\nstd::vector<Expr> default_types;\n
\nthrow ErrorReport(expr) << "Unknown type name '" << type_name << "'";\n} else if (auto name = parseBaseTypeName(expr)) {\nauto itr = string_to_type_lut().find(*name);\nif (itr != string_to_type_lut().end()) {\n
return Var(expr).name().name();\n}\ncase TK_NONE: {\nreturn "None";\n}\ncase TK_NONE_TYPE: {\nreturn "NoneType";\n}\ncase '.': {\nauto select = Select(expr);\nconst std::string& name = select.selector().name();\n
const std::string& name)\n: dims_(dims), name_(name), strides_(dims.size()) {\nfor (int i = (int)dims.size() - 1; i >= 0; --i) {\nif (i == (int)dims.size() - 1) {\nstrides_[i] = 1;\n} else {\nstrides_[i] = strides_[i + 1] * dims[i + 1];\n
}\n\nPaddedBufferBase::PaddedBufferBase(\nconst std::vector<int>& dims,\n// NOLINTNEXTLINE(modernize-pass-by-value)\nconst std::string& name)\n: dims_(dims), name_(name), strides_(dims.size()) {\nfor (int i = (int)dims.size() - 1; i >= 0; --i) {\n
#include <c10/util/irange.h>\n#include <sstream>\n\nnamespace torch {\nnamespace jit {\nnamespace tensorexpr {\n\nint PaddedBufferBase::Index(const std::vector<int>& indices) const {\nTORCH_DCHECK_EQ(dims_.size(), indices.size());\n
} // namespace torch\n
\nstatic uint64_t op_counter = 0;\nstatic uint64_t last_saved_value = 0;\n\n// register guard\nnamespace at {\nnamespace detail {\n\nC10_REGISTER_GUARD_IMPL(PrivateUse1, c10::impl::NoOpDeviceGuardImpl<DeviceType::PrivateUse1>);\n
at::Tensor custom_mul_Tensor(const at::Tensor & self, const at::Tensor & other) {\nop_counter += 1;\n// Since this custom device is just for testing, not bothering to implement kernels.\nreturn at::empty(self.sizes(), self.options());\n
// Since this custom device is just for testing, not bothering to implement kernels.\nreturn at::empty(self.sizes(), self.options());\n}\n\n// basic dummy mul function\nat::Tensor custom_mul_Tensor(const at::Tensor & self, const at::Tensor & other) {\n
#include <ATen/EmptyTensor.h>\n#include <ATen/core/GeneratorForPrivateuseone.h>\n\nstatic uint64_t op_counter = 0;\nstatic uint64_t last_saved_value = 0;\n\n// register guard\nnamespace at {\nnamespace detail {\n
for (const auto i : c10::irange(n.outputs().size())) {\nif (n.outputs().at(i)->type() != BoolType::get()) {\ncontinue;\n}\nbool true_val =\nconstant_as<bool>(n.thenOutputs().at(i)).value_or(false);\nbool false_val =\n
if (node.inputs().size() != 2 || node.input(0)->type() != IntType::get() ||\nnode.input(1)->type() != IntType::get()) {\nreturn {};\n}\n\nif (node.kind() == aten::mul || node.kind() == aten::add) {\nif (auto i = constant_as<int64_t>(node.input(0))) {\n
\n} // namespace jit\n} // namespace torch\n
n.outputs().at(i)->debugName(),\n" (True or False) with ",\nn.cond()->debugName());\nn.outputs().at(i)->replaceAllUsesWith(n.cond());\nchanged = true;\n}\n}\n\n// check for types that can be refined\n
running_var{running_var_t, "running_var", 5};\nCheckedFrom c = "cudnn_batch_norm";\n\ncheckAllDefined(c, {input, weight, bias});\nif (!training) {\ncheckAllDefined(c, {running_mean, running_var});\n}\n
// Unused: but we require them to be passed so that double backwards\n// has access\nconst std::optional<Tensor>& running_mean_opt,\nconst std::optional<Tensor>& running_var_opt,\nconst std::optional<Tensor>& save_mean_t_opt,\n
nullptr,\n&workspace_size));\nTensor workspace = at::empty(workspace_size, input->options().dtype(kByte));\n\n// get the reserved size and allocate as tensor\nsize_t reserve_size =\n_get_cudnn_batch_norm_reserve_space_size(input_t, true /* training */);\n
&workspace_size));\nTensor workspace = at::empty(workspace_size, input->options().dtype(kByte));\n\nAT_CUDNN_CHECK(cudnnBatchNormalizationBackwardEx(\nhandle,\nmode,\nop,\n&one,\n&zero,\n&one,\n&zero,\n
});                                                        \\n}\n\nAOTI_TORCH_ITEM_IMPL(float32, float)\nAOTI_TORCH_ITEM_IMPL(float64, double)\nAOTI_TORCH_ITEM_IMPL(uint8, uint8_t)\nAOTI_TORCH_ITEM_IMPL(uint16, uint16_t)\n
*ret0 = new_tensor_handle(std::move(r0));\n*ret1 = new_tensor_handle(std::move(r1));\n*ret2 = new_tensor_handle(std::move(r2));\n*ret3 = new_tensor_handle(std::move(r3));\n});\n}\n\nAOTI_TORCH_EXPORT AOTITorchError aoti_torch_convolution(\n
}\n\nAOTI_TORCH_SCALAR_TO_TENSOR_IMPL(float32, float, Float)\nAOTI_TORCH_SCALAR_TO_TENSOR_IMPL(float64, double, Double)\nAOTI_TORCH_SCALAR_TO_TENSOR_IMPL(uint8, uint8_t, Byte)\nAOTI_TORCH_SCALAR_TO_TENSOR_IMPL(uint16, uint16_t, UInt16)\n
c10::IntArrayRef sizes(sizes_ptr, ndim);\nc10::IntArrayRef strides(strides_ptr, ndim);\n*ret_new_tensor = new_tensor_handle(torch::inductor::_reinterpret_tensor(\n*self_tensor, sizes, strides, offset_increment));\n
int8_t* unpacked_weights_p =\nreinterpret_cast<int8_t*>(unpacked_weights.data_ptr<c10::qint8>());\npacked_weights_p->unpack(unpacked_weights_p);\nif(transpose()){\nunpacked_weights =\nat::native::fbgemm_utils::TransposeConvTensorUnpackConversion<\n
// Tensor for unpacked weights\n// Unpacked format would be physical KRS(C/G) but logical KCRS (channels\n// first) because that's how\n// ChannelsLast3d is not available now.FBGEMM stores the weights\n
output_channels,\nC_per_G,\nkernel_d,\nkernel_h,\nkernel_w,\ndevice(c10::kCPU).dtype(c10::kQInt8),\nscales.toType(c10::kDouble),\nzero_points.toType(c10::kLong));\n} else {\nTORCH_CHECK(false, "Unsupported qscheme: ", toString(q_scheme));\n
\ntemplate std::tuple<at::Tensor, std::optional<at::Tensor>> PackedConvWeightsQnnp<\n2>::unpack();\ntemplate std::tuple<at::Tensor, std::optional<at::Tensor>> PackedConvWeightsQnnp<\n3>::unpack();\n#endif // USE_PYTORCH_QNNPACK\n
\n// After performing CombineConcats:\n//  graph(%0 : Tensor):\n//    %dim : int = prim::Constant[value=0]()\n//    %input : Tensor[] = prim::ListConstruct(%0, %0, %0)\n//    %concat : Tensor = aten::cat(%input, %dim)\n
std::vector<at::Tensor> inputs = {\nat::rand({64, 56, 56}, at::kCPU), at::rand({32, 56, 56}, at::kCPU)};\nauto orig_outputs = runGraph(graph, inputs);\n\nASSERT_TRUE(UseVariadicCat(graph));\ngraph->lint();\n
%res : Tensor[] = prim::ListConstruct(%concat.1, %concat.2, %concat.3, %concat.4)\nreturn (%res)\n)IR";\nparseIR(input, graph.get());\nstd::vector<at::Tensor> inputs = {\nat::rand({64, 56, 56}, at::kCPU),\n
->check_count("= prim::ListConstruct(", 0, /*exactly*/ true)\n->check_count("= aten::cat(", 0, /*exactly*/ true)\n->run(*graph);\n}\n\nTEST(\nConcatOptTest,\nRemoveListMutationUseVariadicCatAndCommonInputsElimination) {\n
} // namespace at::native\n
"Offset tensor contains duplicate values");\n\nauto nnz_per_diag = at::where(\noffsets_1d.le(0),\noffsets_1d.add(shape[0]).clamp_max_(diagonals_2d.size(1)),\noffsets_1d.add(-std::min<int64_t>(shape[1], diagonals_2d.size(1))).neg());\n
offsets_1d.size(0),\n")");\nif (layout) {\nTORCH_CHECK(\n(*layout == Layout::Sparse) || (*layout == Layout::SparseCsc) ||\n(*layout == Layout::SparseCsr),\n"Only output layouts (Sparse, SparseCsc, SparseCsr) are supported, got ",\n
#endif\n\nnamespace at::native {\n\nDEFINE_DISPATCH(spdiags_kernel_stub);\n\nTensor spdiags(\nconst Tensor& diagonals,\nconst Tensor& offsets,\nIntArrayRef shape,\nstd::optional<Layout> layout) {\nauto diagonals_2d = diagonals.dim() == 1 ? diagonals.unsqueeze(0) : diagonals;\n
\nstd::tuple<Tensor, Tensor> ctc_loss_cpu(const Tensor& log_probs, const Tensor& targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t BLANK, bool zero_infinity) {\n(void)zero_infinity; // only used for backwards\n
\nif (input_length == 0) {\nscalar_t log_likelihood = target_length == 0 ? 0 : neginf;\nneg_log_likelihood_a[b] = -log_likelihood;\ncontinue;\n}\n\n// the first two items of alpha_t above eq (6)\nlog_alpha_a[0][0] = log_probs_a[0][BLANK];\n
auto current_target_prime = get_target_prime(targets_data, tg_batch_offset, tg_target_stride, s, BLANK);\nif (s < 2*target_length) {\nlb2 = log_beta_a[t+1][s+1];\nif (lb2 > lbmax)\nlbmax = lb2;\n} else {\n
auto neg_log_likelihood_a = neg_log_likelihood.accessor<scalar_t, 1>();\n\n// alpha calculation for the first row, the three equations for alpha_1 above eq (6)\n// first the default\nlog_alpha.narrow(1, 0, 1).fill_(neginf);\n
}\n\nreturn alloc<Block>(stmts);\n}\n\n// NOLINTNEXTLINE(cppcoreguidelines-init-variables)\nstd::vector<StmtPtr> stmts;\nfor (auto& segment : innerSegments) {\nbool need_sync = false;\n// We never mask loops, they'll mask their contents.\n
static const char* device_resource_string = R"(\n#define NAN __int_as_float(0x7fffffff)\n#define POS_INFINITY __int_as_float(0x7f800000)\n#define NEG_INFINITY __int_as_float(0xff800000)\n\n)";\n\nstatic const char* shared_resource_string = R"(\n
}\nif (v->indices().empty() && load_v->indices().empty()) {\nreturn v;\n}\nbool index_equal = CheckEqual(v->flat_index(), load_v->flat_index());\nif (!index_equal) {\nreturn v;\n}\n\n// TODO: this checks that the metavars occur directly as an index, but this\n
stmt_v = stmt_v->accept_mutator(metavar_rewriter_.get());\n\nAtomicAddFuser atomic_add_fuser(\ncuda_analysis_->thread_local_bufs(), *metavar_rewriter_.get());\nstmt_v = stmt_v->accept_mutator(&atomic_add_fuser);\n
})\n.def(\n"append",\n[](const std::shared_ptr<ScriptList>& self, py::object value) {\ntry {\nreturn self->append(\ntoIValue(std::move(value), self->type()->getElementType()));\n} catch (const py::cast_error& e) {\n
return toPyObject(result);\n})\n.def("__iter__", [](ScriptListIterator& iter) { return iter; });\n\npy::class_<ScriptList, std::shared_ptr<ScriptList>>(m, "ScriptList")\n.def(py::init([](py::list list) {\n
return seq;\n})\n.def(\n"__setitem__",\n[](const std::shared_ptr<ScriptList>& self,\nScriptList::diff_type idx,\npy::object value) {\ntry {\nself->setItem(\nidx,\ntoIValue(std::move(value), self->type()->getElementType()));\n
start, toIValue(value[i], self->type()->getElementType()));\n} catch (const py::cast_error& e) {\nthrow py::type_error();\n}\nstart += step;\n}\n})\n.def(\n"__delitem__",\n[](const std::shared_ptr<ScriptList>& self,\n
\nTensor NestedTensor_sgn(const Tensor& self) {\nreturn map_nt(self, at::sgn);\n}\n\nTensor& NestedTensor_sgn_(Tensor& self) {\nauto self_ptr = get_nested_tensor_impl(self);\ncheck_numel_equals_buffer_size(self_ptr);\n
return at::gelu(buffer, approximate);\n});\n}\n\nTensor& NestedTensor_tanh_(Tensor& self) {\nauto self_ptr = get_nested_tensor_impl(self);\ncheck_numel_equals_buffer_size(self_ptr);\nauto buffer = self_ptr->get_buffer();\n
auto buffer = self_ptr->get_buffer();\nat::neg_(buffer);\nreturn self;\n}\n\nTensor NestedTensor_neg(const Tensor& self) {\nreturn map_nt(self, at::neg);\n}\n\nTensor& zero_nested_(Tensor& self) {\nconst auto& self_buf = get_nested_tensor_impl(self)->get_buffer();\n
check_numel_equals_buffer_size(self_ptr);\nauto buffer = self_ptr->get_buffer();\nbuffer.logical_not_();\nreturn self;\n}\n\nTensor NestedTensor_logical_not(const Tensor& self) {\nreturn map_nt(self, at::logical_not);\n
\nnamespace torch {\nnamespace jit {\nTEST(EliminateDeadCodeTest, Basic) {\nauto graph = std::make_shared<Graph>();\n\n// Consider the following loop:\n//   for i in range(3):\n//     tot += a[0][0]\n
R"IR(\ngraph():\n%48 : None = prim::Constant()\n%50 : bool = prim::Constant[value=1]()\n%0 : int = prim::Constant[value=2]()\n%12 : int = prim::Constant[value=1]()\n%24 : int = prim::Constant[value=3]()\n
%tot.3 : Tensor = aten::add_(%tot.6, %35, %12)\n%b.1 : Tensor = aten::select(%a.1, %31, %31)\n%44 : Tensor = aten::select(%b.1, %31, %31)\n# CHECK: add_\n%46 : Tensor = aten::add_(%44, %12, %12)\n-> (%50, %tot.3)\n
\nnamespace torch {\nnamespace jit {\nTEST(EliminateDeadCodeTest, Basic) {\nauto graph = std::make_shared<Graph>();\n\n// Consider the following loop:\n//   for i in range(3):\n//     tot += a[0][0]\n
dtype,\nc10::kStrided,\ndevice,\npin_memory,\nnon_blocking,\ntrue, // force copy since we are in _to_copy\nmemory_format);\n\nreturn at::_sparse_coo_tensor_unsafe(\nnew_indices,\nnew_values,\nself.sizes(),\n
// one in.\nif (!block_sparse) {\nreturn dense.reshape(self.sizes());\n} else {\nreturn dense\n.unflatten(0, {-1, nrows, ncols})\n.transpose(2, 3)\n.reshape(self.sizes());\n}\n}\n\n// Computes the strides for view_dtype output when the view dtype is\n
if (self.layout() == layout_to) {\n_to_sparse_check_arguments("to_sparse", self, sparse_dim);\nreturn self;\n}\nreturn self._to_sparse(sparse_dim);\n}\n\nTensor to_sparse(const Tensor& self, std::optional<c10::Layout> layout, OptionalIntArrayRef blocksize, std::optional<int64_t> dense_dim_opt) {\n
AT_ASSERT(input_.layout() == c10::kStrided);\nreturn grad.to_dense(input_.scalar_type());\n}\n\nTensor to_dense(const Tensor& tensor, std::optional<c10::ScalarType> dtype, std::optional<bool> masked_grad) {\n
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS\n#include <ATen/native/Lerp.h>\n#include <ATen/core/Tensor.h>\n#include <ATen/TensorIterator.h>\n#include <ATen/TensorMeta.h>\n\n#ifndef AT_PER_OPERATOR_HEADERS\n
#endif\n\nnamespace at::meta {\n\nTORCH_META_FUNC(lerp_Tensor)(\nconst Tensor& self, const Tensor& end, const Tensor& weight) {\nTORCH_CHECK(self.dtype() == end.dtype(), "expected dtype ", self.dtype(),\n
lerp_kernel_scalar_weight(device_type(), *this, weight);\n}\n\nDEFINE_DISPATCH(lerp_kernel_scalar_weight);\nDEFINE_DISPATCH(lerp_kernel_tensor_weight);\n\n} // namespace at::native\n
#include <ATen/native/Lerp.h>\n#include <ATen/core/Tensor.h>\n#include <ATen/TensorIterator.h>\n#include <ATen/TensorMeta.h>\n\n#ifndef AT_PER_OPERATOR_HEADERS\n#include <ATen/NativeFunctions.h>\n#else\n
if (((self.dtype() == at::kFloat && src.dtype() == at::kHalf) ||\n(self.dtype() == at::kHalf && src.dtype() == at::kFloat)) &&\n(self.device().is_cpu() && src.device().is_cpu()) &&\n((self.is_contiguous() && src.is_contiguous()) ||\n
#else\nreturn at::vulkan::vulkan_copy_(self, src);\n#endif\n}\n\nif (self.device().type() == at::kMetal || src.device().type() == at::kMetal) {\nreturn at::metal::metal_copy_(self, src);\n}\n\n// Exit early if self and src are views of the same data\n
}\n\nif (!self.is_quantized() && src.is_quantized()) {\nTORCH_CHECK(false, "Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor");\n
NoNamesGuard guard;\nif (self._is_zerotensor()) {\nTORCH_CHECK(false, "ZeroTensors are immutable. Please materialize the tensor using `.clone()`, if you want a mutable zero tensor.");\n}\nif (src._is_zerotensor()) {\n
\nstd::string add_negative_flag(const std::string& flag) {\nstd::string filter = ::testing::GTEST_FLAG(filter);\nif (filter.find('-') == std::string::npos) {\nfilter.push_back('-');\n} else {\nfilter.push_back(':');\n
::testing::GTEST_FLAG(filter) = add_negative_flag("*_CUDA:*_MultiCUDA");\n\nreturn RUN_ALL_TESTS();\n}\n
std::string add_negative_flag(const std::string& flag) {\nstd::string filter = ::testing::GTEST_FLAG(filter);\nif (filter.find('-') == std::string::npos) {\nfilter.push_back('-');\n} else {\nfilter.push_back(':');\n
std::string filter = ::testing::GTEST_FLAG(filter);\nif (filter.find('-') == std::string::npos) {\nfilter.push_back('-');\n} else {\nfilter.push_back(':');\n}\nfilter += flag;\nreturn filter;\n}\nint main(int argc, char* argv[]) {\n
Tensor& output) {\nmulti_margin_loss_out_cpu_template(\noutput, input, target, p.toInt(), margin, weight, reduction);\nreturn output;\n}\n\nTensor multi_margin_loss_cpu_backward(\nconst Tensor& grad_output,\n
TORCH_CHECK(p == 1 || p == 2, "only p == 1 and p == 2 supported");\n\nmulti_margin_loss_shape_check(nframe, dim, ndims, input, target, weight);\n\n// produce a scalar output for 1d input\nif (reduction == Reduction::None && target.dim() > 0) {\n
h *= weight_data[target_idx];\n}\ngrad_input_target -= h;\ngrad_input_row_data[d] = h;\n} else {\ngrad_input_row_data[d] = 0;\n}\n}\ngrad_input_row_data[target_idx] = grad_input_target;\n\ninput_data += dim;\n
Tensor& output) {\nmulti_margin_loss_out_cpu_template(\noutput, input, target, p.toInt(), margin, weight, reduction);\nreturn output;\n}\n\nTensor multi_margin_loss_cpu_backward(\nconst Tensor& grad_output,\n
ASSERT_TRUE(copied_list[0].get().toTensor().allclose(\ntensor_list[0].get().toTensor()));\nASSERT_TRUE(copied_list[1].get().toTensor().allclose(\ntensor_list[1].get().toTensor()));\nASSERT_TRUE(copied_list[2].get().toTensor().allclose(\n
// NOTE: this is actually incorrect. Ideally, these _should_ be aliases.\nASSERT_FALSE(copied_list[0].get().isAliasOf(copied_list[2].get()));\n\nASSERT_TRUE(copied_list[0].get().toTensor().allclose(\n
\nc10::IValue::CompIdentityIValues ivalue_compare;\n\n// Make sure our setup configuration is correct\nASSERT_TRUE(ivalue_compare(tensor_list[0].get(), tensor_list[3].get()));\nASSERT_FALSE(ivalue_compare(tensor_list[0].get(), tensor_list[1].get()));\n
#include <torch/torch.h>\n\n#include <test/cpp/api/support.h>\n\n#include <cstdio>\n#include <memory>\n#include <sstream>\n#include <string>\n#include <vector>\n\nusing namespace torch::test;\nusing namespace torch::nn;\n
\nauto raw_obj = src.ptr();\nif (THPUtils_checkBool(raw_obj)) {\nvalue = c10::SymBool{THPUtils_unpackBool(raw_obj)};\nreturn true;\n}\nreturn false;\n}\n\npy::handle type_caster<c10::SymBool>::cast(\n
} // namespace pybind11\n
\npy::handle type_caster<c10::SymInt>::cast(\nconst c10::SymInt& si,\nreturn_value_policy /* policy */,\nhandle /* parent */) {\nif (si.is_symbolic()) {\nauto* py_node = dynamic_cast<torch::impl::PythonSymNodeImpl*>(\n
} else if (scalar.isBoolean()) {\nif (scalar.isSymbolic()) {\nreturn py::cast(scalar.toSymBool()).release();\n}\nreturn py::cast(scalar.toBool()).release();\n} else if (scalar.isComplex()) {\nreturn py::cast(scalar.toComplexDouble()).release();\n
for (int64_t i =\nstatic_cast<size_t>(TorchDispatchModeKey::NUM_MODE_KEYS) - 1;\ni >= 0;\n--i) {\nif (torchDispatchModeState.infra_modes_[i].has_value()) {\n// NOLINTNEXTLINE(bugprone-unchecked-optional-access)\n
// - any infra modes (members of torchDispatchModeState.infra_modes_ that are\n// not None)\n\n// idx == 0 means the "bottom" of the stack, which starts with any infra\n// modes (iterating from lowest-priority to highest-priority).\n
TORCH_CHECK(idx < stack_len(), "Tried to get stack at idx that's too big");\n// Our "logical" stack includes both:\n// - any user modes (the entire torchDispatchModeState.stack_)\n// - any infra modes (members of torchDispatchModeState.infra_modes_ that are\n
// not None)\n\n// idx == 0 means the "bottom" of the stack, which starts with any infra\n// modes (iterating from lowest-priority to highest-priority).\nauto curr_idx = idx;\nfor (const auto i :\nc10::irange(static_cast<size_t>(TorchDispatchModeKey::NUM_MODE_KEYS))) {\n
py::class_<Doubler>(m, "Doubler")\n.def(py::init<int, int>())\n.def("forward", &Doubler::forward)\n.def("get", &Doubler::get);\n}\n
m.def("exp_add", &exp_add, "exp(x) + exp(y)");\npy::class_<Doubler>(m, "Doubler")\n.def(py::init<int, int>())\n.def("forward", &Doubler::forward)\n.def("get", &Doubler::get);\n}\n
\nTensor tanh_add(Tensor x, Tensor y) {\nreturn x.tanh() + y.tanh();\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\nm.def("tanh_add", &tanh_add, "tanh(x) + tanh(y)");\nm.def("exp_add", &exp_add, "exp(x) + exp(y)");\n
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\nm.def("tanh_add", &tanh_add, "tanh(x) + tanh(y)");\nm.def("exp_add", &exp_add, "exp(x) + exp(y)");\npy::class_<Doubler>(m, "Doubler")\n.def(py::init<int, int>())\n
#include <ATen/core/Tensor.h>\n#include <ATen/native/ReduceOps.h>\n#include <ATen/native/TensorCompare.h>\n\n#include <numeric>\n#include <iterator>\n#include <algorithm>\n#include <utility>\n#include <vector>\n
ScalarType common_type = at::result_type(elements, test_elements);\nTensor promoted_elements = elements.to(common_type);\nTensor test_elements_flat = test_elements.to(common_type).view(-1);\nauto test_elements_stride = test_elements_flat.stride(0);\n
Tensor promoted_elements = elements.to(common_type);\nTensor test_elements_flat = test_elements.to(common_type).view(-1);\nauto test_elements_stride = test_elements_flat.stride(0);\n\nauto iter = TensorIteratorConfig()\n
}\n}\n}\n*result_data = min_number;\n*indice_data = index;\n}\n);\n});\n}\n\nstatic void max_kernel_impl(\nconst Tensor& result,\nconst Tensor& indice,\nconst Tensor& self,\nint64_t dim,\nbool keepdim) {\n
