\n\ndef sig_for_ops(opname: str) -> List[str]:\n"""sig_for_ops(opname : str) -> List[str]\n\nReturns signatures for operator special functions (__add__ etc.)"""\n\n# we have to do this by hand, because they are hand-bound in Python\n
],\n"sparse_compressed_tensor": [\n"def sparse_compressed_tensor({}) -> Tensor: ...".format(\n", ".join(\n[\n"compressed_indices: Union[Tensor, List]",\n"plain_indices: Union[Tensor, List]",\n"values: Union[Tensor, List]",\n
# TODO: don't explicitly list dtypes here; get it from canonical\n# source\ndtype_class_hints = [\nf"{n}: dtype = ..."\nfor n in [\n"float32",\n"float",\n"float64",\n"double",\n"float16",\n"bfloat16",\n
"_from_functional_tensor": [\n"def _from_functional_tensor(t: Tensor) -> Tensor: ..."\n],\n"_to_functional_tensor": [\n"def _to_functional_tensor(t: Tensor) -> Tensor: ..."\n],\n"_functionalize_replace": [\n
if "..." in hint:  # function or method\nassert hint.endswith("..."), f"Hint `{hint}` does not end with '...'"\nhint = hint[:-3]  # remove "..."\nreturn "\n    ".join([hint, 'r"""'] + docstr.split("\n") + ['"""', "..."])\n
f"def fractional_max_pool{d}d({{}}) -> {{}}: ...".format(\n", ".join(\n[\nf"{INPUT}",\nf"{KERNEL_SIZE}",\n"output_size: Union[_int, _size]",\n"_random_samples: Tensor",\n]\n),\n"Tuple[Tensor, Tensor]",\n
"double",\n"float16",\n"bfloat16",\n"float8_e4m3fn",\n"float8_e4m3fnuz",\n"float8_e5m2",\n"float8_e5m2fnuz",\n"half",\n"uint8",\n"uint16",\n"uint32",\n"uint64",\n"int8",\n"int16",\n"short",\n"int32",\n
],\n),\n**get_max_pool_dispatch(\nf"fractional_max_pool{d}d",\n[\nf"{INPUT}",\nf"{KERNEL_SIZE}",\n"output_size: Optional[Union[_int, _size]] = None",\n"output_ratio: Optional[_ratio_any_t] = None",\n"{return_indices}",\n
t,\n]\n* chunks_\nfor t, in_dim in zip(flat_args_, flat_in_dims_)\n)\n# transpose chunk dim and flatten structure\n# chunks_flat_args is a list of flatten args\nchunks_flat_args = zip(*flat_args_chunks)\n
vmap is useful for handling batch dimensions: one can write a function\n``func`` that runs on examples and then lift it to a function that can\ntake batches of examples with ``vmap(func)``. vmap can also be used to\n
randomness,\n**kwargs,\n)\n\nreturn wrapped_with_chunks\n\n\n@exposed_in("torch.func")\ndef grad(func: Callable, argnums: argnums_t = 0, has_aux: bool = False) -> Callable:\n"""``grad`` operator helps computing gradients of ``func`` with respect to the\n
>>>     return ((y - target) ** 2).mean()  # MSELoss\n>>>\n>>> weights = torch.randn(feature_size, requires_grad=True)\n>>> examples = torch.randn(batch_size, feature_size)\n>>> targets = torch.randn(batch_size)\n
rummaging through docs, use :func:`vmap` to construct a new function.\n\n>>> torch.dot                            # [D], [D] -> []\n>>> batched_dot = torch.func.vmap(torch.dot)  # [N, D], [N, D] -> [N]\n
# chunks_flat_args is a list of flatten args\nchunks_flat_args = zip(*flat_args_chunks)\nreturn chunks_flat_args\n\n@functools.wraps(func)\ndef wrapped_with_chunks(*args, **kwargs):\n_check_out_dims_is_int_or_int_pytree(out_dims, func)\n
f"vmap: chunk_size should be None or greater than 0. (got {chunk_size})"\n)\n\ndef wrapped(*args, **kwargs):\nreturn vmap_impl(\nfunc, in_dims, out_dims, randomness, chunk_size, *args, **kwargs\n)\n\n
\n@exposed_in("torch.func")\ndef grad(func: Callable, argnums: argnums_t = 0, has_aux: bool = False) -> Callable:\n"""``grad`` operator helps computing gradients of ``func`` with respect to the\ninput(s) specified by ``argnums``. This operator can be nested to\n
)\nexample_inputs = (torch.randn(2, 2),)\nm = M().eval()\nm = capture_pre_autograd_graph(m, example_inputs)\nm = prepare_pt2e(m, quantizer)\n# Use a linear count instead of names because the names might change, but\n
}\nnode_list = [\ntorch.ops.quantized_decomposed.dequantize_per_tensor.default,\ntorch.ops.quantized_decomposed.dequantize_per_tensor.default,\ntorch.ops.aten.add.Tensor,\ntorch.ops.quantized_decomposed.quantize_per_tensor.default,\n
model_fx,\nqconfig_mapping,\nexample_inputs,\nbackend_config=get_qnnpack_backend_config(),\n)\nmodel_fx(*example_inputs)\nmodel_fx = _convert_to_reference_decomposed_fx(model_fx)\n\nwith torchdynamo.config.patch(allow_rnn=True):\n
default_dynamic_qconfig,\nobserver,\nQConfig,\nQConfigMapping,\n)\nfrom torch.ao.quantization.backend_config import get_qnnpack_backend_config\nfrom torch.ao.quantization.qconfig import (\ndefault_per_channel_symmetric_qnnpack_qconfig,\n
is_per_channel=True,\nis_dynamic=True,\nweight_qmin=0,\nweight_qmax=15,\n)\nquantizer.set_global(quantization_config)\nm_eager = TestHelperModules.TwoLinearModule().eval()\n\nnode_occurrence = {\n# input and output are using quantize_per_tensor and weight is using quantize_per_channel\n
\nm = convert_pt2e(m)\nnode_occurrence = {\n# input and output are using quantize_per_tensor and weight is using quantize_per_channel\nns.call_function(\ntorch.ops.quantized_decomposed.quantize_per_tensor.default\n
\nwith override_quantized_engine("qnnpack"):\nmodel_fx = RNNDynamicModel("GRU")\nmodule_types = [torch.nn.GRU]\nniter = 10\nexample_inputs = (\n# input_tensor\ntorch.tensor([[100, -155], [-155, 100], [100, -155]], dtype=torch.float)\n
torch.ops.quantized_decomposed.dequantize_per_tensor.default: 5,\n# quantize_per_channel for weights are const propagated\ntorch.ops.quantized_decomposed.quantize_per_channel.default: 0,\ntorch.ops.quantized_decomposed.dequantize_per_channel.default: 1,\n
unittest.skip("testing takes an unreasonably long time, #79528"),\n"TestCommon",\n"test_compare_cpu",\n),\n),\nsupports_one_python_scalar=True,\nsupports_autograd=False,\n),\nUnaryUfuncInfo(\n"special.modified_bessel_i0",\n
unittest.skip(\n"Skipping - testing takes an unreasonably long time, #79528"\n)\n),\nDecorateInfo(unittest.skip("Skipped!"), "TestCudaFuserOpInfo"),\nDecorateInfo(unittest.skip("Skipped!"), "TestNNCOpInfo"),\n
# >>> scipy.special.polygamma(0, np.array(501, dtype=np.float32)).dtype\n# dtype('float64')\n# >>> scipy.special.polygamma(0, np.array([501], dtype=np.float32)).dtype\n# dtype('float32')\n#\n# Thus we cast output to the default torch dtype or preserve double\n
op_db=op_db,\ndecorators=(\nprecisionOverride(\n{\ntorch.float32: 1e-04,\ntorch.float64: 1e-05,\n},\n),\n),\n),\nElementwiseUnaryPythonRefInfo(\n"_refs.special.bessel_j1",\ntorch_opinfo_name="special.bessel_j1",\n
DecorateInfo(\nunittest.skip(\n"Skipping - testing takes an unreasonably long time, #79528"\n)\n),\nDecorateInfo(unittest.skip("Skipped!"), "TestCudaFuserOpInfo"),\nDecorateInfo(unittest.skip("Skipped!"), "TestNNCOpInfo"),\n
{\ntorch.float32: 1e-04,\ntorch.float64: 1e-05,\n},\n),\n),\ndtypes=all_types_and(torch.bool),\nref=scipy.special.y1 if TEST_SCIPY else None,\nsupports_autograd=False,\n),\nBinaryUfuncInfo(\n"special.chebyshev_polynomial_t",\n
\ndef reference_polygamma(x, n):\n# WEIRD `scipy.special.polygamma` behavior\n# >>> scipy.special.polygamma(0, np.array(501, dtype=np.float32)).dtype\n# dtype('float64')\n# >>> scipy.special.polygamma(0, np.array([501], dtype=np.float32)).dtype\n
else None,\ndtypes=all_types_and(torch.bool),\ndtypesIfCUDA=all_types_and(torch.bool),\nsample_inputs_func=sample_inputs_i0_i1,\ndecorators=(\nDecorateInfo(\ntoleranceOverride(\n{\ntorch.float32: tol(atol=1e-4, rtol=0),\n
assert t.size() == r.size()\nassert t.stride() == r.stride()\nelse:\nr = t\n# TODO: suppress guards\nreturn fake_mode.from_tensor(r)\nreturn t\n\ndef maybe_detach(t):\nif isinstance(t, torch.Tensor):\n
fmt_args = ", ".join(\nitertools.chain(\n(repr(pytree.tree_map(_fmt, a)) for a in orig_f_args),\n(\nf"{k}={pytree.tree_map(_fmt, v)}"\nfor k, v in orig_f_kwargs.items()\n),\n)\n)\nreturn f"{op}({fmt_args})"\n
\n\ndef check_metadata_matches(n, r, desc):\nassert callable(desc)\nn_vals, n_spec = pytree.tree_flatten(n)\nr_vals, r_spec = pytree.tree_flatten(r)\n# TODO: test the specs match; empirically  sometimes we have a tuple\n
# NB: enabling this is slow, don't do it in a hot loop.  This is purely\n# for debugging purposes.\n@contextmanager\ndef enable_crossref_functionalize():\nfor op in all_py_loaded_overloads():\nop._uncache_dispatch(torch._C.DispatchKey.Functionalize)\n
if f_tls:\ntorch._enable_functionalization(reapply_views=f_rv)\n\n\ndef check_tensor_metadata_matches(nv, rv, desc):\nassert callable(desc)\nassert nv.size() == rv.size(), f"{desc()}: sizes {nv.size()} != {rv.size()}"\n
yield\nfinally:\nfor op in all_py_loaded_overloads():\nop._uncache_dispatch(torch._C.DispatchKey.Functionalize)\n
\ndef maybe_detach(t):\nif isinstance(t, torch.Tensor):\nreturn t.detach()\nelse:\nreturn t\n\n# TODO: This probably does the wrong thing if you're running other\n# substantive modes with the normal op outside here\n
(\nf"{k}={pytree.tree_map(_fmt, v)}"\nfor k, v in orig_f_kwargs.items()\n),\n)\n)\nreturn f"{op}({fmt_args})"\n\ncheck_metadata_matches(f_r, r, desc)\nreturn r\n\nreturn handler\n\n\n# NB: enabling this is slow, don't do it in a hot loop.  This is purely\n
@property\ndef underlying_value(self):\nif isinstance(self.vt, variables.TensorVariable):\nx = self.vt.as_proxy().node.meta["example_value"]\nelif isinstance(self.vt, variables.TupleVariable):\nHashable = ConstDictVariable._HashableTracker\n
codegen.foreach(d.values())\nkeys = tuple(d.keys())\ncodegen.extend_output(codegen.create_call_function_kw(len(keys), keys, True))\n\ndef call_method(\nself,\ntx,\nname,\nargs: "List[VariableTracker]",\n
name == "update"\nand len(args) == 1\nand isinstance(\nargs[0],\n(\nConstDictVariable,\nListVariable,\nTupleVariable,\nListIteratorVariable,\n),\n)\nand self.mutable_local\n):\ntx.output.side_effects.mutation(self)\n
def _call_hasattr_customobj(self, tx, name: str) -> "VariableTracker":\n"""Shared method between DataClassVariable and CustomizedDictVariable where items are attrs"""\nif name in self.items or hasattr(self.user_cls, name):\n
from ..bytecode_transformation import (\ncreate_call_function,\ncreate_call_method,\ncreate_instruction,\ncreate_load_method,\n)\nfrom ..eval_frame import skip_code\nfrom ..exc import unimplemented\nfrom ..guards import GuardBuilder, install_guard\n
kv = "keys"\n\n@property\ndef set_items(self):\nreturn set(self.view_items)\n\n@property\ndef view_items_vt(self):\n# Returns an iterable of the unpacked items\nreturn [x.vt for x in self.view_items]\n
]\n)\n\ndef call_method(\nself,\ntx,\nname,\nargs: List["VariableTracker"],\nkwargs: Dict[str, "VariableTracker"],\n) -> "VariableTracker":\nif name == "__len__":\nreturn self.dv_dict.call_method(tx, name, args, kwargs)\n
bound = inspect.signature(user_cls).bind(*args, **kwargs)\nbound.apply_defaults()\nassert set(bound.arguments.keys()) == set(keys)\nitems = {}\nfor key in keys:\nval = bound.arguments[key]\nkey = ConstantVariable.create(key)\n
(np.int8, np.int16),\n(np.int16, np.int8),\n],\n)\n@parametrize("kind", [None, "sort", "table"])\ndef test_in1d_mixed_dtype(self, dtype1, dtype2, kind):\n"""Test that in1d works as expected for mixed dtype input."""\n
import torch._numpy as np\nfrom torch._numpy import unique\nfrom torch._numpy.testing import assert_array_equal, assert_equal\n\n\n@skipIf(numpy.__version__ < "1.24", reason="NP_VER: fails on NumPy 1.23.x")\n
True,\nTrue,\nTrue,\nTrue,\nTrue,\nTrue,\nTrue,\nTrue,\nTrue,\nTrue,\nFalse,\nTrue,\nTrue,\n]\nc = in1d(a, b, kind=kind)\nassert_array_equal(c, ec)\n\na = np.array([5, 7, 1, 2])\nb = np.array([2, 4, 3, 1, 5] * mult)\n
True,\nTrue,\nTrue,\nTrue,\nFalse,\nTrue,\nFalse,\nFalse,\nFalse,\n]\nc = in1d(a, b, kind=kind)\nassert_array_equal(c, ec)\n\nb = b + [5, 5, 4] * mult\nec = [\nTrue,\nTrue,\nTrue,\nTrue,\nTrue,\nTrue,\n
ea = np.array([2, 6, 7, 8])\nassert_array_equal(ea, a[ui1])\nassert_array_equal(ea, b[ui2])\n\n# non1d, not assumed to be uniqueinputs\na = np.array([[2, 4, 5, 6, 6], [4, 7, 8, 7, 2]])\nb = np.array([[3, 2, 7, 7], [10, 12, 8, 7]])\n
a = np.array([5, 7, 1, 2])\nb = np.array([2, 4, 3, 1, 5])\n\nec = np.array([1, 2, 5])\nc = intersect1d(a, b, assume_unique=True)\nassert_array_equal(c, ec)\n\n# non-unique inputs\na = np.array([5, 5, 7, 1, 2])\n
c = setdiff1d(a, b)\nassert_array_equal(c, ec)\n\na = np.arange(21)\nb = np.arange(19)\nec = np.array([19, 20])\nc = setdiff1d(a, b)\nassert_array_equal(c, ec)\n\nassert_array_equal([], setdiff1d([], []))\n
\ndef test_manyways(self):\na = np.array([5, 7, 1, 2, 8])\nb = np.array([9, 8, 2, 4, 3, 1, 5])\n\nc1 = setxor1d(a, b)\naux1 = intersect1d(a, b)\naux2 = union1d(a, b)\nc2 = setdiff1d(aux2, aux1)\nassert_array_equal(c1, c2)\n
\nclass TestFSDPMixedPrecision(FSDPTest):\n@property\ndef world_size(self):\nraise ValueError("To be implemented by child classes")\n\ndef _get_simple_nested_model(\nself, param_dtype, run_checks, *fsdp_args, **fsdp_kwargs\n
@skip_if_lt_x_gpu(2)\ndef test_mp_embedding_default(self):\ndefault_mp_config = MixedPrecision(\nparam_dtype=torch.float16,\nbuffer_dtype=torch.float16,\nreduce_dtype=torch.float16,\n)\nself._test_mixed_precision_embedding_table(mp_config=default_mp_config)\n
if isinstance(x, torch.Tensor):\ntensors.append(x)\nfor x in kwargs.values():\nif isinstance(x, torch.Tensor):\ntensors.append(x)\n\n# reduce_dtype has higher priority than param_dtype, because mixed_precision\n
cast_forward_inputs,\nuse_full_prec_in_eval,\n) in itertools.product([True, False], [True, False], [True, False]):\nmp_config = MixedPrecision(\nparam_dtype=torch.float16,\nreduce_dtype=torch.float16,\n
full_precision_param_dtype=torch.float64,\nsharding_strategy=ShardingStrategy.FULL_SHARD,\nenable_sharded_grad_scaler=False,\n)\n\n\ninstantiate_parametrized_tests(TestFSDPMixedPrecisionSharded)\n\n\n
\nself.assertEqual(fsdp_bn, n_bn)\n# Would throw type mismatch issue without mixed precision autowrapping.\nloss = fsdp(inp).sum()\nloss.backward()\n\n@skip_if_lt_x_gpu(2)\ndef test_grads_reduced_precision(self):\n
fsdp = FSDP(model)\n\nfsdp(x).sum().backward()\n\nself.assertEqual(forward_inputs[model].dtype, torch.float32)\nself.assertEqual(forward_inputs[c1].dtype, torch.float32)\nself.assertEqual(forward_inputs[c2].dtype, torch.float16)\n
self._reduce_scatter_validate_mp,\norig_reduce_scatter,\nmp_config,\nTrue,\n)\nwith patch_reduce_scatter(test_reduce_scatter, full_precision_param_dtype):\nscaler = ShardedGradScaler(enabled=enable_sharded_grad_scaler)\n
\n\nop_bench_c2.generate_c2_test(add_long_configs + add_short_configs, AddBenchmark)\n\n\nif __name__ == "__main__":\nop_bench.benchmark_runner.main()\n
class AddBenchmark(op_bench_c2.Caffe2BenchmarkBase):\ndef init(self, M, N, K, dtype):\nself.input_one = self.tensor([M, N, K], dtype)\nself.input_two = self.tensor([M, N, K], dtype)\nself.output = self.tensor([M, N, K], dtype)\n
dtype=["int", "float"],\ntags=["long"],\n)\n\n\nadd_short_configs = op_bench.config_list(\nattrs=[\n[8, 16, 32, "int"],\n[16, 16, 64, "float"],\n[64, 64, 128, "int"],\n],\nattr_names=["M", "N", "K", "dtype"],\n
K=[2**x for x in range(0, 3)],\ndtype=["int", "float"],\ntags=["long"],\n)\n\n\nadd_short_configs = op_bench.config_list(\nattrs=[\n[8, 16, 32, "int"],\n[16, 16, 64, "float"],\n[64, 64, 128, "int"],\n
op_bench_c2.generate_c2_test(add_long_configs + add_short_configs, AddBenchmark)\n\n\nif __name__ == "__main__":\nop_bench.benchmark_runner.main()\n
],\nattr_names=["M", "N", "K", "dtype"],\ntags=["short"],\n)\n\n\nclass AddBenchmark(op_bench_c2.Caffe2BenchmarkBase):\ndef init(self, M, N, K, dtype):\nself.input_one = self.tensor([M, N, K], dtype)\n
[16, 16, 64, "float"],\n[64, 64, 128, "int"],\n],\nattr_names=["M", "N", "K", "dtype"],\ntags=["short"],\n)\n\n\nclass AddBenchmark(op_bench_c2.Caffe2BenchmarkBase):\ndef init(self, M, N, K, dtype):\n
],\nattr_names=["M", "N", "K", "dtype"],\ntags=["short"],\n)\n\n\nclass AddBenchmark(op_bench_c2.Caffe2BenchmarkBase):\ndef init(self, M, N, K, dtype):\nself.input_one = self.tensor([M, N, K], dtype)\n
\ndef test_failed_itemsetting(self):\nwith pytest.raises(TypeError):\nnp.fromiter([1, None, 3], dtype=int)\n\n# The following manages to hit somewhat trickier code paths:\niterable = ((2, 3, 4) for i in range(5))\n
# Test basic arithmetic function errors\nftype = np.obj2sctype(typecode)\nif np.dtype(ftype).kind == "f":\n# Get some extreme values for the type\nfi = np.finfo(ftype)\nft_tiny = fi._machar.tiny\nft_max = fi.max\n
\ndef test_logical_not_abs(self):\nassert_array_equal(~self.t, self.f)\nassert_array_equal(np.abs(~self.t), self.f)\nassert_array_equal(np.abs(~self.f), self.t)\nassert_array_equal(np.abs(self.f), self.f)\n
np.array([1.0]) / np.array([0.0])\n\nnp.seterr(divide="ignore")\nnp.array([1.0]) / np.array([0.0])\n\n@skipif(IS_WASM, reason="no wasm fp exception support")\ndef test_errobj(self):\nolderrobj = np.geterrobj()\n
assert_equal(Ar.shape, (0, 2))\n\nAr = np.resize(A, (2, 0))\nassert_equal(Ar.shape, (2, 0))\n\ndef test_reshape_from_zero(self):\n# See also gh-6740\nA = np.zeros(0, dtype=np.float32)\nAr = np.resize(A, (2, 1))\n
arr = [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\ntgt = [[5, 6, 7, 8, 9]]\nout = np.compress([0, 1], arr, axis=0)\nassert_equal(out, tgt)\n\ndef test_count_nonzero(self):\narr = [[0, 1, 7, 0, 0], [3, 0, 0, 2, 19]]\n
m = -2\nM = 4\nac = self.fastclip(a, m, M)\nact = self.clip(a, m, M)\nassert_array_equal(ac, act)\n\ndef test_array_double(self):\n# Test native double input with array min/max.\na = self._generate_data(self.nr, self.nc)\n
assert_(np.size(A) == 6)\nassert_(np.size(A, 0) == 2)\nassert_(np.size(A, 1) == 3)\n\ndef test_squeeze(self):\nA = [[[1, 1, 1], [2, 2, 2], [3, 3, 3]]]\nassert_equal(np.squeeze(A).shape, (3, 3))\nassert_equal(np.squeeze(np.zeros((1, 3, 1))).shape, (3,))\n
@property\ndef _non_persistent_buffers_set(self):\n...\n\n@property\ndef _load_hook(self):\n...\n\n@property\ndef _initialize_hook(self):\n...\n\n\nclass LazyModuleMixin:\nr"""A mixin for modules that lazily initialize parameters, also known as "lazy modules".\n
"""\n\ndef _register_load_state_dict_pre_hook(self, hook):\n...\n\ndef register_forward_pre_hook(self, hook, *, prepend=False, with_kwargs=False):\n...\n\ndef _lazy_load_hook(\nself, state_dict, prefix, local_metadata, strict,\n
After construction, networks with lazy modules should first\nbe converted to the desired dtype and placed on the expected device.\nThis is because lazy modules only perform shape inference so the usual dtype\n
derive the shapes of their parameters from the first input(s)\nto their forward method. Until that first forward they contain\n:class:`torch.nn.UninitializedParameter` s that should not be accessed\nor used, and afterward they contain regular :class:`torch.nn.Parameter` s.\n
[ 0.0907],\n[ 0.6708],\n[-0.5223],\n[-0.9028],\n[ 0.2851],\n[-0.4537],\n[ 0.6813],\n[ 0.5766],\n[-0.8678]])),\n('fc1.bias',\ntensor([-1.8832e+25,  4.5636e-41, -1.8832e+25,  4.5636e-41, -6.1598e-30,\n4.5637e-41, -1.8788e+22,  4.5636e-41, -2.0042e-31,  4.5637e-41])),\n
[-0.8678]])),\n('fc1.bias',\ntensor([-1.8832e+25,  4.5636e-41, -1.8832e+25,  4.5636e-41, -6.1598e-30,\n4.5637e-41, -1.8788e+22,  4.5636e-41, -2.0042e-31,  4.5637e-41])),\n('fc2.weight',\ntensor([[ 0.1320,  0.2938,  0.0679,  0.2793,  0.1088, -0.1795, -0.2301,  0.2807,\n
\n>>> lazy_mlp = LazyMLP()\n>>> # The state dict shows the uninitialized parameters\n>>> lazy_mlp.state_dict()\nOrderedDict([('fc1.weight', Uninitialized parameter),\n('fc1.bias',\ntensor([-1.8832e+25,  4.5636e-41, -1.8832e+25,  4.5636e-41, -6.1598e-30,\n
\ndef _replicate_for_data_parallel(self: _LazyProtocol):\nraise RuntimeError('Modules with uninitialized parameters can\'t be used with `DataParallel`. '\n'Run a dummy forward pass to correctly initialize the modules')\n
class Stash(nn.Module):\ndef forward(self, input):\nyield stash("foo", input)\nreturn input * 2  # noqa: B901\n\nl1 = Stash()\n\nassert len(skip_tracker.tensors) == 0\n\nwith use_skip_tracker(skip_tracker):\n
# LICENSE file in the root directory of this source tree.\nimport pytest\n\nimport torch\nfrom torch import nn\n\nfrom torch.distributed.pipeline.sync.skip import pop, skippable, stash\nfrom torch.distributed.pipeline.sync.skip.tracker import SkipTracker, use_skip_tracker\n
def forward(self, input):\nfoo = yield pop("foo")\nreturn foo\n\nl1 = Stash()\nl2 = Pop()\n\noutput = l2(l1(torch.tensor(42)))\n\nassert output.item() == 42\n\n\ndef test_declare_but_not_use():\n@skippable(stash=["foo"])\n
class Stash(nn.Module):\ndef forward(self, input):\nyield stash("foo", input)\nreturn input * 2  # noqa: B901\n\n@skippable()\nclass Pop(nn.Module):\ndef forward(self, input):\nfoo = yield pop("foo")\n
\n@pytest.fixture(autouse=True)\ndef skip_tracker():\nskip_tracker = SkipTracker()\nwith use_skip_tracker(skip_tracker):\nyield skip_tracker\n\n\ndef test_stash(skip_tracker):\n@skippable(stash=["foo"])\n
\n@pytest.fixture(autouse=True)\ndef skip_tracker():\nskip_tracker = SkipTracker()\nwith use_skip_tracker(skip_tracker):\nyield skip_tracker\n\n\ndef test_stash(skip_tracker):\n@skippable(stash=["foo"])\n
\noutput = l2(l1(torch.tensor(42)))\n\nassert output.item() == 42\n\n\ndef test_declare_but_not_use():\n@skippable(stash=["foo"])\nclass Stash(nn.Module):\ndef forward(self, input):\nreturn input * 2\n
\nl1 = Stash()\nl2 = Pop()\n\nwith pytest.raises(RuntimeError):\nl1(torch.tensor(42))\n\nwith pytest.raises(RuntimeError):\nl2(torch.tensor(42))\n\n\ndef test_stash_not_declared():\n@skippable()\nclass Stash(nn.Module):\n
return ops.ne(self, other)\n\ndef __gt__(self, other):\nreturn ops.gt(self, other)\n\ndef __ge__(self, other):\nreturn ops.ge(self, other)\n\ndef __and__(self, other):\nreturn ops.bitwise_and(self, other)\n
Sentinel indicating that a global variable is unset ala None.  Typically,\nattempting to access the global variable before it's set is an error, but with\nNullHandler it won't fail until you try to access an attribute on it.\n
import torch\nfrom torch._inductor.debug import DebugContext\nfrom torch._inductor.graph import GraphLowering\nfrom torch._inductor.ir import InterpreterShim\nfrom torch._subclasses import FakeTensorMode\n
return ops.ne(self, other)\n\ndef __gt__(self, other):\nreturn ops.gt(self, other)\n\ndef __ge__(self, other):\nreturn ops.ge(self, other)\n\ndef __and__(self, other):\nreturn ops.bitwise_and(self, other)\n
@property\ndef interpreter(self):\nreturn _interpreter._get_handler()\n\n@property\ndef aot_compilation(self):\nreturn _aot_compilation._get_handler()\n\n@property\ndef current_node(self):\nreturn _current_node._get_handler()\n
return ops.bitwise_and(self, other)\n\ndef __or__(self, other):\nreturn ops.bitwise_or(self, other)\n\ndef __xor__(self, other):\nreturn ops.bitwise_xor(self, other)\n\ndef __invert__(self):\nreturn ops.bitwise_not(self)\n
def __gt__(self, other):\nreturn ops.gt(self, other)\n\ndef __ge__(self, other):\nreturn ops.ge(self, other)\n\ndef __and__(self, other):\nreturn ops.bitwise_and(self, other)\n\ndef __or__(self, other):\n
return ops.pow(self, other)\n\ndef __lt__(self, other):\nreturn ops.lt(self, other)\n\ndef __le__(self, other):\nreturn ops.le(self, other)\n\ndef __eq__(self, other):\nreturn ops.eq(self, other)\n\ndef __ne__(self, other):\n
MinMaxObserver,\nMovingAverageMinMaxObserver,\nMovingAveragePerChannelMinMaxObserver,\nNoopObserver,\nObserverBase,\nPerChannelMinMaxObserver,\nPlaceholderObserver,\nRecordingObserver,\n)\n
default_observer,\ndefault_per_channel_weight_observer,\ndefault_placeholder_observer,\ndefault_weight_observer,\nget_observer_state_dict,\nHistogramObserver,\nload_observer_state_dict,\nMinMaxObserver,\n
"""\nfrom torch.ao.quantization.observer import (\n_is_activation_post_process,\n_is_per_channel_script_obs_instance,\n_ObserverBase,\n_PartialWrapper,\n_with_args,\n_with_callable_args,\nABC,\ndefault_debug_observer,\n
_PartialWrapper,\n_with_args,\n_with_callable_args,\nABC,\ndefault_debug_observer,\ndefault_dynamic_quant_observer,\ndefault_float_qparams_observer,\ndefault_histogram_observer,\ndefault_observer,\ndefault_per_channel_weight_observer,\n
)\n
NoopObserver,\nObserverBase,\nPerChannelMinMaxObserver,\nPlaceholderObserver,\nRecordingObserver,\n)\n
RecordingObserver,\n)\n
MinMaxObserver,\nMovingAverageMinMaxObserver,\nMovingAveragePerChannelMinMaxObserver,\nNoopObserver,\nObserverBase,\nPerChannelMinMaxObserver,\nPlaceholderObserver,\nRecordingObserver,\n)\n
# other dims are passed through\nif end_dim < 0:\nend_dim += ndim\nresults: List[DimSpec] = [InputDim(i) for i in range(start_dim)]\nresults.append(\nFlatten.new(tuple(InputDim(i) for i in range(start_dim, end_dim + 1)))\n
InputDim(i)\nfor i, s in enumerate(shape)\nif s > 1 or (dim is not None and i != normalize_dim(dim, len(shape)))\n)\n\n\ndef dim_unsqueeze(ndim: int, dim: int) -> DimMap:\ndims = tuple(InputDim(i) for i in range(ndim))\n
f = 1\nelse:\nf = from_size[from_idx]\nfrom_group_dim.append(from_idx)\nfrom_idx += 1\n\nif to_idx >= to_len:\nt = 1\nelse:\nt = to_size[to_idx]\nto_group_shape.append(t)\nto_idx += 1\n\n# if any of the groups is singleton, great, we need to backtrack though\n
\nif to_idx >= to_len:\nt = 1\nelse:\nt = to_size[to_idx]\nto_group_shape.append(t)\nto_idx += 1\n\n# if any of the groups is singleton, great, we need to backtrack though\nif f == 1 and t != 1:\n# produces ([1], [])\n
from_idx += 1\n\nif to_idx >= to_len:\nt = 1\nelse:\nt = to_size[to_idx]\nto_group_shape.append(t)\nto_idx += 1\n\n# if any of the groups is singleton, great, we need to backtrack though\nif f == 1 and t != 1:\n
return (Singleton(),) * max(0, min_dims - ndim) + tuple(\nInputDim(i) for i in range(ndim)\n)\n\n\ndef dim_atleast_3d(ndim: int) -> DimMap:\nif ndim == 0:\nreturn (Singleton(), Singleton(), Singleton())\n
\ndef dim_squeeze(shape: Shape, dim: Optional[int] = None) -> DimMap:\n# FIXME: this is wrong when dim=None and one of the dimensions\n# equals size of the mesh. For example squeeze(DTensor(tensor(4), Shard[0])) could\n
\ninput_dim: DimSpec\ntimes: int\n\n@classmethod\ndef new(cls, dim: DimSpec, times: int) -> DimSpec:\nif times == 1:\nreturn dim\nelif isinstance(dim, Singleton):\n# repeating a singleton is the same as broadcasting it\n
flat = tree_flatten_spec(child, child_spec, exact_structural_match)\nresult += flat\nreturn result\n\n\ndef _dict_flatten_spec(d: Dict[Any, Any], spec: TreeSpec) -> List[Any]:\nreturn [d[k] for k in spec.context]\n
)\nregister_pytree_flatten_spec(\nnamedtuple,  # type: ignore[arg-type]\n_namedtuple_flatten_spec,\n_namedtuple_flatten_spec_exact_match,\n)\n
from typing import Any, Callable, Dict, List, NamedTuple, Optional, Tuple, Type\n\nimport torch.return_types\n\nfrom torch.utils._pytree import PyTree, TreeSpec\n\nFlattenFuncSpec = Callable[[PyTree, TreeSpec], List]\n
)\nregister_pytree_flatten_spec(\nnamedtuple,  # type: ignore[arg-type]\n_namedtuple_flatten_spec,\n_namedtuple_flatten_spec_exact_match,\n)\n
register_pytree_flatten_spec(\nnamedtuple,  # type: ignore[arg-type]\n_namedtuple_flatten_spec,\n_namedtuple_flatten_spec_exact_match,\n)\n
def _namedtuple_flatten_spec_exact_match(d: NamedTuple, spec: TreeSpec) -> bool:\nreturn len(d) == spec.num_children\n\n\nregister_pytree_flatten_spec(dict, _dict_flatten_spec, _dict_flatten_spec_exact_match)\n
\nfrom torch.utils._pytree import PyTree, TreeSpec\n\nFlattenFuncSpec = Callable[[PyTree, TreeSpec], List]\nFlattenFuncExactMatchSpec = Callable[[PyTree, TreeSpec], bool]\n\nSUPPORTED_NODES: Dict[Type[Any], FlattenFuncSpec] = {}\n
\ndef _namedtuple_flatten_spec(d: NamedTuple, spec: TreeSpec) -> List[Any]:\nreturn [d[i] for i in range(spec.num_children)]\n\n\ndef _dict_flatten_spec_exact_match(d: Dict[Any, Any], spec: TreeSpec) -> bool:\n
return new_level\n\n\ndef exit_dual_level(*, level=None):\nr"""Exit a forward grad level.\n\nThis function deletes all the gradients associated with this\nlevel. Only deleting the latest entered level is allowed.\n
"""\nglobal _current_level\nif level is None:\nlevel = _current_level\nif level != _current_level:\nraise RuntimeError(\n"Trying to exit a forward AD level that was not the last one "\n"that was created. This is not supported."\n
#   File ".../torch/_decomp/__init__.py", line 1585\n#     else:\n#         buffer = z\n#     return min - torch.log1p(z), buffer\n#     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n_maybe_load_decompositions()\n
>>> _, grad_after = unpack_dual(out)\n>>> grad is None\nTrue\n\nPlease see the `forward-mode AD tutorial <https://pytorch.org/tutorials/intermediate/forward_ad_usage.html>`__\nfor detailed steps on how to use this API.\n
>>> # xdoctest: +SKIP("Undefined variables")\n>>> with dual_level():\n...     inp = make_dual(x, v)\n...     out = f(inp)\n...     y, jvp = unpack_dual(out)\n\nPlease see the `forward-mode AD tutorial <https://pytorch.org/tutorials/intermediate/forward_ad_usage.html>`__\n
if level is None:\nlevel = _current_level\nif level != _current_level:\nraise RuntimeError(\n"Trying to exit a forward AD level that was not the last one "\n"that was created. This is not supported."\n
from torch._decomp import decompositions_for_jvp  # noqa: F401\n\n\ndef make_dual(tensor, tangent, *, level=None):\nr"""Associate a tensor value with its tangent to create a "dual tensor" for forward AD gradient computation.\n
# Import from torch._decomp import decompositions_for_jvp to register\n# decompositions for jvp to the jit registry\n#\n# FIXME: We specify that __debug__ must be True because\n# if python is run with -OO or -O flags (i.e., __debug__ is False), we encounter the\n
x = torch.randn(D)\n\n# Let's think of ``predict`` as a function that maps the input ``x`` from R^D -> R^D.\n# PyTorch Autograd computes vector-Jacobian products. In order to compute the full\n# Jacobian of this R^D -> R^D function, we would have to compute it row-by-row\n
def compute_jac(xp):\njacobian_rows = [\ntorch.autograd.grad(predict(weight, bias, xp), xp, vec)[0]\nfor vec in unit_vectors\n]\nreturn torch.stack(jacobian_rows)\n\n\njacobian = compute_jac(xp)\n\n# Instead of computing the jacobian row-by-row, we can use ``vmap`` to get rid\n
# Instead of computing the jacobian row-by-row, we can use ``vmap`` to get rid\n# of the for-loop and vectorize the computation. We can't directly apply vmap\n# to PyTorch Autograd; instead, functorch provides a ``vjp`` transform:\n
=============================\nJacobians, hessians, and more\n=============================\n\nComputing jacobians or hessians are useful in a number of non-traditional\ndeep learning models. It is difficult (or annoying) to compute these quantities\n
compute_batch_hessian = vmap(hessian(predict, argnums=2), in_dims=(None, None, 0))\nbatch_hess = compute_batch_hessian(weight, bias, x)\n
# the vmap-vjp composition to compute jacobians. ``jacrev`` accepts an argnums\n# argument that says which argument we would like to compute Jacobians with\n# respect to.\nfrom functorch import jacrev\n
\n# Instead of computing the jacobian row-by-row, we can use ``vmap`` to get rid\n# of the for-loop and vectorize the computation. We can't directly apply vmap\n# to PyTorch Autograd; instead, functorch provides a ``vjp`` transform:\n
torch.manual_seed(0)\n\n\n######################################################################\n# Setup: Comparing functorch vs the naive approach\n# --------------------------------------------------------------------\n
)\n\nparser.add_argument(\n"--device",\nhelp="Run tests on the provided architecture (cpu, cuda)",\ndefault="None",\n)\n\nargs, _ = parser.parse_known_args()\n\nif args.omp_num_threads:\n# benchmark_utils.set_omp_threads sets the env variable OMP_NUM_THREADS\n
)\n\nparser.add_argument(\n"--device",\nhelp="Run tests on the provided architecture (cpu, cuda)",\ndefault="None",\n)\n\nargs, _ = parser.parse_known_args()\n\nif args.omp_num_threads:\n# benchmark_utils.set_omp_threads sets the env variable OMP_NUM_THREADS\n
help="Comma-delimited list of frameworks to test (Caffe2, PyTorch)",\ndefault="Caffe2,PyTorch",\n)\n\nparser.add_argument(\n"--device",\nhelp="Run tests on the provided architecture (cpu, cuda)",\ndefault="None",\n
"--tag_filter",\nhelp="tag_filter can be used to run the shapes which matches the tag. (all is used to run all the shapes)",\ndefault="short",\n)\n\n# This option is used to filter test cases to run.\n
default=0,\n)\n\nparser.add_argument(\n"--warmup-iterations",\n"--warmup_iterations",\nhelp="Number of iterations to ignore before measuring performance",\ndefault=100,\ntype=int,\n)\n\nparser.add_argument(\n
default=None,\ntype=int,\n)\n\nparser.add_argument(\n"--report-aibench",\n"--report_aibench",\ntype=benchmark_utils.str2bool,\nnargs="?",\nconst=True,\ndefault=False,\nhelp="Print result when running on AIBench",\n
"--omp-num-threads",\n"--omp_num_threads",\nhelp="Number of OpenMP threads used in PyTorch/Caffe2 runtime",\ndefault=None,\ntype=int,\n)\n\nparser.add_argument(\n"--mkl-num-threads",\n"--mkl_num_threads",\n
\n"""Performance microbenchmarks's main binary.\n\nThis is the main function for running performance microbenchmark tests.\nIt also registers existing benchmark tests via Python module imports.\n"""\n
\n\ndef to_str(item):\nif isinstance(item, float):\nreturn f"{item:.4g}"\nreturn str(item)\n\n\ndef print_header(colwidth=16, sep=" "):\nitems = []\nfor item in BenchResult._fields:\nitems.append(fit_str(item))\n
del bench_args["cuda_pointwise_loop_level"]\ndel bench_args["cuda_pointwise_block_count"]\ndel bench_args["cuda_pointwise_block_size"]\n\nresults = {}\nif should_bench_varlen_lstms:\nif args.nloops + args.warmup > 30:\n
def to_str(item):\nif isinstance(item, float):\nreturn f"{item:.4g}"\nreturn str(item)\n\n\ndef print_header(colwidth=16, sep=" "):\nitems = []\nfor item in BenchResult._fields:\nitems.append(fit_str(item))\n
\nfwd_time = fwd_start_event.elapsed_time(fwd_end_event)\nbwd_time = bwd_start_event.elapsed_time(bwd_end_event)\nreturn fwd_time, bwd_time\n\ncreator_args = creator_args = {\n"seqLength": seqLength,\n
info_fwd=fwd_times,\navg_bwd=bwd_times.mean().item(),\nstd_bwd=bwd_times.std().item(),\ninfo_bwd=bwd_times,\n)\n\n\ndef print_stderr(*args, **kwargs):\nkwargs["file"] = sys.stderr\nreturn print(*args, **kwargs)\n
fwd_end_event.record()\n\n# XXX: Use if need to print something\n# print(modeldef.forward.graph_for(*modeldef.inputs))\n\nif modeldef.backward_setup is not None:\nbackward_input = modeldef.backward_setup(forward_output)\n
seqLength=100,\nnumLayers=1,\ninputSize=512,\nhiddenSize=512,\nminiBatch=64,\ndevice="cuda",\nseed=None,\n):\ndef train_batch(modeldef):\n# CUDA events for timing\nif device == "cuda":\ntimer_class = torch.cuda.Event\n
modeldef = rnn_creator(**creator_args)\n\n[train_batch(modeldef) for _ in range(warmup)]\n\nresults = [train_batch(modeldef) for _ in range(nloops)]\nfwd_times, bwd_times = zip(*results)\n\nfwd_times = torch.tensor(fwd_times)\n
\nx = torch.rand(4, 3, 10, 10)\npipe(x).local_value().mean().backward()\nbn(x).mean().backward()\n\nassert pipe[0].weight.grad is not None\nassert pipe[0].bias.grad is not None\n\nassert torch.allclose(pipe[0].weight.grad, bn.weight.grad, atol=1e-4)\n
@skip_if_no_cuda\ndef test_merged_partitions(setup_rpc):\na = nn.Linear(1, 1).to(0)\nb = nn.Sequential(nn.Linear(1, 1), nn.Linear(1, 2)).to(0)\nc = nn.Linear(1, 1)\nd = nn.Linear(1, 2)\n\nmodel = nn.Sequential(a, b, c, d)\n
# Need atleast one tensor.\nwith pytest.raises(TypeError):\nmodel(a, None, c, None)\n\n\n@pytest.mark.parametrize("checkpoint", ["never", "always", "except_last"])\ndef test_no_tensor_output(checkpoint, setup_rpc):\n
nn.Sequential(nn.Linear(32, 16).cuda(0), nn.Linear(16, 8).cuda(0)),\nnn.Sequential(nn.Linear(8, 4).cuda(1), nn.Linear(4, 2).cuda(1)),\n)\n\npipe = Pipe(model)\nout = pipe(torch.rand(10, 32).cuda(0))\n
# several methods in its namespace.\nwith pytest.raises(AttributeError):\nmodel.a\n\n\ndef test_verify_module_non_sequential(setup_rpc):\nwith pytest.raises(\nTypeError, match="module must be nn.Sequential to be partitioned"\n
model = nn.Sequential(a, b, c, d)\nmodel = Pipe(model)\n\nassert isinstance(model.partitions, nn.ModuleList)\nassert isinstance(model.partitions[0], PipeSequential)\nassert isinstance(model.partitions[1], PipeSequential)\n
res = model(a, b, NoChunk(c)).local_value()\nassert torch.allclose(a, res[0])\nassert [b] * 5 == res[1]\n# c gets replicated due to NoChunk and the same tensor gets concatenated 5\n# times in the output.\n
model = nn.Sequential(nn.Linear(1, 1))\nmodel = Pipe(model)\n\na = torch.rand(1)\nb = torch.rand(1)\n\n# TypeError: forward() takes 2 positional arguments but 3 were given\nwith pytest.raises(TypeError):\n
cross_product_configs={\n"device": ["cpu", "cuda"],\n},\ntags=["short"],\n)\n\nconv_2d_configs_long = op_bench.cross_product_configs(\nIC=[128, 256],\nOC=[128, 256],\nkernel=[3],\nstride=[1, 2],\nN=[4],\n
sparse=[True, False],\ninclude_last_offset=[True, False],\ndevice=["cpu"],\ntags=["short"],\n)\n\nembedding_short_configs = op_bench.cross_product_configs(\nnum_embeddings=[10, 120, 1000, 2300],\nembedding_dim=[64],\n
"N",\n"H",\n"W",\n"G",\n"pad",\n],\nattrs=[\n[256, 256, 3, 1, 1, 16, 16, 1, 0],\n],\ncross_product_configs={\n"device": ["cpu", "cuda"],\n},\ntags=["short"],\n)\n\nconv_2d_configs_long = op_bench.cross_product_configs(\n
# Configs for Conv2dPointwise\nconv_2d_pw_configs_short = op_bench.config_list(\nattr_names=[\n"IC",\n"OC",\n"stride",\n"N",\n"H",\n"W",\n"G",\n"pad",\n],\nattrs=[\n[256, 256, 1, 1, 16, 16, 1, 0],\n],\n
G=[1],\npad=[0],\ndevice=["cpu", "cuda"],\ntags=["long"],\n)\n\n# Configs for Conv3d and ConvTranspose3d\nconv_3d_configs_short = op_bench.config_list(\nattr_names=["IC", "OC", "kernel", "stride", "N", "D", "H", "W"],\n
"pad",\n],\nattrs=[\n[256, 256, 3, 1, 1, 16, 16, 1, 0],\n],\ncross_product_configs={\n"device": ["cpu", "cuda"],\n},\ntags=["short"],\n)\n\nconv_2d_configs_long = op_bench.cross_product_configs(\nIC=[128, 256],\n
# Configs for Conv2dPointwise\nconv_2d_pw_configs_short = op_bench.config_list(\nattr_names=[\n"IC",\n"OC",\n"stride",\n"N",\n"H",\n"W",\n"G",\n"pad",\n],\nattrs=[\n[256, 256, 1, 1, 16, 16, 1, 0],\n],\n
W=[32],\nG=[1],\npad=[0],\ndevice=["cpu", "cuda"],\ntags=["long"],\n)\n\n# Configs for Conv2dPointwise\nconv_2d_pw_configs_short = op_bench.config_list(\nattr_names=[\n"IC",\n"OC",\n"stride",\n"N",\n"H",\n
nested, and may be invoked with positional and keyword arguments.\nAsynchronous execution will only occur when run in TorchScript. If run in pure python,\n`fork` will not execute in parallel. `fork` will also not execute in parallel when invoked\n
`fork` will not execute in parallel. `fork` will also not execute in parallel when invoked\nwhile tracing, however the `fork` and `wait` calls will be captured in the exported IR Graph.\n\n.. warning::\n
\n.. warning::\n`fork` tasks will execute non-deterministically. We recommend only spawning\nparallel fork tasks for pure functions that do not modify their inputs,\nmodule attributes, or global state.\n
`fork` will return immediately, so the return value of `func` may not have been computed yet. To force completion\nof the task and access the return value invoke `torch.jit.wait` on the Future. `fork` invoked\n
while tracing, however the `fork` and `wait` calls will be captured in the exported IR Graph.\n\n.. warning::\n`fork` tasks will execute non-deterministically. We recommend only spawning\nparallel fork tasks for pure functions that do not modify their inputs,\n
"""\n\nimport torch\nfrom torch._jit_internal import Future\nfrom torch.jit._builtins import _register_builtin\n\nfrom torch.utils import set_module\n\nset_module(Future, "torch.jit")\n\n\ndef fork(func, *args, **kwargs):\n
_register_builtin(wait, "aten::wait")\n
* torch.jit.wait\n\nThis is not intended to be imported directly; please use the exposed\nfunctionalities in `torch.jit`.\n"""\n\nimport torch\nfrom torch._jit_internal import Future\nfrom torch.jit._builtins import _register_builtin\n
\nfor requires_grad in [True, False]:\ntensor_to_shard = torch.randn(\n3 * device_count // 2, 3, requires_grad=requires_grad\n)\ndist_tensor = distribute_tensor(\ntensor_to_shard, device_mesh, shard_spec\n
3 * device_count, 3, requires_grad=requires_grad\n)\ndist_tensor = distribute_tensor(tensor_to_shard, device_mesh, shard_spec)\n# TODO(yeounoh) switch to DTensor API when XLAShardedTensor inherits DTensor\n
class DTensorXLAIntegrationTest(TestCase):\nclass SimpleLinear(nn.Module):\ndef __init__(self):\nsuper(DTensorXLAIntegrationTest.SimpleLinear, self).__init__()\nself.fc1 = nn.Linear(128, 64)\nself.relu = nn.ReLU()\n
import torch_xla  # type:ignore[import]  # noqa: F401\nexcept ImportError as exc:\nraise unittest.SkipTest("torch_xla is not installed.") from exc\nself.device_type = "xla"\nfunc(self, *args, **kwargs)  # type: ignore[misc]\n
self.assertTrue(dist_tensor.is_leaf)\n\n@with_xla\ndef test_xla_distribute_tensor_2d(self):\nimport torch_xla.runtime as xr  # type:ignore[import]\n\ndevice_count = xr.global_runtime_device_count()\nif device_count > 1:\n
self.assertTrue(\ntorch_xla._XLAC._get_xla_sharding_spec(sharded_model.fc1.weight) != ""\n)\nself.assertTrue(\ntorch_xla._XLAC._get_xla_sharding_spec(sharded_model.fc2.weight) != ""\n)\n\n\nif __name__ == "__main__":\n
# annoate fc1 and fc2\nif isinstance(mod, nn.Linear):\nfor name, param in mod.named_parameters():\n# annotate the parameter tensors directly\ndistribute_tensor(param, mesh, shard_spec)\n\nsharded_model = distribute_module(model, device_mesh, shard_params)\n
import os\nimport unittest\nfrom functools import wraps\nfrom typing import Any, Callable, Dict, Tuple\n\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch.distributed._tensor import (\n
from contextlib import ExitStack\nfrom dataclasses import dataclass, field\nfrom typing import (\nAny,\ncast,\nDict,\nIterable,\nIterator,\nList,\nNamedTuple,\nno_type_check,\nOptional,\nSequence,\nSet,\n
_module_handle,\n_named_parameters_with_duplicates,\nclean_tensor_name,\n)\nfrom torch.distributed.fsdp._debug_utils import SimpleProfiler\nfrom torch.distributed.fsdp._flat_param import FlatParameter, FlatParamHandle\n
Flattens the optimizer state in ``full_optim_state_dict`` for a single\nflat parameter in ``fsdp_param_info`` corresponding to the unflattened\nparameter names in ``unflat_param_names``.\n\nArgs:\nfsdp_param_info (FSDPParamInfo): The FSDP state, the handle, and a\n
Returns:\ntorch.Tensor: A flat tensor containing the optimizer state\ncorresponding to ``state_name`` constructed by concatenating the\nunflattened parameter tensor states in ``pos_dim_tensors`` (using zero\n
are_pos_dim_tensors &= torch.is_tensor(v) and v.dim() > 0\nare_zero_dim_tensors &= _is_zero_dim_tensor(v)\nare_non_tensors &= not torch.is_tensor(v)\ntypes = {type(v) for v in non_none_state_values}\n
params = cast(List[nn.Parameter], list(optim_input))\nexcept TypeError as e:\nraise TypeError(\n"Optimizer input should be an iterable of Tensors or dicts, "\nf"but got {optim_input}"\n) from e\nif len(params) == 0:\n
if not dtype:\ndtype = info.dtype\nelse:\nassert dtype == info.dtype\nif numels[-1] == 0:\n_empty_ranks.add(rank)\n\nassert not empty_ranks or empty_ranks == _empty_ranks\nempty_ranks = _empty_ranks\n
output_states = _allgather_orig_param_states(\nfsdp_param_info,\ngathered_state_info,\ninput_states,\nshard_state,\nto_save,\ncpu_offload,\n)\nif to_save:\nfor key, idx in fsdp_param_info.param_indices.items():\n
def __repr__(self) -> str:\nreturn f"Batch[atomic={self.atomic!r}]({self._values!r})"\n\ndef __iter__(self):\nif self.atomic:\nyield self._values\nelse:\nyield from self._values\n\ndef __len__(self) -> int:\n
Retrieves the device for this microbatch.\n"""\nif self.atomic:\nreturn self._values.device  # type: ignore[union-attr]\n\nfor value in self._values:\nif torch.is_tensor(value):\nreturn value.device\n
current_outputs = []\nfor batch in outputs:\nif output_type != type(batch[i]):\nraise TypeError(f'Types for microbatch outputs do not match, found: {output_type} and {type(batch[i])}')\ncurrent_outputs.append(batch[i])\n
\nif torch.is_tensor(outputs[0][i]):\noutput_buf.append(torch.cat(current_outputs))\nelse:\noutput_buf.append(current_outputs)\n\noutput = tuple(output_buf)\n\nreturn output\n
"""\nAn abstraction representing a microbatch in the pipeline.\n"""\n\ndef __init__(self, values: Union[List[Any], Tensor]) -> None:\nself._values = values\nself.atomic = torch.is_tensor(values)\n\n# Verify at least on tensor\n
\ndef __iter__(self):\nif self.atomic:\nyield self._values\nelse:\nyield from self._values\n\ndef __len__(self) -> int:\nreturn 1 if self.atomic else len(self._values)\n\ndef __getitem__(self, index: int):\n
from typing import Any, Callable, List, Union, cast, Sequence\n\nimport torch\nfrom torch import Tensor\nimport torch.cuda.comm\n\n__all__: List[str] = ["NoChunk", "Batch", "check", "scatter", "gather"]\n
# Copyright 2019 Kakao Brain\n#\n# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\n#\n# This source code is licensed under the BSD license found in the\n# LICENSE file in the root directory of this source tree.\n
float(x) != float(y),\nint(x) != float(y),\nfloat(x) / float(y),\nint(x) / int(y),\nmax(x),\nmax(x.item(), y.item()),\nmax(int(x), int(y)),\nmax(float(x), float(y)),\nmin(x),\nmin(x.item(), y.item()),\n
\nclass TSCollectionOpsModule(torch.nn.Module):\ndef forward(self):\ns = "abcde"\n# list\nl = ["1", "2", "test"]\nl.reverse()\nl.reverse()\nl[1] = "3"\nl.extend(["4"])\n# str dict\nd = {"key": 1}\nd.clear()\n
\n# https://pytorch.org/docs/stable/jit_builtin_functions.html#builtin-functions\n\n\nclass TSBuiltinOpsModule(torch.nn.Module):\ndef forward(self):\nx = torch.tensor(1)\ny = torch.tensor(0.5)\nb = float(1)\n
max(x),\nmax(x.item(), y.item()),\nmax(int(x), int(y)),\nmax(float(x), float(y)),\nmin(x),\nmin(x.item(), y.item()),\nmin(int(x), int(y)),\nmin(float(x), float(y)),\nint(l[0]),\nfloat(l[0]),\n# string\n
max(int(x), int(y)),\nmax(float(x), float(y)),\nmin(x),\nmin(x.item(), y.item()),\nmin(int(x), int(y)),\nmin(float(x), float(y)),\nint(l[0]),\nfloat(l[0]),\n# string\nstr(torch.tensor(1)),\nl[2].find("t"),\n
x | x,\nbool(x) | bool(x),\nint(x) | int(x),\nx << x,\nint(x) << int(x),\nx >> x,\nint(x) >> int(x),\nx ^ x,\nbool(x) ^ bool(x),\nint(x) ^ int(x),\nb * float(x),\nb * int(x),\nb + float(x),\nb - float(x),\n
float(x),\nfloat(x.item()),\n# math\nx & x,\nbool(x) & bool(x),\nint(x) & int(x),\nx | x,\nbool(x) | bool(x),\nint(x) | int(x),\nx << x,\nint(x) << int(x),\nx >> x,\nint(x) >> int(x),\nx ^ x,\nbool(x) ^ bool(x),\n
str(torch.tensor(1)),\nl[2].find("t"),\nl[2].replace("t", "x"),\nl[2].lower(),\nl[2].startswith("t"),\nl[2].split("t"),\nl[2].strip(),\nl[2].rstrip(),\nl[2].lstrip(),\nl[2][slice(2)],\nl[3].format("x"),\n
\nimport sys\nimport unittest\nimport weakref\nfrom contextlib import ExitStack\n\nfrom copy import deepcopy\nfrom typing import NamedTuple\n\nimport torch\n\nimport torch._inductor\nimport torch._inductor.cudagraph_trees\n
"test_radam_tensor_lr_capturable_weight_decay_decoupled_weight_decay_foreach_cuda": 3,\n"test_sgd_tensor_lr_cpu": 2,\n"test_sgd_tensor_lr_cuda": 2,\n"test_sgd_tensor_lr_foreach_cuda": 2,\n}\n\n# also tracks currently supported optimizers\n
ReduceLROnPlateau,\nStepLR,\n)\n\nfrom torch.testing._internal.common_device_type import (\ninstantiate_device_type_tests,\nskipCUDAIf,\n)\n\nfrom torch.testing._internal.common_optimizers import (\n_get_optim_inputs_including_global_cliquey_kwargs,\n
alpha=weight_decay,\n)\n\ntorch._foreach_mul_(search_directions, -lr)\n# pt2 region 3 update params\ntorch._foreach_add_(params, search_directions)\n\nreturn params, preconditioners, grads\n\ncompiled_fn = torch.compile(shampoo_functional_basic)\n
def setUp(self):\nsuper().setUp()\ntorch._dynamo.reset()\ntorch._inductor.metrics.reset()\n\ndef tearDown(self):\nsuper().tearDown()\ntorch._dynamo.reset()\ntorch._inductor.metrics.reset()\n\ndef check_cudagraphs_ran(self):\n
def training_loop():\ninput = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6]).reshape(3, 2)\n\nmodel = torch.nn.Sequential(\ntorch.nn.Linear(2, 3),\ntorch.nn.Sigmoid(),\ntorch.nn.Linear(3, 1),\ntorch.nn.Sigmoid(),\n
opt_compiled.state,\n)\n\nif run_cudagraphs:\nself.check_cudagraphs_ran()\n\nif self.check_kernel_count:\n# currently, we compile the step and the rest of the computation\n# separately because the step is a single element tensor\n
name_w_scheduler,\nkwargs,\nscheduler_cls,\n)\n)\nelse:\ncompiled_opt_db.append((optim_info.optim_cls, name, kwargs, None))\n\nreturn compiled_opt_db\n\n\nCOMPILED_OPT_KWARG_DB = build_opt_kwarg_db()\n
import sys\nimport time\n\nfrom typing import List\n\n\ndef run_command(args: List[str]) -> "subprocess.CompletedProcess[bytes]":\nlogging.debug("$ %s", " ".join(args))\nstart_time = time.monotonic()\n
\nfrom typing import List\n\n\ndef run_command(args: List[str]) -> "subprocess.CompletedProcess[bytes]":\nlogging.debug("$ %s", " ".join(args))\nstart_time = time.monotonic()\ntry:\nreturn subprocess.run(args, check=True)\n
"""\nimport argparse\nimport logging\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport time\n\nfrom typing import List\n\n\ndef run_command(args: List[str]) -> "subprocess.CompletedProcess[bytes]":\n
parser.add_argument(\n"--no-black-binary",\nhelp="do not use pre-compiled binaries from pip for black.",\naction="store_true",\n)\n\nargs = parser.parse_args()\n\nlogging.basicConfig(\nformat="<%(threadName)s:%(levelname)s> %(message)s",\n
\nif uv_available:\npip_args = ["uv", "pip", "install"]\nelse:\npip_args = ["pip", "install"]\n\n# If we are in a global install, use `--user` to install so that you do not\n# need root access in order to initialize linters.\n
level=logging.NOTSET if args.verbose else logging.DEBUG,\nstream=sys.stderr,\n)\n\nuv_available = shutil.which("uv") is not None\n\nif uv_available:\npip_args = ["uv", "pip", "install"]\nelse:\npip_args = ["pip", "install"]\n
"Please specify a version to produce a consistent linting experience."\n)\nif args.no_black_binary and "black" in package_name:\npip_args.append(f"--no-binary={package_name}")\n\ndry_run = args.dry_run == "1"\n
logging.debug("$ %s", " ".join(args))\nstart_time = time.monotonic()\ntry:\nreturn subprocess.run(args, check=True)\nfinally:\nend_time = time.monotonic()\nlogging.debug("took %dms", (end_time - start_time) * 1000)\n
\ndef _get_num_nonjoined_procs(self):\nr"""Return the number of non-joined processes by shadowing an all-reduce in the non-joined processes."""\nnum_nonjoined_procs = torch.zeros(1, device=self._device)\n
self._notify_procs_to_terminate()\n\n# Run main hooks\nfor join_hook in self._join_hooks:\njoin_hook.main_hook()\n\nis_last_joiner = False\ni += 1\n\n# Run post-hooks\nfor join_hook in self._join_hooks:\n
@abstractmethod\ndef join_device(self) -> torch.device:\nr"""Return the device from which to perform collective communications needed by the join context manager."""\n...\n\n@property\n@abstractmethod\n
\ndef main_hook(self) -> None:\nr"""Call this hook while there exists a non-joined process to shadow collective communications in a training iteration.\n\nTraining iteration i.e., in one forward pass, backward pass, and optimizer step.\n
Only the first :class:`Joinable` object passed into the context\nmanager performs the collective communications in this method, and\nfor the others, this method is vacuous.\n\nArguments:\njoinable (Joinable): the :class:`Joinable` object calling this\n
\n.. warning::\nThe context manager requires each participating :class:`Joinable` to\ncall the method :meth:`notify_join_context()` before its own per-\niteration collective communications to ensure correctness.\n
# Shadow the all-reduce in non-joined processes\nnum_nonjoined_procs = self._get_num_nonjoined_procs()\nif num_nonjoined_procs == 0:\nall_procs_joined = True\nelse:\nif self._throw_on_early_termination:\n
It is passed an additional ``bool`` argument ``is_last_joiner``, which indicates if the rank is one of the last to join.\n\nArguments:\nis_last_joiner (bool): ``True`` if the rank is one of the last to\n
\n# If the log contains a gist URL, extract it so we can include it in the CSV\ngist_url = ""\nm = re.search(r"https://gist.github.com/[a-f0-9]+", full_log)\nif m is not None:\ngist_url = m.group(0)\n
\n# Attempt to extract out useful information from the traceback\n\nlog = log.split(\n"The above exception was the direct cause of the following exception"\n)[0]\nsplit = log.split("Traceback (most recent call last)", maxsplit=1)\n
context = ""\n\n# Temporary file names are meaningless, report it's generated code in this\n# case\nif "/tmp/" in component:\ncomponent = "generated code"\ncontext = ""\n\nout.writerow(\n{\n"bench": bench,\n
bench = "torchbench"\n\n# 3 = 1 + number of matches in the entries split regex\nfor name, name2, log in chunker(entries, 3):\nif name is None:\nname = name2\nif name.startswith("Albert"):\nbench = "huggingface"\n
return f.split("site-packages/", 2)[1]\nelse:\nreturn os.path.relpath(f)\n\n\n# Assume we run torchbench, huggingface, timm_models in that order\n# (as output doesn't say which suite the benchmark is part of)\n
def normalize_file(f):\nif "site-packages/" in f:\nreturn f.split("site-packages/", 2)[1]\nelse:\nreturn os.path.relpath(f)\n\n\n# Assume we run torchbench, huggingface, timm_models in that order\n# (as output doesn't say which suite the benchmark is part of)\n
if "FAIL" in log:\nr = "FAIL"\n\nif r == "UNKNOWN":\nc += 1\n\nbackend_time = None\nframe_time = None\nif "TIMING:" in log:\nresult = re.search("TIMING:(.*)\n", log).group(1)\nsplit_str = result.split("backend_compile:")\n
gist_url = ""\nm = re.search(r"https://gist.github.com/[a-f0-9]+", full_log)\nif m is not None:\ngist_url = m.group(0)\n\n# Split the log into an entry per benchmark\nentries = re.split(\nr"(?:cuda (?:train|eval) +([^ ]+)|WARNING:root:([^ ]+) failed to load)", full_log\n
for (int i = 0; i < {returns_length}; i++) {{\nlazy_tensors.push_back({self.create_lazy_tensor(first_tensor_name)}({getValueT()}(node, i), *common_device));\n}}\nauto result = {self.tuple_aten_from_ltc_tensors}<{returns_length}>(lazy_tensors);"""\n
\n{schema.node_name}({node_ctor_args})\n: {self.node_base_ctor_call(schema)}{scalar_initializers}\n{{\n{has_optional_defs}\n}}\n\nstd::string ToString() const override {{\nstd::stringstream ss;\nss << {self.node_base}::ToString();\n
return f"""{signature} {{\nreturn ReuseOrMakeNode<{schema.node_name}>(data);\n}}"""\n\ndef can_be_reused_function(self, schema: LazyIrSchema, node_ctor_args: str) -> str:\nsignature = f"bool CanBeReused({node_ctor_args}) const"\n
shape_ctor_arg = ""\n\nscalar_hashes = ", ".join(f"{a.name}" for a in scalar_args)\n\nreturn f"""{self.node_base}(\n{schema.node_name}::ClassOpKind(),\nOpList{{{base_ctor_value_args}}},\n{shape_ctor_arg}\n
if is_structured or is_view_copy_op:\nreturn []\nelse:\nshape_sig = ComputeShapeSignature(\nmetadata.kernel, f, symint=metadata.supports_symint()\n)\nreturn ["\n".join([f"{shape_sig.shape_decl};"])]\n
return false;\n}}"""\n\ndef node_base_ctor_call(self, schema: LazyIrSchema) -> str:\nvalue_args = schema.filtered_args(values=True, scalars=False)\n# backends can customize the way the node base class constructor is called,\n
lazy_tensor_decls.append(\nf"{self.lazy_tensor_ptr} lazy_{arg.name} = "\nf"{self.backend_namespace}::{self.try_get_tensor}({arg.name}.value_or(at::Tensor()));"\n)\nelse:\nraise AssertionError(\nf"TODO not sure if there are other valid types to handle here ({arg.lazy_type})"\n
generate a c++ string for materializing an rvalue of that arg for passing into\na lazy Node constructor.\n"""\n\n# TODO: Matching on CType seems wrong; should be matching on Type\nif isValueType(arg.lazy_type):\n
"""\nGiven a nn.Module, define how to shard the module across\nranks, return a ShardingPlan\nArgs:\nmodule (:class:`torch.nn.Module`):\nThe module to apply sharding to.\nReturns:\nA :class:`torch.distributed._shard.sharding_plan.ShardingPlan` object that\n
plan: Dict[str, Union[ShardingSpec, Sharder]]\noutput_plan: Optional[Dict[str, ShardingSpec]] = None\nreturn_local_tensor: Optional[List[str]] = None\n\n\nclass ShardingPlanner(abc.ABC):\n"""\nDefault ShardingPlanner interface, can be extended and\n
\nfrom torch.distributed._shard.sharder import Sharder\nfrom torch.distributed._shard.sharding_spec import ShardingSpec\n\n@dataclass\nclass ShardingPlan:\n"""\nRepresentation of a sharding plan, describes how to shard a module\n
output_plan: Optional[Dict[str, ShardingSpec]] = None\nreturn_local_tensor: Optional[List[str]] = None\n\n\nclass ShardingPlanner(abc.ABC):\n"""\nDefault ShardingPlanner interface, can be extended and\n
from typing import Dict, List, Optional, Union\n\nfrom torch.distributed._shard.sharder import Sharder\nfrom torch.distributed._shard.sharding_spec import ShardingSpec\n\n@dataclass\nclass ShardingPlan:\n
ensure further processing in a data parallel fashion. ("" in list means the\nroot module).\nDefault: None\nExample:\nSuppose we want to shard a module with two linear layers and then run it with DDP, we also\n
>>>    plan={\n>>>        "fc1.weight": spec1,\n>>>        "fc2.weight": spec2\n>>>    },\n>>>    output_plan={\n>>>        "fc2": output_spec\n>>>    },\n>>>    return_local_tensor=["fc2"]\n>>> )\n"""\n
class ShardingPlanner(abc.ABC):\n"""\nDefault ShardingPlanner interface, can be extended and\nimplement advanced sharding strategies.\n"""\n@abc.abstractmethod\ndef build_plan(self, module: nn.Module) -> ShardingPlan:\n
"torch.distributed.constants",\n"torch.distributed.distributed_c10d",\n"torch.distributed.elastic.agent.server",\n"torch.distributed.elastic.rendezvous",\n"torch.distributed.fsdp",\n"torch.distributed.launch",\n
# verifies that each public API has the correct module name and naming semantics\ndef check_one_element(elem, modname, mod, *, is_public, is_all):\nobj = getattr(mod, elem)\n# torch.dtype is not a class nor callable, so we need to check for it separately\n
"torch.contrib._tensorboard_vis",\n"torch.distributed._composable",\n"torch.distributed._functional_collectives",\n"torch.distributed._functional_collectives_impl",\n"torch.distributed._shard",\n"torch.distributed._sharded_tensor",\n
# AttributeError: module 'torch.distributed' has no attribute '_shard'\n@unittest.skipIf(IS_WINDOWS or IS_JETSON or IS_MACOS, "Distributed Attribute Error")\n@skipIfTorchDynamo("Broken and not relevant for now")\n
"CudaBFloat16TensorBase",\n"CudaBoolTensorBase",\n"CudaByteTensorBase",\n"CudaCharTensorBase",\n"CudaComplexDoubleTensorBase",\n"CudaComplexFloatTensorBase",\n"CudaDoubleTensorBase",\n"CudaFloatTensorBase",\n
"set_autocast_dtype",\n"set_autocast_ipu_dtype",\n"set_autocast_cpu_enabled",\n"set_autocast_ipu_enabled",\n"set_autocast_enabled",\n"set_flush_denormal",\n"set_num_interop_threads",\n"set_num_threads",\n
"SymInt",\n"TensorType",\n"ThroughputBenchmark",\n"TracingState",\n"TupleType",\n"Type",\n"unify_type_list",\n"UnionType",\n"Use",\n"Value",\n'set_autocast_gpu_dtype',\n'get_autocast_gpu_dtype',\n"vitals_enabled",\n
"torch.onnx._internal.fx.passes",\n"torch.onnx._internal.fx.passes._utils",\n"torch.onnx._internal.fx.passes.decomp",\n"torch.onnx._internal.fx.passes.functionalization",\n"torch.onnx._internal.fx.passes.modularization",\n
def test_force_outplace_check_zero(self):\ndef f(x):\nreturn torch.empty(x.shape).zero_()\n\nx = torch.randn(10, 15)\nft = torch.jit.trace(f, x, _force_outplace=True)\nself.assertEqual(f(x), ft(x))\n\n
new_input_map = {\n"1": [torch.rand(2, 2), torch.randn(2, 2)],\n"3": [torch.rand(2, 2), torch.rand(2, 2)],\n}\nself.assertEqual(model(new_input_map), traced_model(new_input_map))\n\ndef test_trace_script_returning_complex_dict(self):\n
return torch.mm(x, self.param)\n\nclass ScriptMod(torch.jit.ScriptModule):\ndef __init__(self):\nsuper().__init__()\nself.param = torch.nn.Parameter(torch.rand(4, 3))\nself.tm = torch.jit.trace(TracedMod(), torch.rand(3, 3))\n
y.detach_()\n# Make sure trace kernel redispatches to the right lower kernel.\nassert not y.requires_grad\nreturn y\n\nx, w = torch.rand(3, 4), torch.rand(4, 5, requires_grad=True)\n# With `check_trace=True` it will run with `@torch.no_grad()` and break assert.\n
\n# Check that it behaves as expected\ntraced_arange = torch.jit.trace(arange, x)\nself.assertEqual(traced_arange(y), arange(y))\nself.assertEqual(traced_arange(x), arange(x))\n\ntraced_arange_scalar = torch.jit.trace(arange_scalar, x)\n
def foo(x):\nreturn [[x + 1, x - 1], [x + 2, x - 2]]\n\ndef bar(x):\nlist_stuff = foo(x)\nreturn list_stuff[0][0], list_stuff[1][1]\n\ntraced = torch.jit.trace(bar, torch.rand(3, 4))\nx = torch.rand(5, 6)\n
assert "quick_brown_fox" in graph_str\n\n@skipIfTorchDynamo("Not a suitable test for TorchDynamo")\ndef test_tracing_hooks(self):\nclass Net(nn.Module):\ndef forward(self, x):\nreturn x + x\n\ndef test_hook(is_post_hook, hook, fc):\n
\ntraced = torch.jit.trace(\nf, (torch.zeros(2, 3), torch.ones(2, 3)), check_trace=False\n)\nmean, std = torch.zeros(5, 5), torch.ones(5, 5)\nwith torch.random.fork_rng(devices=[]):\noutput = f(mean, std)\n
#\n# This lists exists so we can more easily add large numbers of failing tests,\nif test_dir is None:\ndynamo_expected_failures = set()\ndynamo_skips = set()\nelse:\nfailures_directory = os.path.join(test_dir, "dynamo_expected_failures")\n
dynamo_skips = dynamo_skips.union(extra_dynamo_skips)\n\n\n# verify some invariants\nfor test in dynamo_expected_failures.union(dynamo_skips):\nif len(test.split(".")) != 2:\nraise AssertionError(f'Invalid test name: "{test}"')\n
# when run under PYTORCH_TEST_WITH_DYNAMO=1.\n# see NOTE [dynamo_test_failures.py] for more details\n#\n# This lists exists so we can more easily add large numbers of failing tests,\nif test_dir is None:\n
\n\n# verify some invariants\nfor test in dynamo_expected_failures.union(dynamo_skips):\nif len(test.split(".")) != 2:\nraise AssertionError(f'Invalid test name: "{test}"')\n\nintersection = dynamo_expected_failures.intersection(dynamo_skips)\n
):\nreturn test_dir\ntest_dir = dirname(test_dir)\n\n# Not found\nreturn None\n\n\ntest_dir = find_test_dir()\nif not test_dir:\nlogger = logging.getLogger(__name__)\nlogger.warning(\n"test/dynamo_expected_failures directory not found - known dynamo errors won't be skipped."\n
# NOTE: [dynamo_test_failures.py]\n#\n# We generate xFailIfTorchDynamo* for all tests in `dynamo_expected_failures`\n# We generate skipIfTorchDynamo* for all tests in `dynamo_skips`\n#\n# For an easier-than-manual way of generating and updating these lists,\n
\n# TODO: due to case sensitivity problems, for now list these files by hand\nextra_dynamo_skips = {\n"TestProxyTensorOpInfoCPU.test_make_fx_exhaustive_T_cpu_float32",\n"TestProxyTensorOpInfoCPU.test_make_fx_exhaustive_t_cpu_float32",\n
test_dir = dirname(abspath(file))\nwhile dirname(test_dir) != test_dir:\nif basename(test_dir) == "test" and exists(\njoin(test_dir, "dynamo_expected_failures")\n):\nreturn test_dir\ntest_dir = dirname(test_dir)\n
"u2.l1.bias",\n"u2.seq.1.weight",\n"u2.seq.1.bias",\n"u2.l2.weight",\n"u2.l2.bias",\n"l2.weight",\n"l2.bias",\n],\n[\n"u1.l1.weight",\n"u1.l1.bias",\n"u1.seq.1.weight",\n"u1.seq.1.bias",\n"u1.l2.weight",\n
],\n],\n)\n\n\nif __name__ == "__main__":\nrun_tests()\n
sharded_module_name_to_fqns,\n) = _get_sharded_module_tree_with_module_name_to_fqns(new_model)\nself.assertEqual(\nlist(sharded_module_name_to_fqns.keys()),\n["[CompositeModel]", "u1[UnitModule]"],\n)\n
list(sharded_module_name_to_fqns.keys()),\n["[CompositeModel]", "u1[UnitModule]"],\n)\nself.assertEqual(\nlist(sharded_module_name_to_fqns.values()),\n[\n[\n"l1.weight",\n"l1.bias",\n"u2.l1.weight",\n
],\n[\n"u1.l1.weight",\n"u1.l1.bias",\n"u1.seq.1.weight",\n"u1.seq.1.bias",\n"u1.l2.weight",\n"u1.l2.bias",\n],\n],\n)\n\n\nif __name__ == "__main__":\nrun_tests()\n
\nclass TestUtils(FSDPTest):\n@property\ndef world_size(self):\nreturn 2\n\n@property\ndef process_group(self):\nreturn dist.distributed_c10d._get_default_group()\n\n@skip_if_lt_x_gpu(2)\ndef test_get_sharded_module_tree_with_module_name_to_fqns(self):\n
"u1.seq.1.bias",\n"u1.l2.weight",\n"u1.l2.bias",\n],\n[\n"u2.l1.weight",\n"u2.l1.bias",\n"u2.seq.1.weight",\n"u2.seq.1.bias",\n"u2.l2.weight",\n"u2.l2.bias",\n],\n],\n)\n# Test nested fully_shard\nnew_model = CompositeModel(torch.device("cuda"))\n
"u2.l1.weight",\n"u2.l1.bias",\n"u2.seq.1.weight",\n"u2.seq.1.bias",\n"u2.l2.weight",\n"u2.l2.bias",\n"l2.weight",\n"l2.bias",\n],\n[\n"u1.l1.weight",\n"u1.l1.bias",\n"u1.seq.1.weight",\n"u1.seq.1.bias",\n
_BACK_COMPAT_OBJECTS.setdefault(fn)\n_MARKED_WITH_COMPATIBILITY.setdefault(fn)\nreturn fn\n\nreturn mark_back_compat\nelse:\n\ndef mark_not_back_compat(fn):\ndocstring = textwrap.dedent(getattr(fn, '__doc__', None) or '')\n
def mark_not_back_compat(fn):\ndocstring = textwrap.dedent(getattr(fn, '__doc__', None) or '')\ndocstring += """\n.. warning::\nThis API is experimental and is *NOT* backward-compatible.\n"""\nfn.__doc__ = docstring\n
"""\nfn.__doc__ = docstring\n_BACK_COMPAT_OBJECTS.setdefault(fn)\n_MARKED_WITH_COMPATIBILITY.setdefault(fn)\nreturn fn\n\nreturn mark_back_compat\nelse:\n\ndef mark_not_back_compat(fn):\ndocstring = textwrap.dedent(getattr(fn, '__doc__', None) or '')\n
\nreturn mark_not_back_compat\n
fn.__doc__ = docstring\n_BACK_COMPAT_OBJECTS.setdefault(fn)\n_MARKED_WITH_COMPATIBILITY.setdefault(fn)\nreturn fn\n\nreturn mark_back_compat\nelse:\n\ndef mark_not_back_compat(fn):\ndocstring = textwrap.dedent(getattr(fn, '__doc__', None) or '')\n
return mark_not_back_compat\n
else:\n\ndef mark_not_back_compat(fn):\ndocstring = textwrap.dedent(getattr(fn, '__doc__', None) or '')\ndocstring += """\n.. warning::\nThis API is experimental and is *NOT* backward-compatible.\n"""\n
if is_backward_compatible:\n\ndef mark_back_compat(fn):\ndocstring = textwrap.dedent(getattr(fn, '__doc__', None) or '')\ndocstring += """\n.. note::\nBackwards-compatibility for this API is guaranteed.\n
#\n# Decode a base64 string (of type `str` or `bytes`).\n# Return type is `bytes`, which is more convenient with the Store interface.\n#\ndef _decode(self, value) -> bytes:\nif type(value) == bytes:\n
key=self.prefix,\nrecursive=True,\ntimeout=watch_timeout,\nindex=all_nodes.etcd_index + 1,\n)\nexcept etcd.EtcdWatchTimedOut:\nif time.time() >= deadline:\nreturn None\nelse:\ncontinue\nexcept etcd.EtcdEventIndexCleared:\n
import datetime\nimport random\nimport time\nfrom base64 import b64decode, b64encode\nfrom typing import Optional\n\nimport etcd  # type: ignore[import]\n\n# pyre-ignore[21]: Could not find name `Store` in `torch.distributed`.\n
#\n# Get all of the (base64-encoded) etcd keys at once, or wait until all the keys\n# are published or timeout occurs.\n# This is a helper method for the public interface methods.\n#\n# On success, a dictionary of {etcd key -> etcd value} is returned.\n
continue\nexcept etcd.EtcdEventIndexCleared:\ncontinue\n
except etcd.EtcdEventIndexCleared:\ncontinue\n
\nwatch_timeout = deadline - time.time()\nif watch_timeout <= 0:\nreturn None\n\ntry:\nself.client.watch(\nkey=self.prefix,\nrecursive=True,\ntimeout=watch_timeout,\nindex=all_nodes.etcd_index + 1,\n)\n
import etcd  # type: ignore[import]\n\n# pyre-ignore[21]: Could not find name `Store` in `torch.distributed`.\nfrom torch.distributed import Store\n\n\n# Delay (sleep) for a small random amount to reduce CAS failures.\n
def _make_model(self, **kwargs):\nmodel = nn.Sequential(\nnn.Linear(13, 17),\nnn.Dropout(0.5),\nnn.Linear(17, 3),\n)\nreturn model\n\ndef _make_scheduler(self, model, **kwargs):\nsparsifier = WeightNormSparsifier()\n
3,\nmsg="Scheduler step_count is expected to increment",\n)\n# Value before t_0 is supposed to be 0\nself.assertEqual(\nself._get_sparsity_levels(sparsifier),\nscheduler._make_sure_a_list(0.0),\nmsg="Scheduler step updating the sparsity level before t_0",\n
)\n\nscheduler.step()  # Step = 4  =>  sparsity ~ [0.3, 0.2]\nself.assertEqual(\nself._get_sparsity_levels(sparsifier, 1),\n[0.3, 0.2],\nmsg="Sparsity level is not set correctly after the first step",\n
assert sparsifier.groups[0]["sparsity_level"] == 0.25\n\ndef test_lambda_scheduler(self):\nmodel = nn.Sequential(nn.Linear(16, 16))\nsparsifier = WeightNormSparsifier()\nsparsifier.prepare(model, config=None)\n
for _ in range(more_steps_needed):  # More steps needed to final sparsity level\nscheduler.step()\nself.assertEqual(\nself._get_sparsity_levels(sparsifier),\nself.sorted_sparse_levels,\nmsg="Sparsity level is not reaching the target level afer delta_t * n steps ",\n
scheduler_args = {\n"init_sl": self.initial_sparsity,\n"init_t": self.initial_step,\n}\nscheduler_args.update(kwargs)\n\nscheduler = CubicSL(sparsifier, **scheduler_args)\nreturn sparsifier, scheduler\n
\nimport warnings\n\nfrom torch import nn\n\nfrom torch.ao.pruning import BaseScheduler, CubicSL, LambdaSL, WeightNormSparsifier\n\nfrom torch.testing._internal.common_utils import TestCase\n\n\nclass ImplementedScheduler(BaseScheduler):\n
sparsifier.step()\nscheduler.step()\nassert sparsifier.groups[0]["sparsity_level"] == 0.25\n\ndef test_lambda_scheduler(self):\nmodel = nn.Sequential(nn.Linear(16, 16))\nsparsifier = WeightNormSparsifier()\n
# and cleans it up in this case.\n#\n# This function cannot be an inner function since otherwise mp_context="spawn" would\n# not work for ProcessPoolExecutor since inner functions cannot be pickled.\n
\n\n_watchdog_thread: Optional[Thread] = None\n_original_parent: Optional[int] = None\n\n\ndef has_parent_changed() -> bool:\nreturn _original_parent != os.getppid()\n
\ndef has_parent_changed() -> bool:\nreturn _original_parent != os.getppid()\n
_watchdog_thread.start()\n# Ignore Ctrl-C (i.e. SIGINT) sent to pool workers to avoid meaningless log spam.\nsignal.signal(signal.SIGINT, signal.SIG_IGN)\n\n\n_watchdog_thread: Optional[Thread] = None\n
# This function cannot be an inner function since otherwise mp_context="spawn" would\n# not work for ProcessPoolExecutor since inner functions cannot be pickled.\ndef _async_compile_initializer(orig_ppid) -> None:\n
\n\n# If this process dies abnormally (e.g. segfault)\n# it will not shut down the workers. Instead,\n# the workers will have their parent reassigned to the\n# init process. This launches a separate thread to\n
\n_watchdog_thread: Optional[Thread] = None\n_original_parent: Optional[int] = None\n\n\ndef has_parent_changed() -> bool:\nreturn _original_parent != os.getppid()\n
from typing import Optional\n\n\n# If this process dies abnormally (e.g. segfault)\n# it will not shut down the workers. Instead,\n# the workers will have their parent reassigned to the\n# init process. This launches a separate thread to\n
# each rank have its own tensor, all_gather gives a bigger tensor\nlocal_tensor = torch.ones([3, 3, 3], device=device)\ngathered_tensor = ft_c.all_gather_tensor(\nlocal_tensor, gather_dim=dim, group=(mesh, 0)\n
torch.cuda.set_device(dist.get_rank())\n\ntensors = [torch.ones([4], device=device), torch.ones([4], device=device) + 1]\nmesh = dt.DeviceMesh(device, torch.arange(4))\n\nres = ft_c.all_gather_into_tensor_coalesced(tensors, mesh)\n
group = dist.group.WORLD.group_name\n\nt = torch.rand((self.world_size, 2), requires_grad=True)\n\ndef my_func(t: torch.Tensor, world_size: int) -> torch.Tensor:\nsizes = [1] * world_size\nt = t * 10\n
This means we should create duplicates.\n- _expand_group on _default-tagged pg should always resolve to it\nThis mean we can't depend on empty tag + rankset.\n"""\n\ndef test_pg_creation_with_tag(self):\n
from torch.testing import FileCheck\nfrom torch.testing._internal.distributed.fake_pg import FakeStore\nfrom torch.utils._triton import has_triton\n\nif not dist.is_available():\nprint("Distributed not available, skipping tests", file=sys.stderr)\n
def setUp(self):\nsuper().setUp()\nself._spawn_threads()\n\n@parametrize("device", ["cpu", "cuda"])\ndef test_broadcast(self, device):\nif device == "cuda":\nif torch.cuda.device_count() < self.world_size:\n
self.assertEqual(pg_notag1, roundtrip(pg_notag1))\n\ndef test_pg_lookup_with_tag(self):\npg_tag0, _ = new_subgroups(group_size=2, pg_tag="blu")\npg_tag1, _ = new_subgroups(group_size=2, pg_tag="bla")\n
if compile:\ncompiled = torch.compile(my_func, fullgraph=True, backend="aot_eager")\nelse:\ncompiled = my_func\n\ndims_to_gather = [0, 1, 2]\nfor dim in dims_to_gather:\noutput_size = [3, 3, 3]\noutput_size[dim] *= self.world_size\n
\n\nif __name__ == "__main__":\nif False:\nrun_tests()\n
with open(path) as fp:\nfake_graph = fake_graph1 if i == 1 else fake_graph2\nself.assertEqual(fp.readline(), fake_graph[f"fake_graph{i}"].graph)\nos.remove(path)\n\nos.rmdir(folder)\n\n\nif __name__ == "__main__":\n
class FakeGraph:\ndef __init__(self, postfix):\nself.graph = f"fake graph str {postfix}"\n\ndef __str__(self) -> str:\nreturn self.graph\n\nfake_graph1 = {"fake_graph1": FakeGraph(1)}\nfolder = dump_graphs_to_files(fake_graph1)\n
\nclass GraphUtilsTest(DTensorTestBase):\n@property\ndef world_size(self):\nreturn 1\n\ndef test_dump_graphs(self):\nclass FakeGraph:\ndef __init__(self, postfix):\nself.graph = f"fake graph str {postfix}"\n
class FakeGraph:\ndef __init__(self, postfix):\nself.graph = f"fake graph str {postfix}"\n\ndef __str__(self) -> str:\nreturn self.graph\n\nfake_graph1 = {"fake_graph1": FakeGraph(1)}\nfolder = dump_graphs_to_files(fake_graph1)\n
if __name__ == "__main__":\nif False:\nrun_tests()\n
run_tests()\n
@property\ndef world_size(self):\nreturn 1\n\ndef test_dump_graphs(self):\nclass FakeGraph:\ndef __init__(self, postfix):\nself.graph = f"fake graph str {postfix}"\n\ndef __str__(self) -> str:\nreturn self.graph\n
\nCLOSING_COMMENT = (\n"I cannot find any mention of this test in rockset for the past 7 days "\n"or in the logs for the past 5 commits on viable/strict.  Closing this "\n"issue as it is highly likely that this test has either been renamed or "\n
\n\ndef download_log_worker(temp_dir: str, id: int, name: str) -> None:\nurl = f"https://ossci-raw-job-status.s3.amazonaws.com/log/{id}"\ndata = requests.get(url).text\nwith open(f"{temp_dir}/{name.replace('/', '_')} {id}.txt", "x") as f:\n
return False, "not found"\nreturn True, "found in rockset"\n\n\nif __name__ == "__main__":\nargs = parse_args()\ndisabled_tests_json = json.loads(requests.get(DISABLED_TESTS_JSON).text)\n\nall_logs = []\n
host="api.rs2.usw2.rockset.com", api_key=os.environ["ROCKSET_API_KEY"]\n).sql(query, params)\nresults: List[Dict[str, Any]] = res.results\nreturn results\n\n\ndef download_log_worker(temp_dir: str, id: int, name: str) -> None:\n
import multiprocessing as mp\nimport os\nimport re\nimport tempfile\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport requests\nimport rockset  # type: ignore[import]\nfrom gitutils import retries_decorator\n
requests.post(\nf"https://api.github.com/repos/pytorch/pytorch/issues/{num}/comments",\ndata=json.dumps({"body": CLOSING_COMMENT}),\nheaders=headers,\n)\nrequests.patch(\nf"https://api.github.com/repos/pytorch/pytorch/issues/{num}",\n
return True, "found in logs"\n\n# Query rockset to see if the test is there\ncount = query_rockset(\nTEST_EXISTS_QUERY, {"name": f"{name}%", "classname": f"{classname}%"}\n)\nif count[0]["c"] == 0:\nreturn False, "not found"\n
\nname = reg[1]\nclassname = reg[2].split(".")[-1]\n\n# Check if there is any mention of the link or the test name in the logs.\n# The link usually shows up in the skip reason.\npresent = False\nfor log in all_logs:\n
def incorrect_compile_fn(gm, example_inputs):\nreturn transform(gm).forward\n\nr1 = toy_example(i1, i2)\nopt_toy_example = torch._dynamo.optimize(incorrect_compile_fn)(toy_example)\nr2 = opt_toy_example(i1, i2)\n
When config.verify_correctness=True, it will\ncheck the correctness of outputs and raise an error\n"""\ni1 = torch.randn(10)\ni2 = torch.randn(10)\n\ndef incorrect_compile_fn(gm, example_inputs):\nreturn transform(gm).forward\n
for node in gm.graph.nodes:\n# Checks if we're calling a function (i.e:\n# operator.add)\nif node.op == "call_function":\n# The target attribute is the function\n# that call_function calls.\nif node.target == operator.mul:\n
\nr1 = toy_example(i1, i2)\nopt_toy_example = torch._dynamo.optimize(incorrect_compile_fn)(toy_example)\nr2 = opt_toy_example(i1, i2)\nself.assertTrue(not same(r1, r2))\n\n\nif __name__ == "__main__":\n
\n\nclass Conv_Bn_Relu(torch.nn.Module):\ndef __init__(self, in_channels, out_channels, **kwargs):\nsuper().__init__()\nself.conv = torch.nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\nself.bn = torch.nn.BatchNorm2d(out_channels, eps=0.001)\n
i2 = torch.randn(10)\n\ndef incorrect_compile_fn(gm, example_inputs):\nreturn transform(gm).forward\n\nr1 = toy_example(i1, i2)\nopt_toy_example = torch._dynamo.optimize(incorrect_compile_fn)(toy_example)\n
\ndef transform(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:\nfor node in gm.graph.nodes:\n# Checks if we're calling a function (i.e:\n# operator.add)\nif node.op == "call_function":\n# The target attribute is the function\n
return self.layers(x)\n\n\nclass Conv_Bn_Relu(torch.nn.Module):\ndef __init__(self, in_channels, out_channels, **kwargs):\nsuper().__init__()\nself.conv = torch.nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n
import torch\nfrom torch.testing import FileCheck\nfrom torch.testing._internal.jit_utils import JitTestCase\n\nif __name__ == "__main__":\nraise RuntimeError(\n"This test file is not meant to be run directly, use:\n\n"\n
self.assertEqual(foo(x), torch.square(x))\n
return torch.var(x, unbiased=True)\n\n# TODO: more robust testing\nfoo_s = torch.jit.script(foo)\nFileCheck().check("aten::var").run(foo_s.graph)\ntorch._C._jit_pass_run_decompositions(foo_s.graph)\ninp = torch.rand([10, 10])\n
foo_s = torch.jit.script(foo)\nFileCheck().check("aten::var").run(foo_s.graph)\ntorch._C._jit_pass_run_decompositions(foo_s.graph)\ninp = torch.rand([10, 10])\nself.assertEqual(foo(inp), foo_s(inp))\n
FileCheck().check_not("aten::var").run(foo_s.graph)\n\ndef test_registered_decomposition(self):\n@torch.jit.script\ndef foo(x):\nreturn torch.square(x)\n\n@torch.jit.script\ndef square_decomp(x):\nreturn torch.pow(x, 2)\n
FileCheck().check_not("aten::square").check("aten::pow").run(foo.graph)\nx = torch.rand([4])\nself.assertEqual(foo(x), torch.square(x))\n
@torch.jit.script\ndef foo(x):\nreturn torch.square(x)\n\n@torch.jit.script\ndef square_decomp(x):\nreturn torch.pow(x, 2)\n\ntorch.jit._register_decomposition(\ntorch.ops.aten.square.default, square_decomp.graph\n
torch.ops.aten.square.default, square_decomp.graph\n)\ntorch._C._jit_pass_run_decompositions(foo.graph)\nFileCheck().check_not("aten::square").check("aten::pow").run(foo.graph)\nx = torch.rand([4])\nself.assertEqual(foo(x), torch.square(x))\n
def forward(self, x):\nreturn torch.sigmoid(self.net2(self.relu(self.net1(x))))\n\n\nclass TestOpCoverage(TestCase):\ndef test_trace_with_inductor_decomp(self):\nmodel = SimpleMLP()\nargs = (torch.randn(8, 50),)\n
class SimpleMLP(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.net1 = nn.Linear(50, 32)\nself.relu = nn.ReLU()\nself.net2 = nn.Linear(32, 8)\n\ndef forward(self, x):\nreturn torch.sigmoid(self.net2(self.relu(self.net1(x))))\n
super().__init__()\nself.net1 = nn.Linear(50, 32)\nself.relu = nn.ReLU()\nself.net2 = nn.Linear(32, 8)\n\ndef forward(self, x):\nreturn torch.sigmoid(self.net2(self.relu(self.net1(x))))\n\n\nclass TestOpCoverage(TestCase):\n
graphs = get_inductor_decomp_graphs(model, args, kwargs)\nassert len(graphs) == 2, "Expect fwd + bwd graphs"\nself.assertIsInstance(graphs[0], torch.fx.GraphModule)\nself.assertIsInstance(graphs[1], torch.fx.GraphModule)\n
class TestOpCoverage(TestCase):\ndef test_trace_with_inductor_decomp(self):\nmodel = SimpleMLP()\nargs = (torch.randn(8, 50),)\nkwargs = {}\ngraphs = get_inductor_decomp_graphs(model, args, kwargs)\nassert len(graphs) == 2, "Expect fwd + bwd graphs"\n
self.net1 = nn.Linear(50, 32)\nself.relu = nn.ReLU()\nself.net2 = nn.Linear(32, 8)\n\ndef forward(self, x):\nreturn torch.sigmoid(self.net2(self.relu(self.net1(x))))\n\n\nclass TestOpCoverage(TestCase):\n
# Owner(s): ["oncall: distributed"]\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.distributed._tensor.debug._op_coverage import get_inductor_decomp_graphs\n\nfrom torch.testing._internal.common_utils import run_tests, TestCase\n
super().__init__()\nself.net1 = nn.Linear(50, 32)\nself.relu = nn.ReLU()\nself.net2 = nn.Linear(32, 8)\n\ndef forward(self, x):\nreturn torch.sigmoid(self.net2(self.relu(self.net1(x))))\n\n\nclass TestOpCoverage(TestCase):\n
required=False,\n)\nparser.add_argument(\n"--output-path",\n"--output_path",\nhelp="The location of the output yaml file.",\nrequired=True,\n)\nparser.add_argument(\n"--dep-graph-yaml-path",\n"--dep_graph_yaml_path",\n
\ndef main(argv) -> None:\nparser = argparse.ArgumentParser(description="Generate used operators YAML")\noptions = get_parser_options(parser)\n\nmodel_dict = {\n"model_name": options.model_name,\n"asset_info": {},\n
return parser.parse_args()\n\n\ndef get_parser_options(parser: argparse.ArgumentParser) -> argparse.Namespace:\nparser = add_arguments_parser(parser)\nreturn parse_options(parser)\n\n\ndef main(argv) -> None:\n
\ndef add_arguments_parser(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\nparser.add_argument(\n"--root-ops",\n"--root_ops",\nhelp="A comma separated list of root operators used by the model",\n
selected_models: List[dict],\nnew_style_rule: bool,\n):\nmodel_dict = {\n"asset_info": {},  # maps asset name -> dict of asset metadata like hashes\n"is_new_style_rule": new_style_rule,\n}\n\nfor model in selected_models:\n
model_versions = (\noptions.model_versions.split(",") if options.model_versions is not None else []\n)\nmodel_assets = (\noptions.model_assets.split(",") if options.model_assets is not None else None\n
all_build_features = set()\n\n# Go through each yaml file and retrieve operator information.\nfor model_info in selected_models_yaml:\nif "traced_operators" not in model_info:\n# If this YAML file doesn't specify any traced operators, then it is using\n
# <BEGIN FILE CONTENTS>\n# include_all_non_op_selectives: False\n# include_all_operators: False\n# debug_info:\n#   - model1@v100\n#   - model2@v50\n# operators:\n#   aten::add:\n#     is_root_operator: Yes\n
\n\ndef trace_flex_attention_backward(\nproxy_mode: ProxyTorchDispatchMode,\nquery: torch.Tensor,\nkey: torch.Tensor,\nvalue: torch.Tensor,\nout: torch.Tensor,\nlogsumexp: torch.Tensor,\ngrad_out: torch.Tensor,\n
example_out = flex_attention(query, key, value, score_mod, *other_buffers)\nexample_vals = [\ntorch.zeros((), dtype=query.dtype, requires_grad=query.requires_grad)\n] + [torch.zeros((), dtype=torch.int) for _ in range(4)]\n
grad_out: torch.Tensor,\nfw_graph: Callable,  # GraphModule type hint?\njoint_graph: Callable,\n*other_buffers: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\nworking_precision = torch.float64 if query.dtype == torch.float64 else torch.float32\n
\n@flex_attention_backward.py_impl(FakeTensorMode)\ndef flex_attention_backward_fake_tensor_mode(\nmode: FakeTensorMode,\nquery: torch.Tensor,\nkey: torch.Tensor,\nvalue: torch.Tensor,\nout: torch.Tensor,\n
in_dims=(0, None, None, 0, None, 0) + in_dim_buffers,\nout_dims=out_dims,\n)\njoint_score_mod = torch.vmap(\njoint_score_mod,\nin_dims=(0, None, 0, None, None, 0) + in_dim_buffers,\nout_dims=out_dims,\n
return out, logsumexp\n\n\n# ---------------------------- Backward HOP Implementation ----------------------------\n\n\n@flex_attention_backward.py_impl(DispatchKey.CompositeExplicitAutograd)\ndef sdpa_dense_backward(\n
)\nfrom torch._ops import HigherOrderOperator\nfrom torch._subclasses import FakeTensorMode\nfrom torch.fx.experimental.proxy_tensor import (\nmake_fx,\nProxyTorchDispatchMode,\ntrack_tensor_tree,\n)\n
in_dims=(0, None, None, 0, None, 0) + in_dim_buffers,\nout_dims=out_dims,\n)\njoint_score_mod = torch.vmap(\njoint_score_mod,\nin_dims=(0, None, 0, None, None, 0) + in_dim_buffers,\nout_dims=out_dims,\n
local_tensor=tensor([0.], device='cuda:0'),\ndevice_mesh=DeviceMesh:([0, 1, 2, 3]),\nplacements=[Shard(dim=0)])\n),\n),\n(\n'submesh_sdt',\nDTensor(\nlocal_tensor=tensor([8., 9.], device='cuda:0'),\ndevice_mesh=DeviceMesh:([0, 2]),\n
self, state: Dict[str, Union[int, torch._tensor.Tensor]]\n) -> None:\nself._extra_state = state["extra_state"]  # pyre-ignore[8]\nself._extra_state_tensor = state["extra_state_tensor"]  # pyre-ignore[8]\n
self.assertEqual(v.to_local(), torch.zeros([shard_size]))\nelse:\nself.assertEqual(v.to_local().size(), torch.Size([0]))\nself.assertEqual(v.to_local(), torch.tensor([]))\n\nif k == "_extra_state":\nself.assertEqual(1, v["extra_state"])\n
"""\nWhen the model is initialized, the state_dict on rank 0 are as follows when there are 4 GPUs.\nrank 0:\nOrderedDict(\n[\n(\n'rdt',\nDTensor(\nlocal_tensor=tensor([4., 5., 6., 7.], device='cuda:0'),\n
(\n'rdt',\nDTensor(\nlocal_tensor=tensor([4., 5., 6., 7.], device='cuda:0'),\ndevice_mesh=DeviceMesh:([0, 1, 2, 3]),\nplacements=[Replicate()]\n)\n),\n(\n'sdt',\nDTensor(\nlocal_tensor=tensor([0.], device='cuda:0'),\n
return {\n"extra_state": self._extra_state,\n"extra_state_tensor": self._extra_state_tensor,\n}\n\ndef set_extra_state(\nself, state: Dict[str, Union[int, torch._tensor.Tensor]]\n) -> None:\nself._extra_state = state["extra_state"]  # pyre-ignore[8]\n
submesh_tensor_size = [SUBMESH_TENSOR_SIZE]\nsubmesh_sharded_dt = zeros(\nsubmesh_tensor_size,\ndevice_mesh=submesh,\nplacements=[Shard(0)],\n)\nsubmesh_replicated_dt = zeros(\nsubmesh_tensor_size, device_mesh=submesh, placements=[Replicate()]\n
)\n]\n)\n"""\n\ndist_cp.load_state_dict(\nstate_dict=state_dict,\nstorage_reader=dist_cp.FileSystemReader(CHECKPOINT_DIR),\nplanner=dist_cp.DefaultLoadPlanner(),\n)\n\n"""\nAfter loading the model from the checkpoint, we want to make sure that the values in state_dict\n
to_sparse_kwargs = dict(\nlayout=layout, dense_dim=dense_dim, blocksize=blocksize\n)\nlhs_sparse = _to_sparse(lhs, **to_sparse_kwargs)\nrhs_sparse = _to_sparse(rhs, **to_sparse_kwargs)\n# op(sparse, sparse)\n
if op_info.name == "mul":\nsample = _validate_sample_input_elementwise_binary_sparse_mul(sample)\n\nif check_validate:\n_check_validate(op_info, sample)\nreturn sample\n\n\ndef sample_inputs_sparse_mul(op_info, device, dtype, requires_grad, layout, **kwargs):\n
return ErrorInput(\nsample,\nerror_regex=(\n"coo_to_sparse_csr: conversion from Sparse to SparseCsr for input"\n" tensors with sparse_dim[(][)]!=2 is not supported"\n),\n)\nelif layout is torch.sparse_csc and t_args[0].ndim > 0:\n
"""Sample inputs for reduction operations on sparse tensors."""\nlayout_name = str(layout).split(".", 1)[-1].rsplit("_coo", 1)[0]\nop_supports_layout = getattr(op_info, "supports_" + layout_name)\nif not op_supports_layout:\n
elif layout is torch.sparse_bsc:\nother_layout = torch.sparse_bsr\nelse:\nother_layout = torch.strided\nyield SampleInput(tensor, args=(), kwargs=dict(layout=other_layout))\n\nif layout is not torch.sparse_coo:\n
dtype = torch.float64\nrequires_grad = False\nyield from _error_inputs_sparse(\n_maybe_failing_sample_inputs_sparse_elementwise_binary_mul,\n_validate_sample_input_sparse_elementwise_binary_operation,\n
\ndef _maybe_failing_sample_inputs_sparse_like_fns(\nop_info, device, dtype, requires_grad, layout, **kwargs\n):\nif torch.cuda.is_available() and layout is not torch.sparse_coo:\nother_device = "cuda" if torch.device(device).type == "cpu" else "cpu"\n
if layout is torch.sparse_csr:\nother_layout = torch.sparse_csc\nelif layout is torch.sparse_csc:\nother_layout = torch.sparse_csr\nelif layout is torch.sparse_bsr:\nother_layout = torch.sparse_bsc\nelif layout is torch.sparse_bsc:\n
ObservationType,\nBackendPatternConfig,\n)\n\nweighted_op_quint8_dtype_config = DTypeConfig(\ninput_dtype=torch.quint8,\noutput_dtype=torch.quint8,\nweight_dtype=torch.qint8,\nbias_dtype=torch.float,\n
# original:\n#         weight - t \\n#         input  - addmm\n# observed (no hack):\n#      weight - t - observer \\n#       input - observer - addmm\n# target:\n#      weight - observer - t \\n#        input - observer - addmm\n
BackendPatternConfig(bop_pattern)\n.set_dtype_configs(dtype_configs)  # noqa: E131\n._set_num_tensor_args_to_observation_type(num_tensor_args_to_observation_type_mapping))\n\nreturn binary_op_configs\n
0: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT,\n1: ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT,\n2: ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT,\n}\nfor op_with_quantized_bop_scalar_variant in [torch.ops.aten.add.Tensor, torch.ops.aten.add_.Tensor]:\n
observation_type = ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT\ndtype_configs = [weighted_op_quint8_dtype_config]\nbackend_pattern_configs.append(\nBackendPatternConfig(torch.ops.aten.relu.default)\n
#     .set_observation_type(observation_type)  # noqa: E131\n#     .set_dtype_configs(dtype_configs)\n#     ._set_root_node_getter(root_node_getter))\n\nlinear_configs.append(\nBackendPatternConfig(torch.ops.aten.addmm.default)\n
BackendConfig("qnnpack_pytorch_2.0_export")\n.set_backend_pattern_configs(get_linear_configs())\n.set_backend_pattern_configs(get_binary_op_configs())\n.set_backend_pattern_configs(get_conv_configs())\n
._set_input_type_to_index({"weight": 2, "bias": 0})\n)\n# linear is decomposed to `t - mm` if bias is not present\nlinear_configs.append(\nBackendPatternConfig(torch.ops.aten.mm.default)\n.set_observation_type(observation_type)  # noqa: E131\n
# update offset to synchronize among ranks\nself._set_post_op_offset(spec, old_offset)\nelse:\nyield\n\ndef get_offset(self, name: str) -> int:\nif name not in self.rng_states:\nraise RuntimeError(\nf"{self.__class__.__name__} does not have random state for {name}"\n
seed_tensor = (self.rng_states[name])[0:8]\noffset_tensor = torch.tensor([offset]).view(torch.uint8)\nself.rng_states[name] = torch.cat([seed_tensor, offset_tensor])\n\ndef _set_pre_op_offset(self, spec: DTensorSpec) -> None:\n
\nfrom torch.distributed._tensor.ops.utils import prod\n\nnumel = prod(dtensor_shape)\n# pytorch: offset must be multiple of 4\n# source: aten/src/ATen/cuda/CUDAGeneratorImpl.cpp\nnumel = (numel + 3) // 4 * 4\n
def get_seed(self, name: str) -> int:\nif name not in self.rng_states:\nraise RuntimeError(\nf"{self.__class__.__name__} does not have random state for {name}"\n)\n\nseed_tensor = (self.rng_states[name])[0:8].view(dtype=torch.int64)\n
\n\nclass OffsetBasedRNGTracker(_RNGStateTracker):\n"""\nThis subclass of `_RNGStateTracker` defines the default policy of how RNG states\nshould be shared and synchronized among all ranks to respect the semantics of DTensor\n
if name not in self.rng_states:\nraise RuntimeError(\nf"{self.__class__.__name__} does not have random state for {name}"\n)\n\noffset_tensor = (self.rng_states[name])[8:].view(dtype=torch.int64)\nreturn int(offset_tensor.item())\n
\nfrom torch import Tensor\nfrom torch.distributed._tensor.placement_types import DTensorSpec, Shard\nfrom torch.distributed.device_mesh import _get_device_handle, DeviceMesh\n\n\n_rng_tracker: Optional["_RNGStateTracker"] = None\n
\n# Compute shard coordinate:\n# The coordinate on each tensor dim is a tuple (idx, range)\n# If a DTensor is partitioned on its dim i into n shards, and the current rank\n# holds the j-th, then its shard coordinate will be (idx=j, range=n) on dim i\n
#    GAN training: loss = D(real) - D(fake). Otherwise, engine will\n#    complain that variables needed to do backward for the first forward\n#    (i.e., the `u` and `v` vectors) are changed in the second forward.\n
module.register_parameter(self.name, torch.nn.Parameter(weight.detach()))\n\ndef __call__(self, module: Module, inputs: Any) -> None:\nsetattr(module, self.name, self.compute_weight(module, do_power_iteration=module.training))\n
of PyTorch.\n\nExample::\n\n>>> m = spectral_norm(nn.Linear(20, 40))\n>>> m\nLinear(in_features=20, out_features=40, bias=True)\n>>> m.weight_u.size()\ntorch.Size([40])\n\n"""\nif dim is None:\nif isinstance(module, (torch.nn.ConvTranspose1d,\n
parametrization functionality in\n:func:`torch.nn.utils.parametrize.register_parametrization`. Please use\nthe newer version. This function will be deprecated in a future version\nof PyTorch.\n\nExample::\n
eps: float\n\ndef __init__(self, name: str = 'weight', n_power_iterations: int = 1, dim: int = 0, eps: float = 1e-12) -> None:\nself.name = name\nself.dim = dim\nif n_power_iterations <= 0:\nraise ValueError('Expected n_power_iterations to be positive, but '\n
raise RuntimeError(f"Cannot register two spectral_norm hooks on the same parameter {name}")\n\nfn = SpectralNorm(name, n_power_iterations, dim, eps)\nweight = module._parameters[name]\nif weight is None:\n
name: str = 'weight',\nn_power_iterations: int = 1,\neps: float = 1e-12,\ndim: Optional[int] = None) -> T_module:\nr"""Apply spectral normalization to a parameter in the given module.\n\n.. math::\n\mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})},\n
v = v.clone(memory_format=torch.contiguous_format)\n\nsigma = torch.dot(u, torch.mv(weight_mat, v))\nweight = weight / sigma\nreturn weight\n\ndef remove(self, module: Module) -> None:\nwith torch.no_grad():\n
torch.clip(a, min=-0.5, max=0.5),\ntorch.conj(a),\ntorch.copysign(a, 1),\ntorch.copysign(a, b),\ntorch.cos(a),\ntorch.cosh(a),\ntorch.deg2rad(\ntorch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])\n
torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8)),\ntorch.logical_not(torch.tensor([0.0, 1.5, -10.0], dtype=torch.double)),\ntorch.logical_not(\ntorch.tensor([0.0, 1.0, -10.0], dtype=torch.double),\n
torch.trace(e),\ntorch.tril(e),\ntorch.tril_indices(3, 3),\ntorch.triu(e),\ntorch.triu_indices(3, 3),\ntorch.vander(a),\ntorch.view_as_real(torch.randn(4, dtype=torch.cfloat)),\ntorch.view_as_complex(torch.randn(4, 2)).real,\n
torch.logical_not(torch.tensor([0.0, 1.5, -10.0], dtype=torch.double)),\ntorch.logical_not(\ntorch.tensor([0.0, 1.0, -10.0], dtype=torch.double),\nout=torch.empty(3, dtype=torch.int16),\n),\ntorch.logical_or(r, s),\n
torch.fake_quantize_per_tensor_affine(a, 0.1, 0, 0, 255),\ntorch.float_power(torch.randint(10, (4,)), 2),\ntorch.float_power(torch.arange(1, 5), torch.tensor([2, -3, 4, -5])),\ntorch.floor(a),\ntorch.floor(float(torch.tensor(1))),\n
torch.le(a, b),\ntorch.le(a, 1),\ntorch.less_equal(a, b),\ntorch.lt(a, b),\ntorch.lt(a, 1),\ntorch.less(a, b),\ntorch.maximum(a, b),\ntorch.minimum(a, b),\ntorch.fmax(a, b),\ntorch.fmin(a, b),\ntorch.ne(a, b),\n
torch.fmax(a, b),\ntorch.fmin(a, b),\ntorch.ne(a, b),\ntorch.ne(a, 1),\ntorch.not_equal(a, b),\ntorch.sort(a),\ntorch.topk(a, 1),\ntorch.msort(a),\n)\n\n\nclass OtherMathOpsModule(torch.nn.Module):\ndef forward(self):\n
torch.copysign(a, 1),\ntorch.copysign(a, b),\ntorch.cos(a),\ntorch.cosh(a),\ntorch.deg2rad(\ntorch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])\n),\ntorch.div(a, b),\na.div(b),\na.div(1),\n
\n# adds patch, save_config, invalid config checks, etc\ninstall_config_module(sys.modules[__name__])\n
\n\n# Bans recomputation of nodes that are reading from nodes that is far before\n# the current node\nban_recompute_used_far_apart = True\n# Breaks up long chain of fusible ops, as otherwise we can have an arbitrarily\n
ban_recompute_long_fusible_chains = True\n# Bans recomputation of nodes that must be materialized in the backwards pass\n# (used by a non-fusible node)\nban_recompute_materialized_backward = True\n# Chooses to ban recomputation of nodes based off an allowlist. Setting it to\n
\n# adds patch, save_config, invalid config checks, etc\ninstall_config_module(sys.modules[__name__])\n
# (e.g. you have a bunch of model parameters that all alias the same underlying buffer),\n# our checks for this situation are very slow if these inputs have dynamic shapes.\n# This config is set to ensure that there aren't too many aliased inputs in this situation,\n
_max_aliased_inputs_with_dynamic_shapes_enabled = 5\n\nstatic_weight_shapes = True\n\n# Applies CSE to the graph before partitioning\ncse = True\n\n# When AOTAutograd regenerates aliased graph outputs,\n
#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree.\n\n"""\nGlobal flags for aot autograd\n"""\nimport os\nimport sys\n
fake_tensor_allow_unsafe_data_ptr_access = True\n\n# Unlifts effect tokens from the inputs/outputs in the traced graph and instead\n# inserts make_token/sink_token calls in the graph to create tokens and then\n
elif type(mod_a) == type(mod_b):\nreturn SubgraphTypeRelationship.EQUAL\nelse:\nreturn SubgraphTypeRelationship.RELATED_BUT_NOT_EQUAL\n\nreturn SubgraphTypeRelationship.NOT_RELATED\n\ndef _get_name_for_subgraph(\n
node_a_has_prev = subgraph_a.base_op_node == subgraph_a.start_node\nnode_b_has_prev = subgraph_b.base_op_node == subgraph_b.start_node\nif node_a_has_prev and (not node_b_has_prev):\nreturn SubgraphTypeRelationship.RELATED_BUT_NOT_EQUAL\n
if type(mod_a) == type(mod_b):\nreturn SubgraphTypeRelationship.EQUAL_BUT_UKNOWN\nelse:\nreturn SubgraphTypeRelationship.NOT_RELATED\nelif type(mod_a) == type(mod_b):\nreturn SubgraphTypeRelationship.EQUAL\n
if node_a.op in ('call_function', 'call_method'):\nkey = (node_a.target, node_b.target)\n\nif key not in type_a_related_to_b:\nif node_a.target == node_b.target:\nreturn SubgraphTypeRelationship.EQUAL_BUT_UKNOWN\n
self.seen_nodes.add(cur_start_node)\n# for now, assume that there are no other nodes\n# which need to be added to the stack\ncur_start_node = cur_start_node.args[0]  # type: ignore[assignment]\n# if the base op index matches the current node, set it\n
"""\ntarget_type = _get_node_target_type(subgraph_a.base_op_node, gm_a)\ntarget_base_type = None\nfor base_name, sets_of_related_ops in base_name_to_sets_of_related_ops.items():\nif target_type in sets_of_related_ops:\n
if cur_subgraph_b is not None:\ntype_start_b = _get_node_target_type(cur_subgraph_b.start_node, gm_b)\n\n# check for results and determine what to do next\nif cur_subgraph_a is not None and cur_subgraph_b is not None:\n
return proposed_name\n\ndef _get_node_target_type(node: Node, gm: GraphModule) -> Optional[NSNodeTargetType]:\nif node.op in ('call_function', 'call_method'):\nreturn node.target\nelif node.op == 'call_module':\n
x: TensorLikeType,\nord: Union[float, int] = 2,\ndim: Optional[DimsType] = None,\nkeepdim: bool = False,\n*,\ndtype: Optional[torch.dtype] = None,\n) -> Tensor:\nfrom torch.fx.experimental.symbolic_shapes import guard_size_oblivious\n
# Implementation\nif ord == 0.0:\nreturn torch.sum(torch.ne(x, 0.0), dim=dim, keepdim=keepdim, dtype=result_dtype)\nelif ord == float("inf"):\nreturn to_result_dtype(torch.amax(torch.abs(x), dim=dim, keepdim=keepdim))  # type: ignore[return-value,arg-type]\n
check_is_matrix,\nDim,\nDimsType,\nELEMENTWISE_TYPE_PROMOTION_KIND,\nIntLike,\nNumberType,\nTensorLikeType,\n)\nfrom torch._prims_common.wrappers import (\n_maybe_convert_to_dtype,\nelementwise_type_promotion_wrapper,\n
elif ord == float("inf"):\nreturn to_result_dtype(torch.amax(torch.abs(x), dim=dim, keepdim=keepdim))  # type: ignore[return-value,arg-type]\nelif ord == float("-inf"):\nreturn to_result_dtype(torch.amin(torch.abs(x), dim=dim, keepdim=keepdim))  # type: ignore[return-value,arg-type]\n
)\n_check_norm_dtype(dtype, x.dtype, "linalg.vector_norm")\n\ncomputation_dtype, result_dtype = utils.reduction_dtypes(\nx, utils.REDUCTION_OUTPUT_TYPE_KIND.COMPLEX_TO_FLOAT, dtype\n)\n\nto_result_dtype = partial(_maybe_convert_to_dtype, dtype=result_dtype)\n
torch._check(\na.size(dim) == 3 and b.size(dim) == 3,\nlambda: f"linalg.cross: inputs dim {dim} must have length 3, got {a.size(dim)} and {b.size(dim)}",\n)\na, b = torch.broadcast_tensors(a, b)\ndim = utils.canonicalize_dim(a.ndim, dim)\n
lambda: f"{fn_name}: dtype should be floating point or complex. Got {dtype}",\n)\ntorch._check(\nutils.is_complex_dtype(dtype) == utils.is_complex_dtype(x_dtype),\nlambda: "{fn_name}: dtype should be {d} for {d} inputs. Got {dtype}".format(\n
computation_dtype, result_dtype = utils.reduction_dtypes(\nx, utils.REDUCTION_OUTPUT_TYPE_KIND.COMPLEX_TO_FLOAT, dtype\n)\n\nto_result_dtype = partial(_maybe_convert_to_dtype, dtype=result_dtype)\n\n# Implementation\n
TYPE_CODES = np.typecodes["AllInteger"] + np.typecodes["Float"]\n\n@skip(reason="NP_VER: fails on CI; no method=")\n@parametrize("dtype", TYPE_CODES)\ndef test_lower_higher(self, dtype):\nassert_equal(np.percentile(np.arange(10, dtype=dtype), 50, method="lower"), 4)\n
x = np.arange(5)\nassert_array_equal(f(x), x * x)\nassert_equal(_calls[0], len(x))\n\ndef test_otypes(self):\nf = np.vectorize(lambda x: x)\nf.otypes = "i"\nx = np.arange(5)\nassert_array_equal(f(x), x)\n
\n@skip(reason="NP_VER: fails on CI; no method=")\ndef test_out_nan(self):\nwith warnings.catch_warnings(record=True):\nwarnings.filterwarnings("always", "", RuntimeWarning)\no = np.zeros((4,))\nd = np.ones((3, 4))\n
w3 = rand(5).astype(np.float64)\n\nassert_(np.average(y3, weights=w3).dtype == np.result_type(y3, w3))\n\n# test weights with `keepdims=False` and `keepdims=True`\nx = np.array([2, 3, 4]).reshape(3, 1)\n
\nif method in ["inverted_cdf", "closest_observation"]:\nnp.testing.assert_equal(np.asarray(actual).dtype, np.dtype(input_dtype))\nelse:\nnp.testing.assert_equal(np.asarray(actual).dtype, np.dtype(expected_dtype))\n
[0.9297531, 0.32296769, 0.19267156],\n]\n)\nB = np.array(\n[\n[0.10377691, 0.5417086, 0.49807457],\n[0.82872117, 0.77801674, 0.39226705],\n[0.9314666, 0.66800209, 0.03538394],\n]\n)\nres1 = np.array(\n
\ndef test_basic(self):\nslc = np.s_[2:-1]\nfor arr in self.values():\nres = trim_zeros(arr)\nassert_array_equal(res, arr[slc])\n\ndef test_leading_skip(self):\nslc = np.s_[:-1]\nfor arr in self.values():\n
def foo():\nreturn 1\n\nf = vectorize(foo)\nassert_array_equal(f(), 1)\n\ndef test_assigning_docstring(self):\ndef foo(x):\n"""Original documentation"""\nreturn x\n\nf = vectorize(foo)\nassert_equal(f.__doc__, foo.__doc__)\n
device = f"cuda:{self.rank}"\nm = ToyModel(\nin_feat=10,\nhidden_feat=5000,\nout_feat=5,\n).to(device)\ninputs = torch.rand(20, 10).to(device)\nm.apply(init_weights)\ncorrect_outputs = m(inputs)\nfsdp_m = FSDP(m, use_orig_params=True)\n
def __init__(self):\nsuper().__init__()\nmods = [\n(MyLinear(), torch.nn.ReLU()),\n# sandwich the custom in the middle so it comes before and after\n(MyCustomLinear(), torch.nn.ReLU()),\n(MyLinear(), torch.nn.ReLU()),\n
import torch._dynamo\nimport torch._dynamo.logging\nimport torch._dynamo.test_case\nfrom torch import nn\nfrom torch._C import FileCheck\nfrom torch._dynamo import config\nfrom torch._dynamo.backends.distributed import DDPOptimizer\n
CheckpointImpl,\n)\n\nmodel = FSDP(\ncopy.deepcopy(model), auto_wrap_policy=wrap_policy, use_orig_params=True\n)\nif use_activation_checkpointing:\ncheckpoint_wrapper_fn = functools.partial(\ncheckpoint_wrapper,\n
fsdp_m,\ninputs,\n)\n\ndef test_fsdp_skip_guards(self):\n"""\nIt's currently difficult to test dynamo guards.  Most guards tests are indirect- modify something and\nobserve that the guard in question failed. In this case, since the FSDP guards were already deemed\n
class TestMultiProc(DynamoDistributedMultiProcTestCase):\n"""\nNote: MultiProcTestCase spawns processes per test and is slow.\nPrefer MultiThreadedTestCase for most tests. Perhaps use this one\nsparingly for integration tests.\n
inputs = torch.rand((512, 512)).to(device)\n# test duplicated inputs\ninputs = (inputs, inputs)\ncorrect_outputs = m(*inputs)\nreturn m, inputs, correct_outputs\n\n\ndef get_hf_bert(rank):\n# Note: use @import_transformers_or_skip on your test case if you use this\n
def test_hf_bert_fsdp(self):\ndef apply_fsdp(model, wrap_policy):\nmodel = FSDP(\ncopy.deepcopy(model), auto_wrap_policy=wrap_policy, use_orig_params=True\n)\nreturn model\n\nwith _dynamo_dist_per_rank_init(self.rank, self.world_size):\n
Read more about feature classification at:\nhttps://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n\nThis operator requires runtime code generation and so requires support for\n``torch.compile``. Further, only CUDA device codegen is supported at the moment.\n
assert (\no_meta.shape == ()\n), f"combine_fn must return a scalar tensor but got shape {o_meta.shape}"\nassert (\no_meta.shape == ()\n), f"combine_fn must return a scalar tensor but got shape {o_meta.shape}"\n
for x, bdim in zip(input, input_bdims):\nunwrap = get_unwrapped(x)\nif dim is None:\nunwrap = unwrap.unsqueeze(0).expand(batch_size, *x.shape)\nelse:\nunwrap = unwrap.movedim(bdim, 0)\ninput_unwrapped.append(unwrap)\n
input_unwrapped = []\nfor x, bdim in zip(input, input_bdims):\nunwrap = get_unwrapped(x)\nif dim is None:\nunwrap = unwrap.unsqueeze(0).expand(batch_size, *x.shape)\nelse:\nunwrap = unwrap.movedim(bdim, 0)\n
\nreturn track_tensor_tree(out, out_proxy, constant=None, tracer=proxy_mode.tracer)\n\n\n@associative_scan_op.py_impl(DispatchKey.CompositeExplicitAutograd)\ndef associative_scan_op_dense(combine_fn, input, dim):\n
if mode.enable_tracing:\nreturn trace_associative_scan(mode, associative_scan_op, combine_fn, input, dim)\nelse:\nreturn associative_scan_op(mode, associative_scan_op, combine_fn, input, dim)\n\n\n@associative_scan_op.py_impl(FakeTensorMode)\n
\n\ndef associative_scan(\ncombine_fn: Callable[[pytree.PyTree, pytree.PyTree], pytree.PyTree],\ninput: pytree.PyTree,\ndim: int,\n) -> torch.Tensor:\nr"""\nPerforms an inclusive scan with an associative pointwise combine function.\n
from typing import Callable, List\n\nimport torch\n\nimport torch._prims_common as utils\nimport torch._subclasses.functional_tensor\n\nimport torch.utils._pytree as pytree\n\nfrom torch._C import DispatchKey\n
\nstate_dict_metadata: Dict[str, STORAGE_TYPES] = {}\nfor key, tensor in self.state_dict.items():\nif not torch.is_tensor(tensor):\nraise RuntimeError(\nf"Non-tensor value identified at {key}. "\nf"At this time {type(self).__name__} only supports loading Tensors."\n
import os\nfrom enum import Enum\nfrom typing import cast, Dict, List, Optional, Union\n\nimport torch\nimport torch.distributed as dist\nfrom torch.distributed._shard._utils import narrow_tensor_by_index\n
parser.add_argument("src", type=str, help="Path to the source model")\nparser.add_argument("dst", type=str, help="Path to the destination model")\nargs = parser.parse_args()\n\nprint(\nf"Converting checkpoint from {args.src} to {args.dst} using method: '{args.mode}'"\n
\ndef __init__(\nself,\ncheckpoint_id: Optional[Union[str, os.PathLike]] = None,\ncoordinator_rank: int = 0,\n) -> None:\nself.checkpoint_id = checkpoint_id\nself.coordinator_rank = coordinator_rank\n
choices=[m.value for m in FormatMode],\ndefault=FormatMode.TORCH_TO_DCP,\n)\nparser.add_argument("src", type=str, help="Path to the source model")\nparser.add_argument("dst", type=str, help="Path to the destination model")\n
"""Setups of the planner, extnding default behavior by creating the Metadata object from the state dict"""\nsuper().set_up_planner(state_dict, metadata, is_coordinator)\n\nstate_dict_metadata: Dict[str, STORAGE_TYPES] = {}\n
type=str,\nhelp="Conversion mode",\nchoices=[m.value for m in FormatMode],\ndefault=FormatMode.TORCH_TO_DCP,\n)\nparser.add_argument("src", type=str, help="Path to the source model")\nparser.add_argument("dst", type=str, help="Path to the destination model")\n
Extension of DefaultLoadPlanner, which creates a new Metadata object based on the passed in state dict,\navoiding the need to read metadata from disk. This is useful when reading formats which don't have a\n
# In most cases we will use the "as_name" field to store arguments which are\n# SymInts.\n# The "as_int" field is used in the case where we have a list containing a mix\n# of SymInt and ints (ex. [1, s0, ...]). We will serialize this type of list to\n
as_int: int\n\n\n@dataclass(repr=False)\nclass SymBool(_Union):\nas_expr: SymExpr\nas_bool: bool\n\n\n@dataclass\nclass TensorMeta:\ndtype: ScalarType\nsizes: List[SymInt]\nrequires_grad: bool\ndevice: Device\n
\nclass ScalarType(IntEnum):\nUNKNOWN = 0\nBYTE = 1\nCHAR = 2\nSHORT = 3\nINT = 4\nLONG = 5\nHALF = 6\nFLOAT = 7\nDOUBLE = 8\nCOMPLEXHALF = 9\nCOMPLEXFLOAT = 10\nCOMPLEXDOUBLE = 11\nBOOL = 12\nBFLOAT16 = 13\n
range_constraints: Dict[str, RangeConstraint]\nschema_version: SchemaVersion\ndialect: str\n
